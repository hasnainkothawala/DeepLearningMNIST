{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 shape ==> (300, 784)\n",
      "w2 shape ==> (100, 300)\n",
      "w3 shape ==> (10, 100)\n",
      "tr_x shape ==> (60000, 784)\n",
      "tr_y shape ==> (10, 60000)\n",
      "Iteration  0 : Loss =  2.2864292   Acc:  0.9\n",
      "            : Test Loss :2.1789965629577637 Test Accuracy : 0.8999999761581421 \n",
      "****************************************************************************************************\n",
      "Iteration  1 : Loss =  2.1783767   Acc:  0.9\n",
      "            : Test Loss :2.0839076042175293 Test Accuracy : 0.8999999761581421 \n",
      "****************************************************************************************************\n",
      "Iteration  2 : Loss =  2.082427   Acc:  0.9\n",
      "            : Test Loss :1.9781426191329956 Test Accuracy : 0.8999999761581421 \n",
      "****************************************************************************************************\n",
      "Iteration  3 : Loss =  1.975341   Acc:  0.9\n",
      "            : Test Loss :1.8646200895309448 Test Accuracy : 0.8999999761581421 \n",
      "****************************************************************************************************\n",
      "Iteration  4 : Loss =  1.860478   Acc:  0.9\n",
      "            : Test Loss :1.7489622831344604 Test Accuracy : 0.8999999761581421 \n",
      "****************************************************************************************************\n",
      "Iteration  5 : Loss =  1.7433654   Acc:  0.9\n",
      "            : Test Loss :1.6322875022888184 Test Accuracy : 0.9006400108337402 \n",
      "****************************************************************************************************\n",
      "Iteration  6 : Loss =  1.6253525   Acc:  0.90061\n",
      "            : Test Loss :1.516297698020935 Test Accuracy : 0.9035900235176086 \n",
      "****************************************************************************************************\n",
      "Iteration  7 : Loss =  1.5084035   Acc:  0.903885\n",
      "            : Test Loss :1.4040181636810303 Test Accuracy : 0.9076700210571289 \n",
      "****************************************************************************************************\n",
      "Iteration  8 : Loss =  1.3957119   Acc:  0.9078983\n",
      "            : Test Loss :1.300599455833435 Test Accuracy : 0.9120200276374817 \n",
      "****************************************************************************************************\n",
      "Iteration  9 : Loss =  1.292068   Acc:  0.91262835\n",
      "            : Test Loss :1.207292914390564 Test Accuracy : 0.9181699752807617 \n",
      "****************************************************************************************************\n",
      "Iteration  10 : Loss =  1.1983602   Acc:  0.91866666\n",
      "            : Test Loss :1.1246601343154907 Test Accuracy : 0.924239993095398 \n",
      "****************************************************************************************************\n",
      "Iteration  11 : Loss =  1.1149756   Acc:  0.924485\n",
      "            : Test Loss :1.054018497467041 Test Accuracy : 0.9298999905586243 \n",
      "****************************************************************************************************\n",
      "Iteration  12 : Loss =  1.0432899   Acc:  0.93038166\n",
      "            : Test Loss :0.9928473830223083 Test Accuracy : 0.9340999722480774 \n",
      "****************************************************************************************************\n",
      "Iteration  13 : Loss =  0.981041   Acc:  0.934465\n",
      "            : Test Loss :0.9409210681915283 Test Accuracy : 0.9368000030517578 \n",
      "****************************************************************************************************\n",
      "Iteration  14 : Loss =  0.92826056   Acc:  0.937545\n",
      "            : Test Loss :0.8981552720069885 Test Accuracy : 0.9387199878692627 \n",
      "****************************************************************************************************\n",
      "Iteration  15 : Loss =  0.88461477   Acc:  0.93972164\n",
      "            : Test Loss :0.863398551940918 Test Accuracy : 0.9407200217247009 \n",
      "****************************************************************************************************\n",
      "Iteration  16 : Loss =  0.8486658   Acc:  0.94188\n",
      "            : Test Loss :0.833092987537384 Test Accuracy : 0.9423800110816956 \n",
      "****************************************************************************************************\n",
      "Iteration  17 : Loss =  0.8175205   Acc:  0.9438033\n",
      "            : Test Loss :0.8079767823219299 Test Accuracy : 0.9434800148010254 \n",
      "****************************************************************************************************\n",
      "Iteration  18 : Loss =  0.7915884   Acc:  0.94496167\n",
      "            : Test Loss :0.7864574193954468 Test Accuracy : 0.9448999762535095 \n",
      "****************************************************************************************************\n",
      "Iteration  19 : Loss =  0.7689599   Acc:  0.94614834\n",
      "            : Test Loss :0.7669774889945984 Test Accuracy : 0.9459400177001953 \n",
      "****************************************************************************************************\n",
      "Iteration  20 : Loss =  0.7486769   Acc:  0.94756836\n",
      "            : Test Loss :0.7503413558006287 Test Accuracy : 0.9470000267028809 \n",
      "****************************************************************************************************\n",
      "Iteration  21 : Loss =  0.7311417   Acc:  0.94863665\n",
      "            : Test Loss :0.735485851764679 Test Accuracy : 0.9484999775886536 \n",
      "****************************************************************************************************\n",
      "Iteration  22 : Loss =  0.7150705   Acc:  0.9502817\n",
      "            : Test Loss :0.7213349342346191 Test Accuracy : 0.9493399858474731 \n",
      "****************************************************************************************************\n",
      "Iteration  23 : Loss =  0.70005107   Acc:  0.950905\n",
      "            : Test Loss :0.7085326910018921 Test Accuracy : 0.9501399993896484 \n",
      "****************************************************************************************************\n",
      "Iteration  24 : Loss =  0.6864154   Acc:  0.95168\n",
      "            : Test Loss :0.6957911849021912 Test Accuracy : 0.9513199925422668 \n",
      "****************************************************************************************************\n",
      "Iteration  25 : Loss =  0.6734288   Acc:  0.95294166\n",
      "            : Test Loss :0.6843055486679077 Test Accuracy : 0.9523599743843079 \n",
      "****************************************************************************************************\n",
      "Iteration  26 : Loss =  0.6614792   Acc:  0.95398\n",
      "            : Test Loss :0.6733510494232178 Test Accuracy : 0.9535300135612488 \n",
      "****************************************************************************************************\n",
      "Iteration  27 : Loss =  0.65024954   Acc:  0.95529\n",
      "            : Test Loss :0.6640943884849548 Test Accuracy : 0.9541599750518799 \n",
      "****************************************************************************************************\n",
      "Iteration  28 : Loss =  0.64002335   Acc:  0.95540667\n",
      "            : Test Loss :0.6539669036865234 Test Accuracy : 0.9549599885940552 \n",
      "****************************************************************************************************\n",
      "Iteration  29 : Loss =  0.6304403   Acc:  0.9568767\n",
      "            : Test Loss :0.6443065404891968 Test Accuracy : 0.9554799795150757 \n",
      "****************************************************************************************************\n",
      "Iteration  30 : Loss =  0.61982787   Acc:  0.95705664\n",
      "            : Test Loss :0.6328757405281067 Test Accuracy : 0.9563000202178955 \n",
      "****************************************************************************************************\n",
      "Iteration  31 : Loss =  0.6085254   Acc:  0.9583117\n",
      "            : Test Loss :0.6243419647216797 Test Accuracy : 0.9571200013160706 \n",
      "****************************************************************************************************\n",
      "Iteration  32 : Loss =  0.6001159   Acc:  0.9591183\n",
      "            : Test Loss :0.6177045702934265 Test Accuracy : 0.9571099877357483 \n",
      "****************************************************************************************************\n",
      "Iteration  33 : Loss =  0.59220415   Acc:  0.95924\n",
      "            : Test Loss :0.6069518327713013 Test Accuracy : 0.9580600261688232 \n",
      "****************************************************************************************************\n",
      "Iteration  34 : Loss =  0.58269364   Acc:  0.960225\n",
      "            : Test Loss :0.5984225869178772 Test Accuracy : 0.9586099982261658 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  35 : Loss =  0.57357824   Acc:  0.9606983\n",
      "            : Test Loss :0.5925087928771973 Test Accuracy : 0.9587699770927429 \n",
      "****************************************************************************************************\n",
      "Iteration  36 : Loss =  0.56685275   Acc:  0.96101165\n",
      "            : Test Loss :0.5844379663467407 Test Accuracy : 0.9595699906349182 \n",
      "****************************************************************************************************\n",
      "Iteration  37 : Loss =  0.5600356   Acc:  0.9616017\n",
      "            : Test Loss :0.5774889588356018 Test Accuracy : 0.9596800208091736 \n",
      "****************************************************************************************************\n",
      "Iteration  38 : Loss =  0.55214894   Acc:  0.96214664\n",
      "            : Test Loss :0.5712941288948059 Test Accuracy : 0.9603000283241272 \n",
      "****************************************************************************************************\n",
      "Iteration  39 : Loss =  0.5457577   Acc:  0.9625933\n",
      "            : Test Loss :0.5661803483963013 Test Accuracy : 0.960919976234436 \n",
      "****************************************************************************************************\n",
      "Iteration  40 : Loss =  0.54096776   Acc:  0.962905\n",
      "            : Test Loss :0.5623345971107483 Test Accuracy : 0.9611200094223022 \n",
      "****************************************************************************************************\n",
      "Iteration  41 : Loss =  0.5357231   Acc:  0.9633117\n",
      "            : Test Loss :0.5557152628898621 Test Accuracy : 0.961679995059967 \n",
      "****************************************************************************************************\n",
      "Iteration  42 : Loss =  0.5299328   Acc:  0.963805\n",
      "            : Test Loss :0.5515915155410767 Test Accuracy : 0.9621300101280212 \n",
      "****************************************************************************************************\n",
      "Iteration  43 : Loss =  0.5250099   Acc:  0.964185\n",
      "            : Test Loss :0.5491415858268738 Test Accuracy : 0.9620100259780884 \n",
      "****************************************************************************************************\n",
      "Iteration  44 : Loss =  0.52187085   Acc:  0.96426666\n",
      "            : Test Loss :0.547499418258667 Test Accuracy : 0.9621599912643433 \n",
      "****************************************************************************************************\n",
      "Iteration  45 : Loss =  0.5201095   Acc:  0.96444166\n",
      "            : Test Loss :0.5458007454872131 Test Accuracy : 0.9617000222206116 \n",
      "****************************************************************************************************\n",
      "Iteration  46 : Loss =  0.5183925   Acc:  0.9640833\n",
      "            : Test Loss :0.5413675904273987 Test Accuracy : 0.9625599980354309 \n",
      "****************************************************************************************************\n",
      "Iteration  47 : Loss =  0.51253384   Acc:  0.964995\n",
      "            : Test Loss :0.5341225266456604 Test Accuracy : 0.9630500078201294 \n",
      "****************************************************************************************************\n",
      "Iteration  48 : Loss =  0.5062058   Acc:  0.9653183\n",
      "            : Test Loss :0.5339553952217102 Test Accuracy : 0.9632300138473511 \n",
      "****************************************************************************************************\n",
      "Iteration  49 : Loss =  0.5048029   Acc:  0.9652867\n",
      "            : Test Loss :0.5304924845695496 Test Accuracy : 0.9635400176048279 \n",
      "****************************************************************************************************\n",
      "Iteration  50 : Loss =  0.50078666   Acc:  0.96574\n",
      "            : Test Loss :0.5244193077087402 Test Accuracy : 0.963729977607727 \n",
      "****************************************************************************************************\n",
      "Iteration  51 : Loss =  0.4953908   Acc:  0.96598667\n",
      "            : Test Loss :0.5251970291137695 Test Accuracy : 0.9638800024986267 \n",
      "****************************************************************************************************\n",
      "Iteration  52 : Loss =  0.49477994   Acc:  0.96587\n",
      "            : Test Loss :0.5199611186981201 Test Accuracy : 0.9642500281333923 \n",
      "****************************************************************************************************\n",
      "Iteration  53 : Loss =  0.48930964   Acc:  0.96647835\n",
      "            : Test Loss :0.5157328844070435 Test Accuracy : 0.9642199873924255 \n",
      "****************************************************************************************************\n",
      "Iteration  54 : Loss =  0.48570704   Acc:  0.9666283\n",
      "            : Test Loss :0.5166104435920715 Test Accuracy : 0.9645699858665466 \n",
      "****************************************************************************************************\n",
      "Iteration  55 : Loss =  0.48548892   Acc:  0.966475\n",
      "            : Test Loss :0.5102784037590027 Test Accuracy : 0.9650200009346008 \n",
      "****************************************************************************************************\n",
      "Iteration  56 : Loss =  0.4788901   Acc:  0.9671983\n",
      "            : Test Loss :0.5091419219970703 Test Accuracy : 0.9648200273513794 \n",
      "****************************************************************************************************\n",
      "Iteration  57 : Loss =  0.4780546   Acc:  0.96715665\n",
      "            : Test Loss :0.5082463622093201 Test Accuracy : 0.9652199745178223 \n",
      "****************************************************************************************************\n",
      "Iteration  58 : Loss =  0.47643322   Acc:  0.96722335\n",
      "            : Test Loss :0.502214789390564 Test Accuracy : 0.965470016002655 \n",
      "****************************************************************************************************\n",
      "Iteration  59 : Loss =  0.47046885   Acc:  0.96781665\n",
      "            : Test Loss :0.5031686425209045 Test Accuracy : 0.965179979801178 \n",
      "****************************************************************************************************\n",
      "Iteration  60 : Loss =  0.47128054   Acc:  0.9676567\n",
      "            : Test Loss :0.5001300573348999 Test Accuracy : 0.9655500054359436 \n",
      "****************************************************************************************************\n",
      "Iteration  61 : Loss =  0.46754408   Acc:  0.9680333\n",
      "            : Test Loss :0.49638456106185913 Test Accuracy : 0.9658499956130981 \n",
      "****************************************************************************************************\n",
      "Iteration  62 : Loss =  0.46429145   Acc:  0.968135\n",
      "            : Test Loss :0.49613848328590393 Test Accuracy : 0.9656699895858765 \n",
      "****************************************************************************************************\n",
      "Iteration  63 : Loss =  0.46373132   Acc:  0.9682483\n",
      "            : Test Loss :0.49276360869407654 Test Accuracy : 0.966159999370575 \n",
      "****************************************************************************************************\n",
      "Iteration  64 : Loss =  0.45974725   Acc:  0.96874833\n",
      "            : Test Loss :0.49141138792037964 Test Accuracy : 0.9660000205039978 \n",
      "****************************************************************************************************\n",
      "Iteration  65 : Loss =  0.45895335   Acc:  0.9684517\n",
      "            : Test Loss :0.4888891577720642 Test Accuracy : 0.9662799835205078 \n",
      "****************************************************************************************************\n",
      "Iteration  66 : Loss =  0.45595834   Acc:  0.96885836\n",
      "            : Test Loss :0.4866630733013153 Test Accuracy : 0.9664999842643738 \n",
      "****************************************************************************************************\n",
      "Iteration  67 : Loss =  0.45339522   Acc:  0.9691\n",
      "            : Test Loss :0.48583266139030457 Test Accuracy : 0.9662500023841858 \n",
      "****************************************************************************************************\n",
      "Iteration  68 : Loss =  0.4530161   Acc:  0.96887\n",
      "            : Test Loss :0.48267650604248047 Test Accuracy : 0.9668099880218506 \n",
      "****************************************************************************************************\n",
      "Iteration  69 : Loss =  0.4491946   Acc:  0.96936834\n",
      "            : Test Loss :0.48136383295059204 Test Accuracy : 0.9669700264930725 \n",
      "****************************************************************************************************\n",
      "Iteration  70 : Loss =  0.44784114   Acc:  0.96940833\n",
      "            : Test Loss :0.4796180725097656 Test Accuracy : 0.9668700098991394 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  71 : Loss =  0.44638398   Acc:  0.9694617\n",
      "            : Test Loss :0.47754189372062683 Test Accuracy : 0.9673699736595154 \n",
      "****************************************************************************************************\n",
      "Iteration  72 : Loss =  0.4436182   Acc:  0.96981335\n",
      "            : Test Loss :0.47636255621910095 Test Accuracy : 0.9671300053596497 \n",
      "****************************************************************************************************\n",
      "Iteration  73 : Loss =  0.44251725   Acc:  0.96983165\n",
      "            : Test Loss :0.4739672839641571 Test Accuracy : 0.9674299955368042 \n",
      "****************************************************************************************************\n",
      "Iteration  74 : Loss =  0.4400999   Acc:  0.9700317\n",
      "            : Test Loss :0.47253623604774475 Test Accuracy : 0.9676799774169922 \n",
      "****************************************************************************************************\n",
      "Iteration  75 : Loss =  0.43831316   Acc:  0.9702383\n",
      "            : Test Loss :0.47147196531295776 Test Accuracy : 0.9676799774169922 \n",
      "****************************************************************************************************\n",
      "Iteration  76 : Loss =  0.4372321   Acc:  0.97017664\n",
      "            : Test Loss :0.4694216251373291 Test Accuracy : 0.9678699970245361 \n",
      "****************************************************************************************************\n",
      "Iteration  77 : Loss =  0.43479198   Acc:  0.9704367\n",
      "            : Test Loss :0.46778997778892517 Test Accuracy : 0.967930018901825 \n",
      "****************************************************************************************************\n",
      "Iteration  78 : Loss =  0.43324894   Acc:  0.9705167\n",
      "            : Test Loss :0.46664926409721375 Test Accuracy : 0.9683499932289124 \n",
      "****************************************************************************************************\n",
      "Iteration  79 : Loss =  0.43177736   Acc:  0.97063\n",
      "            : Test Loss :0.46491456031799316 Test Accuracy : 0.9683499932289124 \n",
      "****************************************************************************************************\n",
      "Iteration  80 : Loss =  0.42971686   Acc:  0.97079\n",
      "            : Test Loss :0.4636213779449463 Test Accuracy : 0.9684399962425232 \n",
      "****************************************************************************************************\n",
      "Iteration  81 : Loss =  0.4285338   Acc:  0.970825\n",
      "            : Test Loss :0.4625903367996216 Test Accuracy : 0.968529999256134 \n",
      "****************************************************************************************************\n",
      "Iteration  82 : Loss =  0.4269649   Acc:  0.970925\n",
      "            : Test Loss :0.460552841424942 Test Accuracy : 0.9685999751091003 \n",
      "****************************************************************************************************\n",
      "Iteration  83 : Loss =  0.42499554   Acc:  0.9711417\n",
      "            : Test Loss :0.45957323908805847 Test Accuracy : 0.9688599705696106 \n",
      "****************************************************************************************************\n",
      "Iteration  84 : Loss =  0.4236479   Acc:  0.97124\n",
      "            : Test Loss :0.4580497145652771 Test Accuracy : 0.9690300226211548 \n",
      "****************************************************************************************************\n",
      "Iteration  85 : Loss =  0.42201528   Acc:  0.97131\n",
      "            : Test Loss :0.45646747946739197 Test Accuracy : 0.968999981880188 \n",
      "****************************************************************************************************\n",
      "Iteration  86 : Loss =  0.42025396   Acc:  0.9714933\n",
      "            : Test Loss :0.45550841093063354 Test Accuracy : 0.969219982624054 \n",
      "****************************************************************************************************\n",
      "Iteration  87 : Loss =  0.4189766   Acc:  0.971565\n",
      "            : Test Loss :0.45409706234931946 Test Accuracy : 0.969219982624054 \n",
      "****************************************************************************************************\n",
      "Iteration  88 : Loss =  0.41746894   Acc:  0.9716967\n",
      "            : Test Loss :0.45284169912338257 Test Accuracy : 0.9691799879074097 \n",
      "****************************************************************************************************\n",
      "Iteration  89 : Loss =  0.41586432   Acc:  0.97182167\n",
      "            : Test Loss :0.45156729221343994 Test Accuracy : 0.969219982624054 \n",
      "****************************************************************************************************\n",
      "Iteration  90 : Loss =  0.41465148   Acc:  0.971855\n",
      "            : Test Loss :0.4509925842285156 Test Accuracy : 0.9693400263786316 \n",
      "****************************************************************************************************\n",
      "Iteration  91 : Loss =  0.4134116   Acc:  0.9719167\n",
      "            : Test Loss :0.4494807720184326 Test Accuracy : 0.9692999720573425 \n",
      "****************************************************************************************************\n",
      "Iteration  92 : Loss =  0.41223347   Acc:  0.97201836\n",
      "            : Test Loss :0.4494699239730835 Test Accuracy : 0.9693700075149536 \n",
      "****************************************************************************************************\n",
      "Iteration  93 : Loss =  0.4114406   Acc:  0.972055\n",
      "            : Test Loss :0.4490044414997101 Test Accuracy : 0.9693599939346313 \n",
      "****************************************************************************************************\n",
      "Iteration  94 : Loss =  0.41152057   Acc:  0.9720167\n",
      "            : Test Loss :0.44978412985801697 Test Accuracy : 0.9692400097846985 \n",
      "****************************************************************************************************\n",
      "Iteration  95 : Loss =  0.41107222   Acc:  0.9719483\n",
      "            : Test Loss :0.4490605592727661 Test Accuracy : 0.9691699743270874 \n",
      "****************************************************************************************************\n",
      "Iteration  96 : Loss =  0.41128764   Acc:  0.97201\n",
      "            : Test Loss :0.44650566577911377 Test Accuracy : 0.969539999961853 \n",
      "****************************************************************************************************\n",
      "Iteration  97 : Loss =  0.4075031   Acc:  0.97222334\n",
      "            : Test Loss :0.4425477087497711 Test Accuracy : 0.9696699976921082 \n",
      "****************************************************************************************************\n",
      "Iteration  98 : Loss =  0.40407842   Acc:  0.9725983\n",
      "            : Test Loss :0.440792977809906 Test Accuracy : 0.9698299765586853 \n",
      "****************************************************************************************************\n",
      "Iteration  99 : Loss =  0.40192863   Acc:  0.972755\n",
      "            : Test Loss :0.4412526488304138 Test Accuracy : 0.9698399901390076 \n",
      "****************************************************************************************************\n",
      "Iteration  100 : Loss =  0.4018802   Acc:  0.97265834\n",
      "            : Test Loss :0.4412456154823303 Test Accuracy : 0.9698799848556519 \n",
      "****************************************************************************************************\n",
      "Iteration  101 : Loss =  0.40235624   Acc:  0.9726083\n",
      "            : Test Loss :0.44005438685417175 Test Accuracy : 0.9699100255966187 \n",
      "****************************************************************************************************\n",
      "Iteration  102 : Loss =  0.4002149   Acc:  0.97271836\n",
      "            : Test Loss :0.4371747076511383 Test Accuracy : 0.9701499938964844 \n",
      "****************************************************************************************************\n",
      "Iteration  103 : Loss =  0.39771256   Acc:  0.97307\n",
      "            : Test Loss :0.43556249141693115 Test Accuracy : 0.9701700210571289 \n",
      "****************************************************************************************************\n",
      "Iteration  104 : Loss =  0.39578998   Acc:  0.9731867\n",
      "            : Test Loss :0.4353007376194 Test Accuracy : 0.9703599810600281 \n",
      "****************************************************************************************************\n",
      "Iteration  105 : Loss =  0.39511782   Acc:  0.9731783\n",
      "            : Test Loss :0.43489453196525574 Test Accuracy : 0.9702600240707397 \n",
      "****************************************************************************************************\n",
      "Iteration  106 : Loss =  0.39495215   Acc:  0.973175\n",
      "            : Test Loss :0.43426573276519775 Test Accuracy : 0.9704999923706055 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  107 : Loss =  0.3936517   Acc:  0.9732\n",
      "            : Test Loss :0.4320580065250397 Test Accuracy : 0.970550000667572 \n",
      "****************************************************************************************************\n",
      "Iteration  108 : Loss =  0.39170858   Acc:  0.97342336\n",
      "            : Test Loss :0.43047401309013367 Test Accuracy : 0.9706100225448608 \n",
      "****************************************************************************************************\n",
      "Iteration  109 : Loss =  0.38976723   Acc:  0.97355\n",
      "            : Test Loss :0.4296034276485443 Test Accuracy : 0.9706199765205383 \n",
      "****************************************************************************************************\n",
      "Iteration  110 : Loss =  0.38867897   Acc:  0.973605\n",
      "            : Test Loss :0.42908504605293274 Test Accuracy : 0.9706000089645386 \n",
      "****************************************************************************************************\n",
      "Iteration  111 : Loss =  0.38822773   Acc:  0.97366\n",
      "            : Test Loss :0.42889100313186646 Test Accuracy : 0.9707599878311157 \n",
      "****************************************************************************************************\n",
      "Iteration  112 : Loss =  0.3875575   Acc:  0.97358334\n",
      "            : Test Loss :0.42761602997779846 Test Accuracy : 0.970740020275116 \n",
      "****************************************************************************************************\n",
      "Iteration  113 : Loss =  0.38642567   Acc:  0.973815\n",
      "            : Test Loss :0.4262640178203583 Test Accuracy : 0.9709299802780151 \n",
      "****************************************************************************************************\n",
      "Iteration  114 : Loss =  0.38459146   Acc:  0.973815\n",
      "            : Test Loss :0.4245311915874481 Test Accuracy : 0.9708600044250488 \n",
      "****************************************************************************************************\n",
      "Iteration  115 : Loss =  0.382916   Acc:  0.97402835\n",
      "            : Test Loss :0.4235500991344452 Test Accuracy : 0.9710400104522705 \n",
      "****************************************************************************************************\n",
      "Iteration  116 : Loss =  0.3817007   Acc:  0.974105\n",
      "            : Test Loss :0.4230043888092041 Test Accuracy : 0.9712299704551697 \n",
      "****************************************************************************************************\n",
      "Iteration  117 : Loss =  0.38089585   Acc:  0.97405833\n",
      "            : Test Loss :0.4223098158836365 Test Accuracy : 0.9712399840354919 \n",
      "****************************************************************************************************\n",
      "Iteration  118 : Loss =  0.38019046   Acc:  0.97423\n",
      "            : Test Loss :0.42173856496810913 Test Accuracy : 0.9713000059127808 \n",
      "****************************************************************************************************\n",
      "Iteration  119 : Loss =  0.3792407   Acc:  0.9741617\n",
      "            : Test Loss :0.420596182346344 Test Accuracy : 0.9714400172233582 \n",
      "****************************************************************************************************\n",
      "Iteration  120 : Loss =  0.37814537   Acc:  0.97434664\n",
      "            : Test Loss :0.41963496804237366 Test Accuracy : 0.9713600277900696 \n",
      "****************************************************************************************************\n",
      "Iteration  121 : Loss =  0.3767348   Acc:  0.974325\n",
      "            : Test Loss :0.41828128695487976 Test Accuracy : 0.9717000126838684 \n",
      "****************************************************************************************************\n",
      "Iteration  122 : Loss =  0.37541288   Acc:  0.9745067\n",
      "            : Test Loss :0.4173470735549927 Test Accuracy : 0.9716600179672241 \n",
      "****************************************************************************************************\n",
      "Iteration  123 : Loss =  0.3741575   Acc:  0.97456\n",
      "            : Test Loss :0.41636011004447937 Test Accuracy : 0.9718899726867676 \n",
      "****************************************************************************************************\n",
      "Iteration  124 : Loss =  0.37305543   Acc:  0.97463834\n",
      "            : Test Loss :0.4155997037887573 Test Accuracy : 0.9719700217247009 \n",
      "****************************************************************************************************\n",
      "Iteration  125 : Loss =  0.37204558   Acc:  0.974815\n",
      "            : Test Loss :0.41484853625297546 Test Accuracy : 0.9720900058746338 \n",
      "****************************************************************************************************\n",
      "Iteration  126 : Loss =  0.37111455   Acc:  0.97475\n",
      "            : Test Loss :0.4140813946723938 Test Accuracy : 0.9720199704170227 \n",
      "****************************************************************************************************\n",
      "Iteration  127 : Loss =  0.37019935   Acc:  0.9749333\n",
      "            : Test Loss :0.4134993255138397 Test Accuracy : 0.9720199704170227 \n",
      "****************************************************************************************************\n",
      "Iteration  128 : Loss =  0.36931393   Acc:  0.974865\n",
      "            : Test Loss :0.41277745366096497 Test Accuracy : 0.9720799922943115 \n",
      "****************************************************************************************************\n",
      "Iteration  129 : Loss =  0.36851385   Acc:  0.975065\n",
      "            : Test Loss :0.4123125970363617 Test Accuracy : 0.9720199704170227 \n",
      "****************************************************************************************************\n",
      "Iteration  130 : Loss =  0.3677089   Acc:  0.97491336\n",
      "            : Test Loss :0.41184115409851074 Test Accuracy : 0.9722499847412109 \n",
      "****************************************************************************************************\n",
      "Iteration  131 : Loss =  0.367198   Acc:  0.9750867\n",
      "            : Test Loss :0.41158223152160645 Test Accuracy : 0.9721999764442444 \n",
      "****************************************************************************************************\n",
      "Iteration  132 : Loss =  0.36647296   Acc:  0.97502166\n",
      "            : Test Loss :0.4114097058773041 Test Accuracy : 0.9722300171852112 \n",
      "****************************************************************************************************\n",
      "Iteration  133 : Loss =  0.3663368   Acc:  0.97512835\n",
      "            : Test Loss :0.4108515977859497 Test Accuracy : 0.9722800254821777 \n",
      "****************************************************************************************************\n",
      "Iteration  134 : Loss =  0.36526015   Acc:  0.97512\n",
      "            : Test Loss :0.4102467894554138 Test Accuracy : 0.9723600149154663 \n",
      "****************************************************************************************************\n",
      "Iteration  135 : Loss =  0.3647446   Acc:  0.97524166\n",
      "            : Test Loss :0.4088040888309479 Test Accuracy : 0.9725300073623657 \n",
      "****************************************************************************************************\n",
      "Iteration  136 : Loss =  0.36278558   Acc:  0.975315\n",
      "            : Test Loss :0.4071851968765259 Test Accuracy : 0.9725800156593323 \n",
      "****************************************************************************************************\n",
      "Iteration  137 : Loss =  0.3613072   Acc:  0.9754483\n",
      "            : Test Loss :0.4062936305999756 Test Accuracy : 0.9725599884986877 \n",
      "****************************************************************************************************\n",
      "Iteration  138 : Loss =  0.35995337   Acc:  0.97553664\n",
      "            : Test Loss :0.40582334995269775 Test Accuracy : 0.9724699854850769 \n",
      "****************************************************************************************************\n",
      "Iteration  139 : Loss =  0.3594905   Acc:  0.97549\n",
      "            : Test Loss :0.40617501735687256 Test Accuracy : 0.9727200269699097 \n",
      "****************************************************************************************************\n",
      "Iteration  140 : Loss =  0.35947087   Acc:  0.97567\n",
      "            : Test Loss :0.40667524933815 Test Accuracy : 0.9723899960517883 \n",
      "****************************************************************************************************\n",
      "Iteration  141 : Loss =  0.3598464   Acc:  0.975455\n",
      "            : Test Loss :0.4063172936439514 Test Accuracy : 0.9727299809455872 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  142 : Loss =  0.35923204   Acc:  0.97570336\n",
      "            : Test Loss :0.40444648265838623 Test Accuracy : 0.9725900292396545 \n",
      "****************************************************************************************************\n",
      "Iteration  143 : Loss =  0.35714144   Acc:  0.9756617\n",
      "            : Test Loss :0.40219512581825256 Test Accuracy : 0.9728699922561646 \n",
      "****************************************************************************************************\n",
      "Iteration  144 : Loss =  0.3547792   Acc:  0.975945\n",
      "            : Test Loss :0.4005594253540039 Test Accuracy : 0.9728500247001648 \n",
      "****************************************************************************************************\n",
      "Iteration  145 : Loss =  0.35285416   Acc:  0.97603166\n",
      "            : Test Loss :0.40006744861602783 Test Accuracy : 0.9729599952697754 \n",
      "****************************************************************************************************\n",
      "Iteration  146 : Loss =  0.3522648   Acc:  0.976015\n",
      "            : Test Loss :0.4006320536136627 Test Accuracy : 0.973110020160675 \n",
      "****************************************************************************************************\n",
      "Iteration  147 : Loss =  0.3524794   Acc:  0.9761183\n",
      "            : Test Loss :0.401035875082016 Test Accuracy : 0.9727399945259094 \n",
      "****************************************************************************************************\n",
      "Iteration  148 : Loss =  0.35279188   Acc:  0.9758883\n",
      "            : Test Loss :0.400438517332077 Test Accuracy : 0.9731199741363525 \n",
      "****************************************************************************************************\n",
      "Iteration  149 : Loss =  0.35184896   Acc:  0.9761867\n",
      "            : Test Loss :0.3987395167350769 Test Accuracy : 0.9729300141334534 \n",
      "****************************************************************************************************\n",
      "Iteration  150 : Loss =  0.35008392   Acc:  0.97607666\n",
      "            : Test Loss :0.39691147208213806 Test Accuracy : 0.973360002040863 \n",
      "****************************************************************************************************\n",
      "Iteration  151 : Loss =  0.3479906   Acc:  0.976375\n",
      "            : Test Loss :0.39581650495529175 Test Accuracy : 0.9732499718666077 \n",
      "****************************************************************************************************\n",
      "Iteration  152 : Loss =  0.34672666   Acc:  0.976405\n",
      "            : Test Loss :0.3956691324710846 Test Accuracy : 0.9731500148773193 \n",
      "****************************************************************************************************\n",
      "Iteration  153 : Loss =  0.34641913   Acc:  0.9763783\n",
      "            : Test Loss :0.3960312604904175 Test Accuracy : 0.9734200239181519 \n",
      "****************************************************************************************************\n",
      "Iteration  154 : Loss =  0.34643686   Acc:  0.9764683\n",
      "            : Test Loss :0.39591339230537415 Test Accuracy : 0.9730799794197083 \n",
      "****************************************************************************************************\n",
      "Iteration  155 : Loss =  0.34623373   Acc:  0.97634\n",
      "            : Test Loss :0.39524927735328674 Test Accuracy : 0.9734600186347961 \n",
      "****************************************************************************************************\n",
      "Iteration  156 : Loss =  0.34518737   Acc:  0.97659165\n",
      "            : Test Loss :0.39393380284309387 Test Accuracy : 0.9734200239181519 \n",
      "****************************************************************************************************\n",
      "Iteration  157 : Loss =  0.34382865   Acc:  0.97651166\n",
      "            : Test Loss :0.39270833134651184 Test Accuracy : 0.9737200140953064 \n",
      "****************************************************************************************************\n",
      "Iteration  158 : Loss =  0.3422879   Acc:  0.97674835\n",
      "            : Test Loss :0.39171329140663147 Test Accuracy : 0.9735400080680847 \n",
      "****************************************************************************************************\n",
      "Iteration  159 : Loss =  0.3411159   Acc:  0.9767417\n",
      "            : Test Loss :0.3912263810634613 Test Accuracy : 0.9735900163650513 \n",
      "****************************************************************************************************\n",
      "Iteration  160 : Loss =  0.34039646   Acc:  0.9768017\n",
      "            : Test Loss :0.3911134898662567 Test Accuracy : 0.9738399982452393 \n",
      "****************************************************************************************************\n",
      "Iteration  161 : Loss =  0.33999714   Acc:  0.9768983\n",
      "            : Test Loss :0.39100316166877747 Test Accuracy : 0.9736700057983398 \n",
      "****************************************************************************************************\n",
      "Iteration  162 : Loss =  0.33974135   Acc:  0.97671\n",
      "            : Test Loss :0.3911993205547333 Test Accuracy : 0.9738100171089172 \n",
      "****************************************************************************************************\n",
      "Iteration  163 : Loss =  0.3395873   Acc:  0.976995\n",
      "            : Test Loss :0.3911604881286621 Test Accuracy : 0.9737100005149841 \n",
      "****************************************************************************************************\n",
      "Iteration  164 : Loss =  0.33946258   Acc:  0.976785\n",
      "            : Test Loss :0.39247947931289673 Test Accuracy : 0.9734899997711182 \n",
      "****************************************************************************************************\n",
      "Iteration  165 : Loss =  0.34044352   Acc:  0.97682166\n",
      "            : Test Loss :0.393611878156662 Test Accuracy : 0.9733999967575073 \n",
      "****************************************************************************************************\n",
      "Iteration  166 : Loss =  0.3412884   Acc:  0.97661\n",
      "            : Test Loss :0.3990615904331207 Test Accuracy : 0.9729999899864197 \n",
      "****************************************************************************************************\n",
      "Iteration  167 : Loss =  0.3465247   Acc:  0.97627\n",
      "            : Test Loss :0.3990396559238434 Test Accuracy : 0.9728500247001648 \n",
      "****************************************************************************************************\n",
      "Iteration  168 : Loss =  0.34594846   Acc:  0.976105\n",
      "            : Test Loss :0.39623647928237915 Test Accuracy : 0.9730700254440308 \n",
      "****************************************************************************************************\n",
      "Iteration  169 : Loss =  0.34359583   Acc:  0.97630167\n",
      "            : Test Loss :0.3900107145309448 Test Accuracy : 0.9735599756240845 \n",
      "****************************************************************************************************\n",
      "Iteration  170 : Loss =  0.33698118   Acc:  0.9770333\n",
      "            : Test Loss :0.38952991366386414 Test Accuracy : 0.973770022392273 \n",
      "****************************************************************************************************\n",
      "Iteration  171 : Loss =  0.33657572   Acc:  0.9768483\n",
      "            : Test Loss :0.39452415704727173 Test Accuracy : 0.9733999967575073 \n",
      "****************************************************************************************************\n",
      "Iteration  172 : Loss =  0.34124756   Acc:  0.9766783\n",
      "            : Test Loss :0.3889651894569397 Test Accuracy : 0.9735400080680847 \n",
      "****************************************************************************************************\n",
      "Iteration  173 : Loss =  0.33518744   Acc:  0.97715336\n",
      "            : Test Loss :0.3858643174171448 Test Accuracy : 0.9737899899482727 \n",
      "****************************************************************************************************\n",
      "Iteration  174 : Loss =  0.3323975   Acc:  0.97714\n",
      "            : Test Loss :0.3904888331890106 Test Accuracy : 0.9736400246620178 \n",
      "****************************************************************************************************\n",
      "Iteration  175 : Loss =  0.33659315   Acc:  0.97704834\n",
      "            : Test Loss :0.3875991702079773 Test Accuracy : 0.9738600254058838 \n",
      "****************************************************************************************************\n",
      "Iteration  176 : Loss =  0.33334467   Acc:  0.97716665\n",
      "            : Test Loss :0.38340792059898376 Test Accuracy : 0.9740399718284607 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  177 : Loss =  0.3290826   Acc:  0.9774783\n",
      "            : Test Loss :0.38381171226501465 Test Accuracy : 0.974120020866394 \n",
      "****************************************************************************************************\n",
      "Iteration  178 : Loss =  0.329367   Acc:  0.977545\n",
      "            : Test Loss :0.385646790266037 Test Accuracy : 0.9738500118255615 \n",
      "****************************************************************************************************\n",
      "Iteration  179 : Loss =  0.33104792   Acc:  0.9773267\n",
      "            : Test Loss :0.3848942518234253 Test Accuracy : 0.9739999771118164 \n",
      "****************************************************************************************************\n",
      "Iteration  180 : Loss =  0.32985958   Acc:  0.97744834\n",
      "            : Test Loss :0.3813709616661072 Test Accuracy : 0.9741700291633606 \n",
      "****************************************************************************************************\n",
      "Iteration  181 : Loss =  0.32626444   Acc:  0.977665\n",
      "            : Test Loss :0.3824404776096344 Test Accuracy : 0.9741500020027161 \n",
      "****************************************************************************************************\n",
      "Iteration  182 : Loss =  0.3272251   Acc:  0.977585\n",
      "            : Test Loss :0.38418975472450256 Test Accuracy : 0.9741600155830383 \n",
      "****************************************************************************************************\n",
      "Iteration  183 : Loss =  0.32872164   Acc:  0.9775367\n",
      "            : Test Loss :0.38102245330810547 Test Accuracy : 0.9741899967193604 \n",
      "****************************************************************************************************\n",
      "Iteration  184 : Loss =  0.32538328   Acc:  0.977665\n",
      "            : Test Loss :0.38029974699020386 Test Accuracy : 0.9742100238800049 \n",
      "****************************************************************************************************\n",
      "Iteration  185 : Loss =  0.32434604   Acc:  0.977795\n",
      "            : Test Loss :0.3813033699989319 Test Accuracy : 0.9742299914360046 \n",
      "****************************************************************************************************\n",
      "Iteration  186 : Loss =  0.32536492   Acc:  0.9776817\n",
      "            : Test Loss :0.37995144724845886 Test Accuracy : 0.9742799997329712 \n",
      "****************************************************************************************************\n",
      "Iteration  187 : Loss =  0.32379   Acc:  0.9778117\n",
      "            : Test Loss :0.3783835470676422 Test Accuracy : 0.9742799997329712 \n",
      "****************************************************************************************************\n",
      "Iteration  188 : Loss =  0.3219268   Acc:  0.9779533\n",
      "            : Test Loss :0.3786487877368927 Test Accuracy : 0.9743099808692932 \n",
      "****************************************************************************************************\n",
      "Iteration  189 : Loss =  0.32210556   Acc:  0.97794\n",
      "            : Test Loss :0.37912118434906006 Test Accuracy : 0.9742199778556824 \n",
      "****************************************************************************************************\n",
      "Iteration  190 : Loss =  0.32232404   Acc:  0.97796834\n",
      "            : Test Loss :0.3777637183666229 Test Accuracy : 0.9743599891662598 \n",
      "****************************************************************************************************\n",
      "Iteration  191 : Loss =  0.32084844   Acc:  0.9780267\n",
      "            : Test Loss :0.3767128586769104 Test Accuracy : 0.974399983882904 \n",
      "****************************************************************************************************\n",
      "Iteration  192 : Loss =  0.31946856   Acc:  0.9781567\n",
      "            : Test Loss :0.3768249750137329 Test Accuracy : 0.9745399951934814 \n",
      "****************************************************************************************************\n",
      "Iteration  193 : Loss =  0.31946114   Acc:  0.9781067\n",
      "            : Test Loss :0.37690600752830505 Test Accuracy : 0.9745100140571594 \n",
      "****************************************************************************************************\n",
      "Iteration  194 : Loss =  0.31934908   Acc:  0.97812665\n",
      "            : Test Loss :0.37574025988578796 Test Accuracy : 0.9745200276374817 \n",
      "****************************************************************************************************\n",
      "Iteration  195 : Loss =  0.31794354   Acc:  0.9782017\n",
      "            : Test Loss :0.37487277388572693 Test Accuracy : 0.974590003490448 \n",
      "****************************************************************************************************\n",
      "Iteration  196 : Loss =  0.3169558   Acc:  0.978285\n",
      "            : Test Loss :0.3750505745410919 Test Accuracy : 0.9745799899101257 \n",
      "****************************************************************************************************\n",
      "Iteration  197 : Loss =  0.31691238   Acc:  0.97829336\n",
      "            : Test Loss :0.374843567609787 Test Accuracy : 0.9746999740600586 \n",
      "****************************************************************************************************\n",
      "Iteration  198 : Loss =  0.31651458   Acc:  0.9782517\n",
      "            : Test Loss :0.3742097020149231 Test Accuracy : 0.9744700193405151 \n",
      "****************************************************************************************************\n",
      "Iteration  199 : Loss =  0.31554   Acc:  0.97840834\n",
      "            : Test Loss :0.373307466506958 Test Accuracy : 0.9746800065040588 \n",
      "****************************************************************************************************\n",
      "Iteration  200 : Loss =  0.31458125   Acc:  0.9784433\n",
      "            : Test Loss :0.373290479183197 Test Accuracy : 0.974560022354126 \n",
      "****************************************************************************************************\n",
      "Iteration  201 : Loss =  0.31427622   Acc:  0.9784783\n",
      "            : Test Loss :0.37326017022132874 Test Accuracy : 0.9745799899101257 \n",
      "****************************************************************************************************\n",
      "Iteration  202 : Loss =  0.3140179   Acc:  0.97844\n",
      "            : Test Loss :0.37281787395477295 Test Accuracy : 0.9745299816131592 \n",
      "****************************************************************************************************\n",
      "Iteration  203 : Loss =  0.3132684   Acc:  0.97854334\n",
      "            : Test Loss :0.37197721004486084 Test Accuracy : 0.9746599793434143 \n",
      "****************************************************************************************************\n",
      "Iteration  204 : Loss =  0.3124321   Acc:  0.97850835\n",
      "            : Test Loss :0.3719981610774994 Test Accuracy : 0.9745500087738037 \n",
      "****************************************************************************************************\n",
      "Iteration  205 : Loss =  0.3120333   Acc:  0.97868\n",
      "            : Test Loss :0.372130423784256 Test Accuracy : 0.9746400117874146 \n",
      "****************************************************************************************************\n",
      "Iteration  206 : Loss =  0.3121023   Acc:  0.97854\n",
      "            : Test Loss :0.37309008836746216 Test Accuracy : 0.9747300148010254 \n",
      "****************************************************************************************************\n",
      "Iteration  207 : Loss =  0.31251758   Acc:  0.97861\n",
      "            : Test Loss :0.37348076701164246 Test Accuracy : 0.9745699763298035 \n",
      "****************************************************************************************************\n",
      "Iteration  208 : Loss =  0.3131845   Acc:  0.97841835\n",
      "            : Test Loss :0.37580421566963196 Test Accuracy : 0.9744300246238708 \n",
      "****************************************************************************************************\n",
      "Iteration  209 : Loss =  0.31458458   Acc:  0.9783817\n",
      "            : Test Loss :0.3784898817539215 Test Accuracy : 0.9739500284194946 \n",
      "****************************************************************************************************\n",
      "Iteration  210 : Loss =  0.31781602   Acc:  0.978005\n",
      "            : Test Loss :0.3790881931781769 Test Accuracy : 0.9742400050163269 \n",
      "****************************************************************************************************\n",
      "Iteration  211 : Loss =  0.3173047   Acc:  0.978025\n",
      "            : Test Loss :0.3760313093662262 Test Accuracy : 0.9742199778556824 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  212 : Loss =  0.31504273   Acc:  0.9782183\n",
      "            : Test Loss :0.3704278767108917 Test Accuracy : 0.97461998462677 \n",
      "****************************************************************************************************\n",
      "Iteration  213 : Loss =  0.30868956   Acc:  0.978825\n",
      "            : Test Loss :0.36864423751831055 Test Accuracy : 0.9747700095176697 \n",
      "****************************************************************************************************\n",
      "Iteration  214 : Loss =  0.30685782   Acc:  0.97886336\n",
      "            : Test Loss :0.3708157539367676 Test Accuracy : 0.9747099876403809 \n",
      "****************************************************************************************************\n",
      "Iteration  215 : Loss =  0.30917937   Acc:  0.97867835\n",
      "            : Test Loss :0.37205564975738525 Test Accuracy : 0.9745399951934814 \n",
      "****************************************************************************************************\n",
      "Iteration  216 : Loss =  0.309559   Acc:  0.9786583\n",
      "            : Test Loss :0.3696650564670563 Test Accuracy : 0.9748700261116028 \n",
      "****************************************************************************************************\n",
      "Iteration  217 : Loss =  0.3075463   Acc:  0.97879165\n",
      "            : Test Loss :0.3681333363056183 Test Accuracy : 0.9748700261116028 \n",
      "****************************************************************************************************\n",
      "Iteration  218 : Loss =  0.3054231   Acc:  0.97912335\n",
      "            : Test Loss :0.36825108528137207 Test Accuracy : 0.9747499823570251 \n",
      "****************************************************************************************************\n",
      "Iteration  219 : Loss =  0.3052552   Acc:  0.9790317\n",
      "            : Test Loss :0.3683238625526428 Test Accuracy : 0.9748499989509583 \n",
      "****************************************************************************************************\n",
      "Iteration  220 : Loss =  0.3054923   Acc:  0.97898\n",
      "            : Test Loss :0.36781930923461914 Test Accuracy : 0.9748600125312805 \n",
      "****************************************************************************************************\n",
      "Iteration  221 : Loss =  0.30433246   Acc:  0.97913665\n",
      "            : Test Loss :0.36669835448265076 Test Accuracy : 0.9749199748039246 \n",
      "****************************************************************************************************\n",
      "Iteration  222 : Loss =  0.3033145   Acc:  0.97906667\n",
      "            : Test Loss :0.3668791651725769 Test Accuracy : 0.9749299883842468 \n",
      "****************************************************************************************************\n",
      "Iteration  223 : Loss =  0.30317754   Acc:  0.9791883\n",
      "            : Test Loss :0.36643075942993164 Test Accuracy : 0.9750199913978577 \n",
      "****************************************************************************************************\n",
      "Iteration  224 : Loss =  0.30248314   Acc:  0.979185\n",
      "            : Test Loss :0.3651708662509918 Test Accuracy : 0.9750900268554688 \n",
      "****************************************************************************************************\n",
      "Iteration  225 : Loss =  0.30128202   Acc:  0.979275\n",
      "            : Test Loss :0.3646577298641205 Test Accuracy : 0.9752100110054016 \n",
      "****************************************************************************************************\n",
      "Iteration  226 : Loss =  0.30031776   Acc:  0.97941\n",
      "            : Test Loss :0.364656537771225 Test Accuracy : 0.9751499891281128 \n",
      "****************************************************************************************************\n",
      "Iteration  227 : Loss =  0.30028072   Acc:  0.97934\n",
      "            : Test Loss :0.3653295338153839 Test Accuracy : 0.9751200079917908 \n",
      "****************************************************************************************************\n",
      "Iteration  228 : Loss =  0.30062383   Acc:  0.97938335\n",
      "            : Test Loss :0.36455920338630676 Test Accuracy : 0.9750800132751465 \n",
      "****************************************************************************************************\n",
      "Iteration  229 : Loss =  0.2996099   Acc:  0.97941\n",
      "            : Test Loss :0.3632362484931946 Test Accuracy : 0.9753699898719788 \n",
      "****************************************************************************************************\n",
      "Iteration  230 : Loss =  0.29816687   Acc:  0.9795383\n",
      "            : Test Loss :0.3624226152896881 Test Accuracy : 0.9753599762916565 \n",
      "****************************************************************************************************\n",
      "Iteration  231 : Loss =  0.29712525   Acc:  0.979595\n",
      "            : Test Loss :0.36253035068511963 Test Accuracy : 0.9752500057220459 \n",
      "****************************************************************************************************\n",
      "Iteration  232 : Loss =  0.2969889   Acc:  0.9795883\n",
      "            : Test Loss :0.36289751529693604 Test Accuracy : 0.9753599762916565 \n",
      "****************************************************************************************************\n",
      "Iteration  233 : Loss =  0.29715788   Acc:  0.9796633\n",
      "            : Test Loss :0.3625841438770294 Test Accuracy : 0.9753400087356567 \n",
      "****************************************************************************************************\n",
      "Iteration  234 : Loss =  0.2966222   Acc:  0.9796017\n",
      "            : Test Loss :0.3619520664215088 Test Accuracy : 0.9754599928855896 \n",
      "****************************************************************************************************\n",
      "Iteration  235 : Loss =  0.2957523   Acc:  0.97980833\n",
      "            : Test Loss :0.36110174655914307 Test Accuracy : 0.9754300117492676 \n",
      "****************************************************************************************************\n",
      "Iteration  236 : Loss =  0.2947986   Acc:  0.9797183\n",
      "            : Test Loss :0.3609718680381775 Test Accuracy : 0.9755399823188782 \n",
      "****************************************************************************************************\n",
      "Iteration  237 : Loss =  0.29427344   Acc:  0.97981\n",
      "            : Test Loss :0.36071938276290894 Test Accuracy : 0.9754499793052673 \n",
      "****************************************************************************************************\n",
      "Iteration  238 : Loss =  0.29399082   Acc:  0.979725\n",
      "            : Test Loss :0.36078882217407227 Test Accuracy : 0.9753000140190125 \n",
      "****************************************************************************************************\n",
      "Iteration  239 : Loss =  0.29355735   Acc:  0.979825\n",
      "            : Test Loss :0.3601524531841278 Test Accuracy : 0.9755499958992004 \n",
      "****************************************************************************************************\n",
      "Iteration  240 : Loss =  0.29288074   Acc:  0.9798783\n",
      "            : Test Loss :0.35968971252441406 Test Accuracy : 0.9755399823188782 \n",
      "****************************************************************************************************\n",
      "Iteration  241 : Loss =  0.29210958   Acc:  0.97991\n",
      "            : Test Loss :0.359347939491272 Test Accuracy : 0.9756600260734558 \n",
      "****************************************************************************************************\n",
      "Iteration  242 : Loss =  0.2915097   Acc:  0.9800183\n",
      "            : Test Loss :0.35904476046562195 Test Accuracy : 0.9755499958992004 \n",
      "****************************************************************************************************\n",
      "Iteration  243 : Loss =  0.29113933   Acc:  0.97997\n",
      "            : Test Loss :0.3592420518398285 Test Accuracy : 0.9755100011825562 \n",
      "****************************************************************************************************\n",
      "Iteration  244 : Loss =  0.29088598   Acc:  0.980015\n",
      "            : Test Loss :0.3588721454143524 Test Accuracy : 0.9755399823188782 \n",
      "****************************************************************************************************\n",
      "Iteration  245 : Loss =  0.29063985   Acc:  0.97993666\n",
      "            : Test Loss :0.3590855598449707 Test Accuracy : 0.9755899906158447 \n",
      "****************************************************************************************************\n",
      "Iteration  246 : Loss =  0.29024684   Acc:  0.98011\n",
      "            : Test Loss :0.35847845673561096 Test Accuracy : 0.9755499958992004 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  247 : Loss =  0.28985938   Acc:  0.979985\n",
      "            : Test Loss :0.35873866081237793 Test Accuracy : 0.9758099913597107 \n",
      "****************************************************************************************************\n",
      "Iteration  248 : Loss =  0.28945884   Acc:  0.9801717\n",
      "            : Test Loss :0.358368843793869 Test Accuracy : 0.975570023059845 \n",
      "****************************************************************************************************\n",
      "Iteration  249 : Loss =  0.28932878   Acc:  0.980065\n",
      "            : Test Loss :0.35930365324020386 Test Accuracy : 0.9757800102233887 \n",
      "****************************************************************************************************\n",
      "Iteration  250 : Loss =  0.2895676   Acc:  0.98012\n",
      "            : Test Loss :0.35944756865501404 Test Accuracy : 0.975380003452301 \n",
      "****************************************************************************************************\n",
      "Iteration  251 : Loss =  0.2899977   Acc:  0.979955\n",
      "            : Test Loss :0.3615868091583252 Test Accuracy : 0.9755399823188782 \n",
      "****************************************************************************************************\n",
      "Iteration  252 : Loss =  0.29133973   Acc:  0.9799233\n",
      "            : Test Loss :0.361206978559494 Test Accuracy : 0.9751399755477905 \n",
      "****************************************************************************************************\n",
      "Iteration  253 : Loss =  0.29138803   Acc:  0.97979164\n",
      "            : Test Loss :0.3628665506839752 Test Accuracy : 0.9756100177764893 \n",
      "****************************************************************************************************\n",
      "Iteration  254 : Loss =  0.2921478   Acc:  0.9797817\n",
      "            : Test Loss :0.3599264919757843 Test Accuracy : 0.9751499891281128 \n",
      "****************************************************************************************************\n",
      "Iteration  255 : Loss =  0.289774   Acc:  0.9799517\n",
      "            : Test Loss :0.35834652185440063 Test Accuracy : 0.9758099913597107 \n",
      "****************************************************************************************************\n",
      "Iteration  256 : Loss =  0.28739148   Acc:  0.98022\n",
      "            : Test Loss :0.3557571768760681 Test Accuracy : 0.9758099913597107 \n",
      "****************************************************************************************************\n",
      "Iteration  257 : Loss =  0.28514668   Acc:  0.98026\n",
      "            : Test Loss :0.3553260862827301 Test Accuracy : 0.9757999777793884 \n",
      "****************************************************************************************************\n",
      "Iteration  258 : Loss =  0.2841102   Acc:  0.980445\n",
      "            : Test Loss :0.35561761260032654 Test Accuracy : 0.9758300185203552 \n",
      "****************************************************************************************************\n",
      "Iteration  259 : Loss =  0.28430942   Acc:  0.98051834\n",
      "            : Test Loss :0.3562169075012207 Test Accuracy : 0.9757199883460999 \n",
      "****************************************************************************************************\n",
      "Iteration  260 : Loss =  0.28470227   Acc:  0.9803783\n",
      "            : Test Loss :0.35726019740104675 Test Accuracy : 0.9759200215339661 \n",
      "****************************************************************************************************\n",
      "Iteration  261 : Loss =  0.28527892   Acc:  0.98037165\n",
      "            : Test Loss :0.35607820749282837 Test Accuracy : 0.9756199717521667 \n",
      "****************************************************************************************************\n",
      "Iteration  262 : Loss =  0.28429094   Acc:  0.980365\n",
      "            : Test Loss :0.3557124137878418 Test Accuracy : 0.9759500026702881 \n",
      "****************************************************************************************************\n",
      "Iteration  263 : Loss =  0.28326848   Acc:  0.98054165\n",
      "            : Test Loss :0.3542213439941406 Test Accuracy : 0.9758399724960327 \n",
      "****************************************************************************************************\n",
      "Iteration  264 : Loss =  0.28209135   Acc:  0.98047835\n",
      "            : Test Loss :0.35415157675743103 Test Accuracy : 0.9758999943733215 \n",
      "****************************************************************************************************\n",
      "Iteration  265 : Loss =  0.28130868   Acc:  0.98073\n",
      "            : Test Loss :0.3537077307701111 Test Accuracy : 0.9759899973869324 \n",
      "****************************************************************************************************\n",
      "Iteration  266 : Loss =  0.28108367   Acc:  0.980585\n",
      "            : Test Loss :0.35427147150039673 Test Accuracy : 0.9758700132369995 \n",
      "****************************************************************************************************\n",
      "Iteration  267 : Loss =  0.28100964   Acc:  0.98066\n",
      "            : Test Loss :0.35410425066947937 Test Accuracy : 0.9759399890899658 \n",
      "****************************************************************************************************\n",
      "Iteration  268 : Loss =  0.28091624   Acc:  0.9806517\n",
      "            : Test Loss :0.35380318760871887 Test Accuracy : 0.9758999943733215 \n",
      "****************************************************************************************************\n",
      "Iteration  269 : Loss =  0.280195   Acc:  0.980705\n",
      "            : Test Loss :0.3531166911125183 Test Accuracy : 0.9760400056838989 \n",
      "****************************************************************************************************\n",
      "Iteration  270 : Loss =  0.27943718   Acc:  0.98081\n",
      "            : Test Loss :0.3522179126739502 Test Accuracy : 0.9760500192642212 \n",
      "****************************************************************************************************\n",
      "Iteration  271 : Loss =  0.2783369   Acc:  0.980845\n",
      "            : Test Loss :0.3516848683357239 Test Accuracy : 0.9761499762535095 \n",
      "****************************************************************************************************\n",
      "Iteration  272 : Loss =  0.27752265   Acc:  0.9809383\n",
      "            : Test Loss :0.3511713743209839 Test Accuracy : 0.9761899709701538 \n",
      "****************************************************************************************************\n",
      "Iteration  273 : Loss =  0.277003   Acc:  0.9809\n",
      "            : Test Loss :0.3515021800994873 Test Accuracy : 0.9761899709701538 \n",
      "****************************************************************************************************\n",
      "Iteration  274 : Loss =  0.27679297   Acc:  0.98100334\n",
      "            : Test Loss :0.3513638973236084 Test Accuracy : 0.9760500192642212 \n",
      "****************************************************************************************************\n",
      "Iteration  275 : Loss =  0.2768239   Acc:  0.98086834\n",
      "            : Test Loss :0.352263480424881 Test Accuracy : 0.9759600162506104 \n",
      "****************************************************************************************************\n",
      "Iteration  276 : Loss =  0.27694288   Acc:  0.98094\n",
      "            : Test Loss :0.3520824611186981 Test Accuracy : 0.9759799838066101 \n",
      "****************************************************************************************************\n",
      "Iteration  277 : Loss =  0.27713832   Acc:  0.9807583\n",
      "            : Test Loss :0.35311299562454224 Test Accuracy : 0.9758999943733215 \n",
      "****************************************************************************************************\n",
      "Iteration  278 : Loss =  0.27717024   Acc:  0.98089\n",
      "            : Test Loss :0.3524334132671356 Test Accuracy : 0.9759100079536438 \n",
      "****************************************************************************************************\n",
      "Iteration  279 : Loss =  0.2771058   Acc:  0.9807467\n",
      "            : Test Loss :0.3530665636062622 Test Accuracy : 0.9758399724960327 \n",
      "****************************************************************************************************\n",
      "Iteration  280 : Loss =  0.2765824   Acc:  0.9809567\n",
      "            : Test Loss :0.3516511917114258 Test Accuracy : 0.9758300185203552 \n",
      "****************************************************************************************************\n",
      "Iteration  281 : Loss =  0.27594212   Acc:  0.980845\n",
      "            : Test Loss :0.35179901123046875 Test Accuracy : 0.9760400056838989 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  282 : Loss =  0.27493972   Acc:  0.9810917\n",
      "            : Test Loss :0.3505876064300537 Test Accuracy : 0.9759699702262878 \n",
      "****************************************************************************************************\n",
      "Iteration  283 : Loss =  0.2743773   Acc:  0.98095\n",
      "            : Test Loss :0.3513045310974121 Test Accuracy : 0.9763399958610535 \n",
      "****************************************************************************************************\n",
      "Iteration  284 : Loss =  0.27416205   Acc:  0.98115164\n",
      "            : Test Loss :0.35131844878196716 Test Accuracy : 0.975849986076355 \n",
      "****************************************************************************************************\n",
      "Iteration  285 : Loss =  0.2744935   Acc:  0.980985\n",
      "            : Test Loss :0.3533703684806824 Test Accuracy : 0.9761199951171875 \n",
      "****************************************************************************************************\n",
      "Iteration  286 : Loss =  0.27588797   Acc:  0.98099\n",
      "            : Test Loss :0.3530660569667816 Test Accuracy : 0.9755799770355225 \n",
      "****************************************************************************************************\n",
      "Iteration  287 : Loss =  0.2756473   Acc:  0.98087335\n",
      "            : Test Loss :0.354281485080719 Test Accuracy : 0.9760299921035767 \n",
      "****************************************************************************************************\n",
      "Iteration  288 : Loss =  0.2764194   Acc:  0.98087835\n",
      "            : Test Loss :0.35139918327331543 Test Accuracy : 0.9758800268173218 \n",
      "****************************************************************************************************\n",
      "Iteration  289 : Loss =  0.27358222   Acc:  0.98104334\n",
      "            : Test Loss :0.3496958911418915 Test Accuracy : 0.9762700200080872 \n",
      "****************************************************************************************************\n",
      "Iteration  290 : Loss =  0.27153203   Acc:  0.98132664\n",
      "            : Test Loss :0.3477526605129242 Test Accuracy : 0.9762600064277649 \n",
      "****************************************************************************************************\n",
      "Iteration  291 : Loss =  0.26964936   Acc:  0.981445\n",
      "            : Test Loss :0.3477994501590729 Test Accuracy : 0.9763399958610535 \n",
      "****************************************************************************************************\n",
      "Iteration  292 : Loss =  0.26911968   Acc:  0.9815033\n",
      "            : Test Loss :0.34817472100257874 Test Accuracy : 0.9760800004005432 \n",
      "****************************************************************************************************\n",
      "Iteration  293 : Loss =  0.26957175   Acc:  0.9813833\n",
      "            : Test Loss :0.34930098056793213 Test Accuracy : 0.9760400056838989 \n",
      "****************************************************************************************************\n",
      "Iteration  294 : Loss =  0.2700114   Acc:  0.9813967\n",
      "            : Test Loss :0.3495270609855652 Test Accuracy : 0.9762399792671204 \n",
      "****************************************************************************************************\n",
      "Iteration  295 : Loss =  0.27032667   Acc:  0.9813067\n",
      "            : Test Loss :0.348865807056427 Test Accuracy : 0.9760500192642212 \n",
      "****************************************************************************************************\n",
      "Iteration  296 : Loss =  0.26920763   Acc:  0.98146\n",
      "            : Test Loss :0.3478633761405945 Test Accuracy : 0.9763000011444092 \n",
      "****************************************************************************************************\n",
      "Iteration  297 : Loss =  0.26814932   Acc:  0.98148\n",
      "            : Test Loss :0.3467440903186798 Test Accuracy : 0.9763299822807312 \n",
      "****************************************************************************************************\n",
      "Iteration  298 : Loss =  0.26690885   Acc:  0.981595\n",
      "            : Test Loss :0.3466070294380188 Test Accuracy : 0.976419985294342 \n",
      "****************************************************************************************************\n",
      "Iteration  299 : Loss =  0.2663144   Acc:  0.981715\n",
      "            : Test Loss :0.34642189741134644 Test Accuracy : 0.9761800169944763 \n",
      "****************************************************************************************************\n",
      "Iteration  300 : Loss =  0.2663913   Acc:  0.9815817\n",
      "            : Test Loss :0.3479231297969818 Test Accuracy : 0.9761199951171875 \n",
      "****************************************************************************************************\n",
      "Iteration  301 : Loss =  0.2668901   Acc:  0.9815933\n",
      "            : Test Loss :0.3479783236980438 Test Accuracy : 0.9760100245475769 \n",
      "****************************************************************************************************\n",
      "Iteration  302 : Loss =  0.26770326   Acc:  0.98139834\n",
      "            : Test Loss :0.3500707745552063 Test Accuracy : 0.9758999943733215 \n",
      "****************************************************************************************************\n",
      "Iteration  303 : Loss =  0.26834103   Acc:  0.9815083\n",
      "            : Test Loss :0.3496171534061432 Test Accuracy : 0.9758300185203552 \n",
      "****************************************************************************************************\n",
      "Iteration  304 : Loss =  0.26915163   Acc:  0.981195\n",
      "            : Test Loss :0.3512527048587799 Test Accuracy : 0.9757999777793884 \n",
      "****************************************************************************************************\n",
      "Iteration  305 : Loss =  0.2690024   Acc:  0.981405\n",
      "            : Test Loss :0.35021546483039856 Test Accuracy : 0.9759100079536438 \n",
      "****************************************************************************************************\n",
      "Iteration  306 : Loss =  0.26952663   Acc:  0.98115\n",
      "            : Test Loss :0.35103192925453186 Test Accuracy : 0.9762799739837646 \n",
      "****************************************************************************************************\n",
      "Iteration  307 : Loss =  0.26858202   Acc:  0.98141\n",
      "            : Test Loss :0.34971553087234497 Test Accuracy : 0.9755600094795227 \n",
      "****************************************************************************************************\n",
      "Iteration  308 : Loss =  0.26832184   Acc:  0.9812383\n",
      "            : Test Loss :0.35084986686706543 Test Accuracy : 0.9761599898338318 \n",
      "****************************************************************************************************\n",
      "Iteration  309 : Loss =  0.26843658   Acc:  0.98136836\n",
      "            : Test Loss :0.34842729568481445 Test Accuracy : 0.9760100245475769 \n",
      "****************************************************************************************************\n",
      "Iteration  310 : Loss =  0.2659524   Acc:  0.98157\n",
      "            : Test Loss :0.3468208909034729 Test Accuracy : 0.9762899875640869 \n",
      "****************************************************************************************************\n",
      "Iteration  311 : Loss =  0.26445812   Acc:  0.98162836\n",
      "            : Test Loss :0.3456372618675232 Test Accuracy : 0.9763299822807312 \n",
      "****************************************************************************************************\n",
      "Iteration  312 : Loss =  0.26243523   Acc:  0.98187333\n",
      "            : Test Loss :0.3445887565612793 Test Accuracy : 0.9762300252914429 \n",
      "****************************************************************************************************\n",
      "Iteration  313 : Loss =  0.26186413   Acc:  0.981865\n",
      "            : Test Loss :0.3462376296520233 Test Accuracy : 0.9765700101852417 \n",
      "****************************************************************************************************\n",
      "Iteration  314 : Loss =  0.26257914   Acc:  0.981895\n",
      "            : Test Loss :0.3468748927116394 Test Accuracy : 0.9759500026702881 \n",
      "****************************************************************************************************\n",
      "Iteration  315 : Loss =  0.26337916   Acc:  0.9817167\n",
      "            : Test Loss :0.34812062978744507 Test Accuracy : 0.9763000011444092 \n",
      "****************************************************************************************************\n",
      "Iteration  316 : Loss =  0.26409498   Acc:  0.9816783\n",
      "            : Test Loss :0.34604334831237793 Test Accuracy : 0.9761300086975098 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  317 : Loss =  0.26194552   Acc:  0.98184836\n",
      "            : Test Loss :0.3442780673503876 Test Accuracy : 0.9765300154685974 \n",
      "****************************************************************************************************\n",
      "Iteration  318 : Loss =  0.26004142   Acc:  0.98200834\n",
      "            : Test Loss :0.3430285155773163 Test Accuracy : 0.9765400290489197 \n",
      "****************************************************************************************************\n",
      "Iteration  319 : Loss =  0.25854513   Acc:  0.98218167\n",
      "            : Test Loss :0.34297698736190796 Test Accuracy : 0.9765099883079529 \n",
      "****************************************************************************************************\n",
      "Iteration  320 : Loss =  0.25837412   Acc:  0.9821633\n",
      "            : Test Loss :0.34413617849349976 Test Accuracy : 0.9765700101852417 \n",
      "****************************************************************************************************\n",
      "Iteration  321 : Loss =  0.25911757   Acc:  0.98204666\n",
      "            : Test Loss :0.34472525119781494 Test Accuracy : 0.9762600064277649 \n",
      "****************************************************************************************************\n",
      "Iteration  322 : Loss =  0.2595316   Acc:  0.98201835\n",
      "            : Test Loss :0.3452816605567932 Test Accuracy : 0.976580023765564 \n",
      "****************************************************************************************************\n",
      "Iteration  323 : Loss =  0.25966427   Acc:  0.9820217\n",
      "            : Test Loss :0.34404605627059937 Test Accuracy : 0.9763200283050537 \n",
      "****************************************************************************************************\n",
      "Iteration  324 : Loss =  0.25845242   Acc:  0.9821067\n",
      "            : Test Loss :0.34343671798706055 Test Accuracy : 0.976580023765564 \n",
      "****************************************************************************************************\n",
      "Iteration  325 : Loss =  0.25731254   Acc:  0.9822367\n",
      "            : Test Loss :0.3422926962375641 Test Accuracy : 0.9764400124549866 \n",
      "****************************************************************************************************\n",
      "Iteration  326 : Loss =  0.25644085   Acc:  0.9822717\n",
      "            : Test Loss :0.3426366150379181 Test Accuracy : 0.9765499830245972 \n",
      "****************************************************************************************************\n",
      "Iteration  327 : Loss =  0.25610515   Acc:  0.9823717\n",
      "            : Test Loss :0.342306911945343 Test Accuracy : 0.976580023765564 \n",
      "****************************************************************************************************\n",
      "Iteration  328 : Loss =  0.2561394   Acc:  0.98221\n",
      "            : Test Loss :0.3431233763694763 Test Accuracy : 0.9763299822807312 \n",
      "****************************************************************************************************\n",
      "Iteration  329 : Loss =  0.25612414   Acc:  0.98234665\n",
      "            : Test Loss :0.34262409806251526 Test Accuracy : 0.9766299724578857 \n",
      "****************************************************************************************************\n",
      "Iteration  330 : Loss =  0.25593984   Acc:  0.9822183\n",
      "            : Test Loss :0.34269943833351135 Test Accuracy : 0.9764800071716309 \n",
      "****************************************************************************************************\n",
      "Iteration  331 : Loss =  0.25526726   Acc:  0.98245835\n",
      "            : Test Loss :0.34182220697402954 Test Accuracy : 0.9766200184822083 \n",
      "****************************************************************************************************\n",
      "Iteration  332 : Loss =  0.2545202   Acc:  0.9823533\n",
      "            : Test Loss :0.3414875566959381 Test Accuracy : 0.9765499830245972 \n",
      "****************************************************************************************************\n",
      "Iteration  333 : Loss =  0.2537157   Acc:  0.98253834\n",
      "            : Test Loss :0.34100013971328735 Test Accuracy : 0.9767000079154968 \n",
      "****************************************************************************************************\n",
      "Iteration  334 : Loss =  0.25312334   Acc:  0.9825283\n",
      "            : Test Loss :0.34080609679222107 Test Accuracy : 0.9765599966049194 \n",
      "****************************************************************************************************\n",
      "Iteration  335 : Loss =  0.25276458   Acc:  0.982525\n",
      "            : Test Loss :0.34108665585517883 Test Accuracy : 0.9766600131988525 \n",
      "****************************************************************************************************\n",
      "Iteration  336 : Loss =  0.2525933   Acc:  0.98264\n",
      "            : Test Loss :0.3409118354320526 Test Accuracy : 0.9767299890518188 \n",
      "****************************************************************************************************\n",
      "Iteration  337 : Loss =  0.25253478   Acc:  0.9824733\n",
      "            : Test Loss :0.34164315462112427 Test Accuracy : 0.9765400290489197 \n",
      "****************************************************************************************************\n",
      "Iteration  338 : Loss =  0.25251138   Acc:  0.9826083\n",
      "            : Test Loss :0.3412611186504364 Test Accuracy : 0.976859986782074 \n",
      "****************************************************************************************************\n",
      "Iteration  339 : Loss =  0.25256154   Acc:  0.9824283\n",
      "            : Test Loss :0.34237486124038696 Test Accuracy : 0.9764699935913086 \n",
      "****************************************************************************************************\n",
      "Iteration  340 : Loss =  0.2526445   Acc:  0.9826083\n",
      "            : Test Loss :0.3418666422367096 Test Accuracy : 0.9766700267791748 \n",
      "****************************************************************************************************\n",
      "Iteration  341 : Loss =  0.2529605   Acc:  0.9823933\n",
      "            : Test Loss :0.3439165949821472 Test Accuracy : 0.9763000011444092 \n",
      "****************************************************************************************************\n",
      "Iteration  342 : Loss =  0.25348082   Acc:  0.9825367\n",
      "            : Test Loss :0.34385058283805847 Test Accuracy : 0.9762300252914429 \n",
      "****************************************************************************************************\n",
      "Iteration  343 : Loss =  0.25479743   Acc:  0.9822317\n",
      "            : Test Loss :0.3474556505680084 Test Accuracy : 0.9762200117111206 \n",
      "****************************************************************************************************\n",
      "Iteration  344 : Loss =  0.25619194   Acc:  0.98216665\n",
      "            : Test Loss :0.3487275540828705 Test Accuracy : 0.9759399890899658 \n",
      "****************************************************************************************************\n",
      "Iteration  345 : Loss =  0.25960258   Acc:  0.981805\n",
      "            : Test Loss :0.3517777621746063 Test Accuracy : 0.9764000177383423 \n",
      "****************************************************************************************************\n",
      "Iteration  346 : Loss =  0.2599402   Acc:  0.98175836\n",
      "            : Test Loss :0.3510504961013794 Test Accuracy : 0.9753199815750122 \n",
      "****************************************************************************************************\n",
      "Iteration  347 : Loss =  0.2615509   Acc:  0.98160166\n",
      "            : Test Loss :0.34968990087509155 Test Accuracy : 0.9763200283050537 \n",
      "****************************************************************************************************\n",
      "Iteration  348 : Loss =  0.25830278   Acc:  0.981895\n",
      "            : Test Loss :0.34507593512535095 Test Accuracy : 0.9760599732398987 \n",
      "****************************************************************************************************\n",
      "Iteration  349 : Loss =  0.2541418   Acc:  0.98230666\n",
      "            : Test Loss :0.3449455201625824 Test Accuracy : 0.976639986038208 \n",
      "****************************************************************************************************\n",
      "Iteration  350 : Loss =  0.25452256   Acc:  0.98222667\n",
      "            : Test Loss :0.3463437855243683 Test Accuracy : 0.9761899709701538 \n",
      "****************************************************************************************************\n",
      "Iteration  351 : Loss =  0.2539799   Acc:  0.98240334\n",
      "            : Test Loss :0.34280332922935486 Test Accuracy : 0.9764400124549866 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  352 : Loss =  0.25217998   Acc:  0.98239166\n",
      "            : Test Loss :0.34125301241874695 Test Accuracy : 0.9770699739456177 \n",
      "****************************************************************************************************\n",
      "Iteration  353 : Loss =  0.24907863   Acc:  0.9827433\n",
      "            : Test Loss :0.3413553237915039 Test Accuracy : 0.9764500260353088 \n",
      "****************************************************************************************************\n",
      "Iteration  354 : Loss =  0.24907245   Acc:  0.98275167\n",
      "            : Test Loss :0.3435932695865631 Test Accuracy : 0.9766299724578857 \n",
      "****************************************************************************************************\n",
      "Iteration  355 : Loss =  0.25179386   Acc:  0.98244333\n",
      "            : Test Loss :0.3451072871685028 Test Accuracy : 0.9762799739837646 \n",
      "****************************************************************************************************\n",
      "Iteration  356 : Loss =  0.2515045   Acc:  0.98261\n",
      "            : Test Loss :0.3408830463886261 Test Accuracy : 0.976859986782074 \n",
      "****************************************************************************************************\n",
      "Iteration  357 : Loss =  0.24871357   Acc:  0.9825817\n",
      "            : Test Loss :0.3390105366706848 Test Accuracy : 0.9770100116729736 \n",
      "****************************************************************************************************\n",
      "Iteration  358 : Loss =  0.24583265   Acc:  0.9830833\n",
      "            : Test Loss :0.3393832743167877 Test Accuracy : 0.9769300222396851 \n",
      "****************************************************************************************************\n",
      "Iteration  359 : Loss =  0.24604969   Acc:  0.98300165\n",
      "            : Test Loss :0.34109070897102356 Test Accuracy : 0.9769700169563293 \n",
      "****************************************************************************************************\n",
      "Iteration  360 : Loss =  0.24811855   Acc:  0.982715\n",
      "            : Test Loss :0.34268608689308167 Test Accuracy : 0.9765700101852417 \n",
      "****************************************************************************************************\n",
      "Iteration  361 : Loss =  0.24824832   Acc:  0.98280835\n",
      "            : Test Loss :0.3399762213230133 Test Accuracy : 0.9770900011062622 \n",
      "****************************************************************************************************\n",
      "Iteration  362 : Loss =  0.2463783   Acc:  0.98284835\n",
      "            : Test Loss :0.33848369121551514 Test Accuracy : 0.9769999980926514 \n",
      "****************************************************************************************************\n",
      "Iteration  363 : Loss =  0.24422835   Acc:  0.9831617\n",
      "            : Test Loss :0.33883047103881836 Test Accuracy : 0.9771599769592285 \n",
      "****************************************************************************************************\n",
      "Iteration  364 : Loss =  0.24404553   Acc:  0.9831783\n",
      "            : Test Loss :0.3392699658870697 Test Accuracy : 0.9769799709320068 \n",
      "****************************************************************************************************\n",
      "Iteration  365 : Loss =  0.24504234   Acc:  0.98288333\n",
      "            : Test Loss :0.3406429886817932 Test Accuracy : 0.9767600297927856 \n",
      "****************************************************************************************************\n",
      "Iteration  366 : Loss =  0.24510713   Acc:  0.983125\n",
      "            : Test Loss :0.33860453963279724 Test Accuracy : 0.9771699905395508 \n",
      "****************************************************************************************************\n",
      "Iteration  367 : Loss =  0.24390653   Acc:  0.983045\n",
      "            : Test Loss :0.3378432095050812 Test Accuracy : 0.9770100116729736 \n",
      "****************************************************************************************************\n",
      "Iteration  368 : Loss =  0.24264319   Acc:  0.98327\n",
      "            : Test Loss :0.33819252252578735 Test Accuracy : 0.9772599935531616 \n",
      "****************************************************************************************************\n",
      "Iteration  369 : Loss =  0.2425622   Acc:  0.98329\n",
      "            : Test Loss :0.33816879987716675 Test Accuracy : 0.9768499732017517 \n",
      "****************************************************************************************************\n",
      "Iteration  370 : Loss =  0.24318005   Acc:  0.98306835\n",
      "            : Test Loss :0.3394405245780945 Test Accuracy : 0.9769700169563293 \n",
      "****************************************************************************************************\n",
      "Iteration  371 : Loss =  0.2430694   Acc:  0.98328\n",
      "            : Test Loss :0.33777496218681335 Test Accuracy : 0.9769099950790405 \n",
      "****************************************************************************************************\n",
      "Iteration  372 : Loss =  0.24220353   Acc:  0.9831683\n",
      "            : Test Loss :0.33761048316955566 Test Accuracy : 0.9772700071334839 \n",
      "****************************************************************************************************\n",
      "Iteration  373 : Loss =  0.24109098   Acc:  0.98338336\n",
      "            : Test Loss :0.33720672130584717 Test Accuracy : 0.9771299958229065 \n",
      "****************************************************************************************************\n",
      "Iteration  374 : Loss =  0.24063727   Acc:  0.98341\n",
      "            : Test Loss :0.3373018503189087 Test Accuracy : 0.977180004119873 \n",
      "****************************************************************************************************\n",
      "Iteration  375 : Loss =  0.24075699   Acc:  0.98328334\n",
      "            : Test Loss :0.3382991552352905 Test Accuracy : 0.9770200252532959 \n",
      "****************************************************************************************************\n",
      "Iteration  376 : Loss =  0.24082704   Acc:  0.98346\n",
      "            : Test Loss :0.33743828535079956 Test Accuracy : 0.9771100282669067 \n",
      "****************************************************************************************************\n",
      "Iteration  377 : Loss =  0.24046029   Acc:  0.98330665\n",
      "            : Test Loss :0.3375852406024933 Test Accuracy : 0.9773200154304504 \n",
      "****************************************************************************************************\n",
      "Iteration  378 : Loss =  0.23974368   Acc:  0.983485\n",
      "            : Test Loss :0.33672401309013367 Test Accuracy : 0.977180004119873 \n",
      "****************************************************************************************************\n",
      "Iteration  379 : Loss =  0.23909779   Acc:  0.9834933\n",
      "            : Test Loss :0.33663374185562134 Test Accuracy : 0.9772899746894836 \n",
      "****************************************************************************************************\n",
      "Iteration  380 : Loss =  0.23876815   Acc:  0.9834667\n",
      "            : Test Loss :0.3370284140110016 Test Accuracy : 0.9772499799728394 \n",
      "****************************************************************************************************\n",
      "Iteration  381 : Loss =  0.23868877   Acc:  0.98355\n",
      "            : Test Loss :0.33670955896377563 Test Accuracy : 0.9771100282669067 \n",
      "****************************************************************************************************\n",
      "Iteration  382 : Loss =  0.23863444   Acc:  0.98344666\n",
      "            : Test Loss :0.3373083174228668 Test Accuracy : 0.9773200154304504 \n",
      "****************************************************************************************************\n",
      "Iteration  383 : Loss =  0.23842053   Acc:  0.9836067\n",
      "            : Test Loss :0.33658793568611145 Test Accuracy : 0.9771299958229065 \n",
      "****************************************************************************************************\n",
      "Iteration  384 : Loss =  0.23803554   Acc:  0.98349166\n",
      "            : Test Loss :0.336727112531662 Test Accuracy : 0.9772800207138062 \n",
      "****************************************************************************************************\n",
      "Iteration  385 : Loss =  0.23757477   Acc:  0.983655\n",
      "            : Test Loss :0.33644092082977295 Test Accuracy : 0.9772800207138062 \n",
      "****************************************************************************************************\n",
      "Iteration  386 : Loss =  0.23723793   Acc:  0.9836067\n",
      "            : Test Loss :0.3364589214324951 Test Accuracy : 0.9770600199699402 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  387 : Loss =  0.23712818   Acc:  0.9836317\n",
      "            : Test Loss :0.33746278285980225 Test Accuracy : 0.9774699807167053 \n",
      "****************************************************************************************************\n",
      "Iteration  388 : Loss =  0.2374308   Acc:  0.98370165\n",
      "            : Test Loss :0.33798983693122864 Test Accuracy : 0.9767500162124634 \n",
      "****************************************************************************************************\n",
      "Iteration  389 : Loss =  0.23841721   Acc:  0.983385\n",
      "            : Test Loss :0.3413427770137787 Test Accuracy : 0.9770399928092957 \n",
      "****************************************************************************************************\n",
      "Iteration  390 : Loss =  0.24048561   Acc:  0.983275\n",
      "            : Test Loss :0.3446064591407776 Test Accuracy : 0.9761199951171875 \n",
      "****************************************************************************************************\n",
      "Iteration  391 : Loss =  0.24481411   Acc:  0.98275\n",
      "            : Test Loss :0.35332030057907104 Test Accuracy : 0.9760400056838989 \n",
      "****************************************************************************************************\n",
      "Iteration  392 : Loss =  0.2516749   Acc:  0.982145\n",
      "            : Test Loss :0.3559480607509613 Test Accuracy : 0.9752799868583679 \n",
      "****************************************************************************************************\n",
      "Iteration  393 : Loss =  0.25561044   Acc:  0.98184335\n",
      "            : Test Loss :0.35783758759498596 Test Accuracy : 0.9757599830627441 \n",
      "****************************************************************************************************\n",
      "Iteration  394 : Loss =  0.2562075   Acc:  0.98186165\n",
      "            : Test Loss :0.3418051302433014 Test Accuracy : 0.9765599966049194 \n",
      "****************************************************************************************************\n",
      "Iteration  395 : Loss =  0.24094853   Acc:  0.983145\n",
      "            : Test Loss :0.33571088314056396 Test Accuracy : 0.9772099852561951 \n",
      "****************************************************************************************************\n",
      "Iteration  396 : Loss =  0.2353415   Acc:  0.9836817\n",
      "            : Test Loss :0.34132227301597595 Test Accuracy : 0.9771199822425842 \n",
      "****************************************************************************************************\n",
      "Iteration  397 : Loss =  0.23970218   Acc:  0.9833183\n",
      "            : Test Loss :0.3452681601047516 Test Accuracy : 0.9761499762535095 \n",
      "****************************************************************************************************\n",
      "Iteration  398 : Loss =  0.24434131   Acc:  0.982795\n",
      "            : Test Loss :0.3444060683250427 Test Accuracy : 0.9769499897956848 \n",
      "****************************************************************************************************\n",
      "Iteration  399 : Loss =  0.24239887   Acc:  0.98300666\n",
      "            : Test Loss :0.3379615545272827 Test Accuracy : 0.9770399928092957 \n",
      "****************************************************************************************************\n",
      "Iteration  400 : Loss =  0.23560533   Acc:  0.9836767\n",
      "            : Test Loss :0.33745089173316956 Test Accuracy : 0.9766899943351746 \n",
      "****************************************************************************************************\n",
      "Iteration  401 : Loss =  0.23588304   Acc:  0.983505\n",
      "            : Test Loss :0.3419680893421173 Test Accuracy : 0.9771999716758728 \n",
      "****************************************************************************************************\n",
      "Iteration  402 : Loss =  0.23872282   Acc:  0.98343\n",
      "            : Test Loss :0.3411811888217926 Test Accuracy : 0.9767100214958191 \n",
      "****************************************************************************************************\n",
      "Iteration  403 : Loss =  0.23894544   Acc:  0.98323\n",
      "            : Test Loss :0.33842727541923523 Test Accuracy : 0.9773600101470947 \n",
      "****************************************************************************************************\n",
      "Iteration  404 : Loss =  0.23549834   Acc:  0.9836417\n",
      "            : Test Loss :0.3366963863372803 Test Accuracy : 0.9774900078773499 \n",
      "****************************************************************************************************\n",
      "Iteration  405 : Loss =  0.23327038   Acc:  0.983855\n",
      "            : Test Loss :0.3376954197883606 Test Accuracy : 0.9769300222396851 \n",
      "****************************************************************************************************\n",
      "Iteration  406 : Loss =  0.23514725   Acc:  0.98353165\n",
      "            : Test Loss :0.3405579626560211 Test Accuracy : 0.9772800207138062 \n",
      "****************************************************************************************************\n",
      "Iteration  407 : Loss =  0.2362789   Acc:  0.98358667\n",
      "            : Test Loss :0.33777832984924316 Test Accuracy : 0.9768499732017517 \n",
      "****************************************************************************************************\n",
      "Iteration  408 : Loss =  0.23434879   Acc:  0.98368335\n",
      "            : Test Loss :0.3353820741176605 Test Accuracy : 0.9775400161743164 \n",
      "****************************************************************************************************\n",
      "Iteration  409 : Loss =  0.2314734   Acc:  0.98399\n",
      "            : Test Loss :0.33630239963531494 Test Accuracy : 0.9776399731636047 \n",
      "****************************************************************************************************\n",
      "Iteration  410 : Loss =  0.23167665   Acc:  0.9839683\n",
      "            : Test Loss :0.3377312421798706 Test Accuracy : 0.9768700003623962 \n",
      "****************************************************************************************************\n",
      "Iteration  411 : Loss =  0.23378587   Acc:  0.98371\n",
      "            : Test Loss :0.33869439363479614 Test Accuracy : 0.9773200154304504 \n",
      "****************************************************************************************************\n",
      "Iteration  412 : Loss =  0.23329252   Acc:  0.9838617\n",
      "            : Test Loss :0.3354547619819641 Test Accuracy : 0.9771100282669067 \n",
      "****************************************************************************************************\n",
      "Iteration  413 : Loss =  0.2310378   Acc:  0.9839183\n",
      "            : Test Loss :0.33456170558929443 Test Accuracy : 0.9774699807167053 \n",
      "****************************************************************************************************\n",
      "Iteration  414 : Loss =  0.22966996   Acc:  0.9841783\n",
      "            : Test Loss :0.33589625358581543 Test Accuracy : 0.9777500033378601 \n",
      "****************************************************************************************************\n",
      "Iteration  415 : Loss =  0.2304051   Acc:  0.9841167\n",
      "            : Test Loss :0.3362281620502472 Test Accuracy : 0.9771100282669067 \n",
      "****************************************************************************************************\n",
      "Iteration  416 : Loss =  0.2313932   Acc:  0.983875\n",
      "            : Test Loss :0.33660298585891724 Test Accuracy : 0.9776300191879272 \n",
      "****************************************************************************************************\n",
      "Iteration  417 : Loss =  0.23054737   Acc:  0.984105\n",
      "            : Test Loss :0.3345756232738495 Test Accuracy : 0.9773100018501282 \n",
      "****************************************************************************************************\n",
      "Iteration  418 : Loss =  0.22922884   Acc:  0.98415333\n",
      "            : Test Loss :0.334699422121048 Test Accuracy : 0.9775599837303162 \n",
      "****************************************************************************************************\n",
      "Iteration  419 : Loss =  0.22857268   Acc:  0.98424333\n",
      "            : Test Loss :0.33523988723754883 Test Accuracy : 0.977649986743927 \n",
      "****************************************************************************************************\n",
      "Iteration  420 : Loss =  0.2287854   Acc:  0.98427\n",
      "            : Test Loss :0.3352464735507965 Test Accuracy : 0.9771699905395508 \n",
      "****************************************************************************************************\n",
      "Iteration  421 : Loss =  0.22902784   Acc:  0.98409164\n",
      "            : Test Loss :0.33585700392723083 Test Accuracy : 0.9777399897575378 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  422 : Loss =  0.22865279   Acc:  0.984245\n",
      "            : Test Loss :0.3346629738807678 Test Accuracy : 0.9772099852561951 \n",
      "****************************************************************************************************\n",
      "Iteration  423 : Loss =  0.22790115   Acc:  0.98424834\n",
      "            : Test Loss :0.334553986787796 Test Accuracy : 0.9774500131607056 \n",
      "****************************************************************************************************\n",
      "Iteration  424 : Loss =  0.22719537   Acc:  0.9843183\n",
      "            : Test Loss :0.3343967795372009 Test Accuracy : 0.9776700139045715 \n",
      "****************************************************************************************************\n",
      "Iteration  425 : Loss =  0.22696993   Acc:  0.9843467\n",
      "            : Test Loss :0.3344806134700775 Test Accuracy : 0.9772199988365173 \n",
      "****************************************************************************************************\n",
      "Iteration  426 : Loss =  0.22709703   Acc:  0.9843017\n",
      "            : Test Loss :0.3352239429950714 Test Accuracy : 0.9777299761772156 \n",
      "****************************************************************************************************\n",
      "Iteration  427 : Loss =  0.22712363   Acc:  0.9843683\n",
      "            : Test Loss :0.3345573842525482 Test Accuracy : 0.9772999882698059 \n",
      "****************************************************************************************************\n",
      "Iteration  428 : Loss =  0.22674371   Acc:  0.984325\n",
      "            : Test Loss :0.3343774378299713 Test Accuracy : 0.9776700139045715 \n",
      "****************************************************************************************************\n",
      "Iteration  429 : Loss =  0.22609073   Acc:  0.9844217\n",
      "            : Test Loss :0.3340548276901245 Test Accuracy : 0.9774600267410278 \n",
      "****************************************************************************************************\n",
      "Iteration  430 : Loss =  0.22559   Acc:  0.9844883\n",
      "            : Test Loss :0.33383017778396606 Test Accuracy : 0.9774799942970276 \n",
      "****************************************************************************************************\n",
      "Iteration  431 : Loss =  0.22545238   Acc:  0.98441\n",
      "            : Test Loss :0.3347035050392151 Test Accuracy : 0.9777600169181824 \n",
      "****************************************************************************************************\n",
      "Iteration  432 : Loss =  0.22548014   Acc:  0.98446834\n",
      "            : Test Loss :0.33407723903656006 Test Accuracy : 0.9773399829864502 \n",
      "****************************************************************************************************\n",
      "Iteration  433 : Loss =  0.22541289   Acc:  0.984385\n",
      "            : Test Loss :0.33480316400527954 Test Accuracy : 0.9778500199317932 \n",
      "****************************************************************************************************\n",
      "Iteration  434 : Loss =  0.225089   Acc:  0.9844767\n",
      "            : Test Loss :0.3338748514652252 Test Accuracy : 0.9774900078773499 \n",
      "****************************************************************************************************\n",
      "Iteration  435 : Loss =  0.2247252   Acc:  0.9844033\n",
      "            : Test Loss :0.33447328209877014 Test Accuracy : 0.9777699708938599 \n",
      "****************************************************************************************************\n",
      "Iteration  436 : Loss =  0.22445445   Acc:  0.9846\n",
      "            : Test Loss :0.33414310216903687 Test Accuracy : 0.977649986743927 \n",
      "****************************************************************************************************\n",
      "Iteration  437 : Loss =  0.22441462   Acc:  0.9844533\n",
      "            : Test Loss :0.3350302278995514 Test Accuracy : 0.9775800108909607 \n",
      "****************************************************************************************************\n",
      "Iteration  438 : Loss =  0.22459033   Acc:  0.9845033\n",
      "            : Test Loss :0.33542492985725403 Test Accuracy : 0.9775800108909607 \n",
      "****************************************************************************************************\n",
      "Iteration  439 : Loss =  0.22519289   Acc:  0.9843533\n",
      "            : Test Loss :0.33697253465652466 Test Accuracy : 0.9770299792289734 \n",
      "****************************************************************************************************\n",
      "Iteration  440 : Loss =  0.22589919   Acc:  0.98423666\n",
      "            : Test Loss :0.338268518447876 Test Accuracy : 0.9771699905395508 \n",
      "****************************************************************************************************\n",
      "Iteration  441 : Loss =  0.22783814   Acc:  0.984055\n",
      "            : Test Loss :0.341305136680603 Test Accuracy : 0.9767600297927856 \n",
      "****************************************************************************************************\n",
      "Iteration  442 : Loss =  0.22922602   Acc:  0.984015\n",
      "            : Test Loss :0.34343281388282776 Test Accuracy : 0.9768400192260742 \n",
      "****************************************************************************************************\n",
      "Iteration  443 : Loss =  0.2328613   Acc:  0.983685\n",
      "            : Test Loss :0.3464165925979614 Test Accuracy : 0.9762499928474426 \n",
      "****************************************************************************************************\n",
      "Iteration  444 : Loss =  0.23328099   Acc:  0.98364836\n",
      "            : Test Loss :0.34409642219543457 Test Accuracy : 0.9767299890518188 \n",
      "****************************************************************************************************\n",
      "Iteration  445 : Loss =  0.23354277   Acc:  0.98354834\n",
      "            : Test Loss :0.34193938970565796 Test Accuracy : 0.9766899943351746 \n",
      "****************************************************************************************************\n",
      "Iteration  446 : Loss =  0.2287754   Acc:  0.98407334\n",
      "            : Test Loss :0.3348606824874878 Test Accuracy : 0.9773799777030945 \n",
      "****************************************************************************************************\n",
      "Iteration  447 : Loss =  0.22382262   Acc:  0.98436165\n",
      "            : Test Loss :0.3336998522281647 Test Accuracy : 0.9777799844741821 \n",
      "****************************************************************************************************\n",
      "Iteration  448 : Loss =  0.2215374   Acc:  0.98474\n",
      "            : Test Loss :0.33590933680534363 Test Accuracy : 0.9773899912834167 \n",
      "****************************************************************************************************\n",
      "Iteration  449 : Loss =  0.22317168   Acc:  0.984475\n",
      "            : Test Loss :0.33820459246635437 Test Accuracy : 0.9772400259971619 \n",
      "****************************************************************************************************\n",
      "Iteration  450 : Loss =  0.22630918   Acc:  0.984135\n",
      "            : Test Loss :0.3401232063770294 Test Accuracy : 0.9768499732017517 \n",
      "****************************************************************************************************\n",
      "Iteration  451 : Loss =  0.22612385   Acc:  0.98426336\n",
      "            : Test Loss :0.33631718158721924 Test Accuracy : 0.9773200154304504 \n",
      "****************************************************************************************************\n",
      "Iteration  452 : Loss =  0.22383079   Acc:  0.9843467\n",
      "            : Test Loss :0.3343956172466278 Test Accuracy : 0.9778100252151489 \n",
      "****************************************************************************************************\n",
      "Iteration  453 : Loss =  0.2206661   Acc:  0.98476166\n",
      "            : Test Loss :0.3335825502872467 Test Accuracy : 0.9778000116348267 \n",
      "****************************************************************************************************\n",
      "Iteration  454 : Loss =  0.21988268   Acc:  0.98484665\n",
      "            : Test Loss :0.3345583975315094 Test Accuracy : 0.9776399731636047 \n",
      "****************************************************************************************************\n",
      "Iteration  455 : Loss =  0.22125435   Acc:  0.984525\n",
      "            : Test Loss :0.33741065859794617 Test Accuracy : 0.9770699739456177 \n",
      "****************************************************************************************************\n",
      "Iteration  456 : Loss =  0.2224999   Acc:  0.984585\n",
      "            : Test Loss :0.33609405159950256 Test Accuracy : 0.9774100184440613 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  457 : Loss =  0.22243077   Acc:  0.98449\n",
      "            : Test Loss :0.33546656370162964 Test Accuracy : 0.9775500297546387 \n",
      "****************************************************************************************************\n",
      "Iteration  458 : Loss =  0.22041677   Acc:  0.98476\n",
      "            : Test Loss :0.3334408104419708 Test Accuracy : 0.9777600169181824 \n",
      "****************************************************************************************************\n",
      "Iteration  459 : Loss =  0.21884236   Acc:  0.98491\n",
      "            : Test Loss :0.3331262171268463 Test Accuracy : 0.9775599837303162 \n",
      "****************************************************************************************************\n",
      "Iteration  460 : Loss =  0.21858668   Acc:  0.98486\n",
      "            : Test Loss :0.3349999785423279 Test Accuracy : 0.9778500199317932 \n",
      "****************************************************************************************************\n",
      "Iteration  461 : Loss =  0.21931975   Acc:  0.9848833\n",
      "            : Test Loss :0.3344600796699524 Test Accuracy : 0.9774500131607056 \n",
      "****************************************************************************************************\n",
      "Iteration  462 : Loss =  0.21987608   Acc:  0.98467666\n",
      "            : Test Loss :0.33555954694747925 Test Accuracy : 0.9775599837303162 \n",
      "****************************************************************************************************\n",
      "Iteration  463 : Loss =  0.21934864   Acc:  0.9848883\n",
      "            : Test Loss :0.33349284529685974 Test Accuracy : 0.9776300191879272 \n",
      "****************************************************************************************************\n",
      "Iteration  464 : Loss =  0.21823177   Acc:  0.9848217\n",
      "            : Test Loss :0.3332463502883911 Test Accuracy : 0.9776999950408936 \n",
      "****************************************************************************************************\n",
      "Iteration  465 : Loss =  0.21722566   Acc:  0.98501\n",
      "            : Test Loss :0.3332717716693878 Test Accuracy : 0.9778900146484375 \n",
      "****************************************************************************************************\n",
      "Iteration  466 : Loss =  0.21695293   Acc:  0.9850583\n",
      "            : Test Loss :0.3331664204597473 Test Accuracy : 0.9775800108909607 \n",
      "****************************************************************************************************\n",
      "Iteration  467 : Loss =  0.21724515   Acc:  0.9849383\n",
      "            : Test Loss :0.33482626080513 Test Accuracy : 0.9778800010681152 \n",
      "****************************************************************************************************\n",
      "Iteration  468 : Loss =  0.21753721   Acc:  0.98498166\n",
      "            : Test Loss :0.33373183012008667 Test Accuracy : 0.9774199724197388 \n",
      "****************************************************************************************************\n",
      "Iteration  469 : Loss =  0.21747302   Acc:  0.98488164\n",
      "            : Test Loss :0.33460575342178345 Test Accuracy : 0.9779000282287598 \n",
      "****************************************************************************************************\n",
      "Iteration  470 : Loss =  0.21690646   Acc:  0.98503333\n",
      "            : Test Loss :0.3330947756767273 Test Accuracy : 0.9774699807167053 \n",
      "****************************************************************************************************\n",
      "Iteration  471 : Loss =  0.21622802   Acc:  0.9850183\n",
      "            : Test Loss :0.33338719606399536 Test Accuracy : 0.977840006351471 \n",
      "****************************************************************************************************\n",
      "Iteration  472 : Loss =  0.21570101   Acc:  0.985165\n",
      "            : Test Loss :0.33313891291618347 Test Accuracy : 0.9775400161743164 \n",
      "****************************************************************************************************\n",
      "Iteration  473 : Loss =  0.21555422   Acc:  0.985105\n",
      "            : Test Loss :0.33354437351226807 Test Accuracy : 0.9778599739074707 \n",
      "****************************************************************************************************\n",
      "Iteration  474 : Loss =  0.21575375   Acc:  0.9850133\n",
      "            : Test Loss :0.3342791497707367 Test Accuracy : 0.9775800108909607 \n",
      "****************************************************************************************************\n",
      "Iteration  475 : Loss =  0.21599506   Acc:  0.98503333\n",
      "            : Test Loss :0.33475327491760254 Test Accuracy : 0.9779099822044373 \n",
      "****************************************************************************************************\n",
      "Iteration  476 : Loss =  0.21648633   Acc:  0.98495\n",
      "            : Test Loss :0.3353309631347656 Test Accuracy : 0.9774500131607056 \n",
      "****************************************************************************************************\n",
      "Iteration  477 : Loss =  0.21666797   Acc:  0.984895\n",
      "            : Test Loss :0.33648139238357544 Test Accuracy : 0.9777100086212158 \n",
      "****************************************************************************************************\n",
      "Iteration  478 : Loss =  0.2174872   Acc:  0.98483\n",
      "            : Test Loss :0.33692666888237 Test Accuracy : 0.9772999882698059 \n",
      "****************************************************************************************************\n",
      "Iteration  479 : Loss =  0.21822923   Acc:  0.98477\n",
      "            : Test Loss :0.34001219272613525 Test Accuracy : 0.9773399829864502 \n",
      "****************************************************************************************************\n",
      "Iteration  480 : Loss =  0.22006829   Acc:  0.98452836\n",
      "            : Test Loss :0.34103628993034363 Test Accuracy : 0.9770799875259399 \n",
      "****************************************************************************************************\n",
      "Iteration  481 : Loss =  0.22247566   Acc:  0.984185\n",
      "            : Test Loss :0.3445937931537628 Test Accuracy : 0.9769999980926514 \n",
      "****************************************************************************************************\n",
      "Iteration  482 : Loss =  0.22374919   Acc:  0.98424\n",
      "            : Test Loss :0.343230277299881 Test Accuracy : 0.9767600297927856 \n",
      "****************************************************************************************************\n",
      "Iteration  483 : Loss =  0.2247005   Acc:  0.98406667\n",
      "            : Test Loss :0.3405751585960388 Test Accuracy : 0.9773499965667725 \n",
      "****************************************************************************************************\n",
      "Iteration  484 : Loss =  0.2196821   Acc:  0.98462665\n",
      "            : Test Loss :0.33422860503196716 Test Accuracy : 0.9773499965667725 \n",
      "****************************************************************************************************\n",
      "Iteration  485 : Loss =  0.21496698   Acc:  0.98499167\n",
      "            : Test Loss :0.3325096368789673 Test Accuracy : 0.9777399897575378 \n",
      "****************************************************************************************************\n",
      "Iteration  486 : Loss =  0.21233703   Acc:  0.98539\n",
      "            : Test Loss :0.3346300721168518 Test Accuracy : 0.977869987487793 \n",
      "****************************************************************************************************\n",
      "Iteration  487 : Loss =  0.21361673   Acc:  0.98526335\n",
      "            : Test Loss :0.33670586347579956 Test Accuracy : 0.9772499799728394 \n",
      "****************************************************************************************************\n",
      "Iteration  488 : Loss =  0.21672873   Acc:  0.98477167\n",
      "            : Test Loss :0.33967867493629456 Test Accuracy : 0.9774399995803833 \n",
      "****************************************************************************************************\n",
      "Iteration  489 : Loss =  0.21757032   Acc:  0.984775\n",
      "            : Test Loss :0.33707791566848755 Test Accuracy : 0.9771999716758728 \n",
      "****************************************************************************************************\n",
      "Iteration  490 : Loss =  0.21651836   Acc:  0.9847983\n",
      "            : Test Loss :0.3352470099925995 Test Accuracy : 0.9779300093650818 \n",
      "****************************************************************************************************\n",
      "Iteration  491 : Loss =  0.21338004   Acc:  0.98520833\n",
      "            : Test Loss :0.3331826329231262 Test Accuracy : 0.9777299761772156 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  492 : Loss =  0.2113646   Acc:  0.98534\n",
      "            : Test Loss :0.3328861892223358 Test Accuracy : 0.9776700139045715 \n",
      "****************************************************************************************************\n",
      "Iteration  493 : Loss =  0.21153156   Acc:  0.98528165\n",
      "            : Test Loss :0.3357253074645996 Test Accuracy : 0.9776399731636047 \n",
      "****************************************************************************************************\n",
      "Iteration  494 : Loss =  0.21278861   Acc:  0.9853017\n",
      "            : Test Loss :0.3353840708732605 Test Accuracy : 0.9771900177001953 \n",
      "****************************************************************************************************\n",
      "Iteration  495 : Loss =  0.21399681   Acc:  0.9849617\n",
      "            : Test Loss :0.33686363697052 Test Accuracy : 0.9777100086212158 \n",
      "****************************************************************************************************\n",
      "Iteration  496 : Loss =  0.21323618   Acc:  0.98528665\n",
      "            : Test Loss :0.33417263627052307 Test Accuracy : 0.9773399829864502 \n",
      "****************************************************************************************************\n",
      "Iteration  497 : Loss =  0.21192133   Acc:  0.98517334\n",
      "            : Test Loss :0.33366674184799194 Test Accuracy : 0.9779700040817261 \n",
      "****************************************************************************************************\n",
      "Iteration  498 : Loss =  0.21009088   Acc:  0.985495\n",
      "            : Test Loss :0.33289286494255066 Test Accuracy : 0.9777500033378601 \n",
      "****************************************************************************************************\n",
      "Iteration  499 : Loss =  0.20935807   Acc:  0.98555833\n",
      "            : Test Loss :0.33287954330444336 Test Accuracy : 0.9775300025939941 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "    # load the training and test data    \n",
    "    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "    # reshape the feature data\n",
    "    tr_x = tr_x.reshape(tr_x.shape[0],784)\n",
    "    te_x = te_x.reshape(te_x.shape[0],784)\n",
    "\n",
    "    # noramlise feature data\n",
    "    tr_x = tr_x / 255.0\n",
    "    te_x = te_x / 255.0\n",
    "\n",
    "    # one hot encode the training labels and get the transpose\n",
    "    tr_y = np_utils.to_categorical(tr_y,10)\n",
    "    tr_y = tr_y.T\n",
    "\n",
    "    # one hot encode the test labels and get the transpose\n",
    "    te_y = np_utils.to_categorical(te_y,10)\n",
    "    te_y = te_y.T\n",
    "\n",
    "    return tr_x, tr_y, te_x, te_y\n",
    "     \n",
    "    \n",
    "\n",
    "\n",
    "def softmax(y_pred):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    out=tf.exp(y_pred) / tf.reduce_sum(tf.exp(y_pred), axis=0) \n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"Forward pass for 1_2_2\"\"\"\n",
    "def forward_pass(x, w3_T, w2_T, w1_T, b):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"Layer 1 - With 300 ReLu Neurons\"\"\"\n",
    "    # We need to mutliply the flattened INPUT by the weights of this layer and add bias\n",
    "    y_pred_layer1 = tf.matmul(w1_T, x) + b\n",
    "    y_pred_relu_layer1 = tf.maximum(y_pred_layer1, 0)\n",
    "    \n",
    "    \n",
    "    \"\"\"Layer 2 - With 100 ReLu Neurons\"\"\"\n",
    "    # We need to mutliply the output of the previous layer by the weights of this layer and add bias\n",
    "    y_pred_layer2 = tf.matmul(w2_T, y_pred_relu_layer1) + b\n",
    "    y_pred_relu_layer2 = tf.maximum(y_pred_layer2, 0)\n",
    "    \n",
    "    \n",
    "    \"\"\"Layer 3 - Softmax Layer with 10 neurons\"\"\"\n",
    "    # We need to mutliply the output of the previous layer by the weights of this layer and add bias\n",
    "    y_pred_layer3 = tf.matmul(w3_T, y_pred_relu_layer2) + b\n",
    "    \n",
    "    '''Pipe the results through the softmax activiation function. '''\n",
    "    y_pred_softmax = softmax(y_pred_layer3)\n",
    "    \n",
    "\n",
    "    return y_pred_softmax\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "    \n",
    "def cross_entropy(y, y_pred):\n",
    "    '''Sometimes the softmax  values (probability) ouptutted by a neuron can se very very close to 0 or 0\n",
    "       In this case the log(0)==> not defined and will result in NaN values.\n",
    "       This can be handled using this function clip_by_value which replaces every value less than threshold\n",
    "       with the minimum value (1e-10)'''\n",
    "    \n",
    "    y_pred_ = tf.clip_by_value(y_pred, 1e-10, 1.0)\n",
    "    \n",
    "    \"\"\"Calculate the crosss entropy loss per image\"\"\"\n",
    "    num = - tf.reduce_sum( y * tf.math.log(y_pred), axis=0 )\n",
    "    \n",
    "    \"\"\"Calculate the average loss\"\"\"\n",
    "    return tf.reduce_mean(num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_accuracy(y_pred_softmax,y):\n",
    "  \n",
    "\n",
    "    \n",
    "    # Round the predictions by the logistical unit to either 1 or 0\n",
    "    predictions = tf.round(y_pred_softmax)\n",
    "\n",
    "    # tf.equal will return a boolean array: True if prediction correct, False otherwise\n",
    "    # tf.cast converts the resulting boolean array to a numerical array \n",
    "    # 1 if True (correct prediction), 0 if False (incorrect prediction)\n",
    "#     predictions_correct = tf.cast(tf.equal(tf.argmax(y_pred_softmax), tf.argmax(y)), tf.float32)\n",
    "\n",
    "    predictions_correct = tf.cast(tf.equal(predictions,y), tf.float32)\n",
    "\n",
    "    # Finally, we just determine the mean value of predictions_correct\n",
    "    accuracy = tf.reduce_mean(predictions_correct)\n",
    "    return accuracy\n",
    "\n",
    "def main():\n",
    "    list_of_test_accuracies=[]\n",
    "    list_of_train_accuracies=[]\n",
    "    list_of_test_loss=[]\n",
    "    list_of_train_loss=[]\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    num_Iterations = 500\n",
    "    adam_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    '''Load data'''\n",
    "    tr_x, tr_y, te_x, te_y  = load_data()\n",
    "    \n",
    "    '''Cast the input values in a compatible tensorflow float32 format'''\n",
    "    tr_x = tf.cast(tr_x, tf.float32)\n",
    "    te_x = tf.cast(te_x, tf.float32)\n",
    "    tr_y = tf.cast(tr_y, tf.float32)\n",
    "    te_y = tf.cast(te_y, tf.float32)\n",
    "\n",
    "\n",
    "    \"\"\"We need a coefficient for each of the features and a single bias value\"\"\"\n",
    "    w1 = tf.Variable(tf.random.normal([ 300,784], mean=0.0, stddev=0.05))\n",
    "    w2 = tf.Variable(tf.random.normal([ 100,300], mean=0.0, stddev=0.05))\n",
    "    w3 = tf.Variable(tf.random.normal([ 10,100], mean=0.0, stddev=0.05))\n",
    "    \n",
    "    print('w1 shape ==>',w1.shape)\n",
    "    print('w2 shape ==>',w2.shape)\n",
    "    print('w3 shape ==>',w3.shape)\n",
    "    print('tr_x shape ==>',tr_x.shape)\n",
    "    print('tr_y shape ==>',tr_y.shape)\n",
    "    \n",
    "    b = tf.Variable([0.])\n",
    "   \n",
    "    # Iterate our training loop\n",
    "    for i in range(num_Iterations):\n",
    "      \n",
    "        # Create an instance of GradientTape to monitor the forward pass\n",
    "        # and calcualte the gradients for each of the variables m and c\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = forward_pass(tf.transpose(tr_x), w3,w2,w1, b)\n",
    "            currentLoss = cross_entropy(tr_y, y_pred)\n",
    "            list_of_train_loss.append(currentLoss)\n",
    "            \n",
    "        \"\"\"Calculate partial derivatives of Average Loss with respect to change in w and b\"\"\"\n",
    "        gradients = tape.gradient(currentLoss, [w1, w2, w3, b])\n",
    "        accuracy = calculate_accuracy(y_pred,tr_y)\n",
    "        list_of_train_accuracies.append(accuracy)\n",
    "        \n",
    "\n",
    "        print (\"Iteration \", i, \": Loss = \",currentLoss.numpy(), \"  Acc: \", accuracy.numpy())\n",
    "        \n",
    "        \"\"\"Give the above calculated derivatives to the optimizer to decent to the most optimal path\"\"\"\n",
    "        adam_optimizer.apply_gradients(zip(gradients, [w1,w2,w3,b]))\n",
    "        \n",
    "        \"\"\"Lets see how the model performs with the newly upadted w and b on the test data\"\"\"\n",
    "        te_y_pred = forward_pass(tf.transpose(te_x), w3, w2, w1, b)\n",
    "        current_Test_Loss = cross_entropy(te_y, te_y_pred)\n",
    "        list_of_test_loss.append(current_Test_Loss.numpy())\n",
    "\n",
    "        test_accuracy = calculate_accuracy(te_y_pred, te_y) \n",
    "        list_of_test_accuracies.append(test_accuracy.numpy())\n",
    "\n",
    "        print (\"            : Test Loss :{0} Test Accuracy : {1} \" .format(current_Test_Loss,test_accuracy) )\n",
    "        \n",
    "        print(\"*\"*100)\n",
    "        \n",
    "        \n",
    "    \"\"\"Plotting the performance\"\"\"\n",
    "#     configuration = 'Network A \\n 300 ReLu  10 softmax neurons'\n",
    "    configuration = 'Network B \\n 300 ReLu 100 Relu, 10 softmax neurons'\n",
    "    plt.title(\"Training Loss \\n\"+configuration+' Epochs:'+str (num_Iterations))\n",
    "    plt.plot(list_of_test_loss, label=\"Val Loss\")\n",
    "    plt.plot(list_of_train_loss, label=\"Train Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "  \n",
    "    \n",
    "main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
