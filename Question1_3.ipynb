{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 shape ==> (300, 784)\n",
      "w2 shape ==> (100, 300)\n",
      "w3 shape ==> (10, 100)\n",
      "Iteration  0 : Loss =  2.9853964   Acc:  0.9\n",
      "            : Test Loss :2.857966423034668 Test Accuracy : 0.8999999761581421 \n",
      "****************************************************************************************************\n",
      "Iteration  1 : Loss =  2.8570738   Acc:  0.9\n",
      "            : Test Loss :2.7582077980041504 Test Accuracy : 0.8999999761581421 \n",
      "****************************************************************************************************\n",
      "Iteration  2 : Loss =  2.756833   Acc:  0.9\n",
      "            : Test Loss :2.651205062866211 Test Accuracy : 0.8999999761581421 \n",
      "****************************************************************************************************\n",
      "Iteration  3 : Loss =  2.6489873   Acc:  0.9\n",
      "            : Test Loss :2.535660743713379 Test Accuracy : 0.8999999761581421 \n",
      "****************************************************************************************************\n",
      "Iteration  4 : Loss =  2.5324252   Acc:  0.9\n",
      "            : Test Loss :2.4159488677978516 Test Accuracy : 0.8999999761581421 \n",
      "****************************************************************************************************\n",
      "Iteration  5 : Loss =  2.4117506   Acc:  0.9\n",
      "            : Test Loss :2.2962522506713867 Test Accuracy : 0.9000399708747864 \n",
      "****************************************************************************************************\n",
      "Iteration  6 : Loss =  2.2911189   Acc:  0.90005666\n",
      "            : Test Loss :2.1772072315216064 Test Accuracy : 0.9006400108337402 \n",
      "****************************************************************************************************\n",
      "Iteration  7 : Loss =  2.171317   Acc:  0.9007\n",
      "            : Test Loss :2.06083345413208 Test Accuracy : 0.9027600288391113 \n",
      "****************************************************************************************************\n",
      "Iteration  8 : Loss =  2.0541744   Acc:  0.90294665\n",
      "            : Test Loss :1.9505645036697388 Test Accuracy : 0.9079099893569946 \n",
      "****************************************************************************************************\n",
      "Iteration  9 : Loss =  1.943198   Acc:  0.90823334\n",
      "            : Test Loss :1.8504817485809326 Test Accuracy : 0.9160400032997131 \n",
      "****************************************************************************************************\n",
      "Iteration  10 : Loss =  1.8425224   Acc:  0.91663665\n",
      "            : Test Loss :1.7614476680755615 Test Accuracy : 0.9230700135231018 \n",
      "****************************************************************************************************\n",
      "Iteration  11 : Loss =  1.7527373   Acc:  0.92345\n",
      "            : Test Loss :1.6832211017608643 Test Accuracy : 0.9286400079727173 \n",
      "****************************************************************************************************\n",
      "Iteration  12 : Loss =  1.6737039   Acc:  0.92880833\n",
      "            : Test Loss :1.6161119937896729 Test Accuracy : 0.9333000183105469 \n",
      "****************************************************************************************************\n",
      "Iteration  13 : Loss =  1.60552   Acc:  0.93337333\n",
      "            : Test Loss :1.5590646266937256 Test Accuracy : 0.9358599781990051 \n",
      "****************************************************************************************************\n",
      "Iteration  14 : Loss =  1.5475192   Acc:  0.93633\n",
      "            : Test Loss :1.509195327758789 Test Accuracy : 0.9380699992179871 \n",
      "****************************************************************************************************\n",
      "Iteration  15 : Loss =  1.4971068   Acc:  0.938775\n",
      "            : Test Loss :1.4674360752105713 Test Accuracy : 0.9397299885749817 \n",
      "****************************************************************************************************\n",
      "Iteration  16 : Loss =  1.4548825   Acc:  0.940495\n",
      "            : Test Loss :1.4323327541351318 Test Accuracy : 0.9404399991035461 \n",
      "****************************************************************************************************\n",
      "Iteration  17 : Loss =  1.4187968   Acc:  0.94170165\n",
      "            : Test Loss :1.401794195175171 Test Accuracy : 0.9415000081062317 \n",
      "****************************************************************************************************\n",
      "Iteration  18 : Loss =  1.3868972   Acc:  0.94273\n",
      "            : Test Loss :1.375233769416809 Test Accuracy : 0.942579984664917 \n",
      "****************************************************************************************************\n",
      "Iteration  19 : Loss =  1.3591384   Acc:  0.94394165\n",
      "            : Test Loss :1.3516404628753662 Test Accuracy : 0.9436799883842468 \n",
      "****************************************************************************************************\n",
      "Iteration  20 : Loss =  1.3343377   Acc:  0.94508\n",
      "            : Test Loss :1.330195665359497 Test Accuracy : 0.9446200132369995 \n",
      "****************************************************************************************************\n",
      "Iteration  21 : Loss =  1.3117054   Acc:  0.9462683\n",
      "            : Test Loss :1.3101003170013428 Test Accuracy : 0.9461899995803833 \n",
      "****************************************************************************************************\n",
      "Iteration  22 : Loss =  1.2909791   Acc:  0.94788\n",
      "            : Test Loss :1.2909488677978516 Test Accuracy : 0.9473299980163574 \n",
      "****************************************************************************************************\n",
      "Iteration  23 : Loss =  1.2711753   Acc:  0.94896835\n",
      "            : Test Loss :1.273195505142212 Test Accuracy : 0.9480900168418884 \n",
      "****************************************************************************************************\n",
      "Iteration  24 : Loss =  1.2527889   Acc:  0.9498967\n",
      "            : Test Loss :1.2557272911071777 Test Accuracy : 0.9493600130081177 \n",
      "****************************************************************************************************\n",
      "Iteration  25 : Loss =  1.2350943   Acc:  0.951095\n",
      "            : Test Loss :1.2389235496520996 Test Accuracy : 0.9503499865531921 \n",
      "****************************************************************************************************\n",
      "Iteration  26 : Loss =  1.2176516   Acc:  0.9520633\n",
      "            : Test Loss :1.2227340936660767 Test Accuracy : 0.9519100189208984 \n",
      "****************************************************************************************************\n",
      "Iteration  27 : Loss =  1.2012105   Acc:  0.95334333\n",
      "            : Test Loss :1.2068321704864502 Test Accuracy : 0.9528499841690063 \n",
      "****************************************************************************************************\n",
      "Iteration  28 : Loss =  1.1850474   Acc:  0.9543\n",
      "            : Test Loss :1.191494345664978 Test Accuracy : 0.9534100294113159 \n",
      "****************************************************************************************************\n",
      "Iteration  29 : Loss =  1.1694214   Acc:  0.9550383\n",
      "            : Test Loss :1.1766756772994995 Test Accuracy : 0.9542700052261353 \n",
      "****************************************************************************************************\n",
      "Iteration  30 : Loss =  1.1545448   Acc:  0.955785\n",
      "            : Test Loss :1.1625711917877197 Test Accuracy : 0.9550399780273438 \n",
      "****************************************************************************************************\n",
      "Iteration  31 : Loss =  1.139999   Acc:  0.95663\n",
      "            : Test Loss :1.1487395763397217 Test Accuracy : 0.9563800096511841 \n",
      "****************************************************************************************************\n",
      "Iteration  32 : Loss =  1.1263381   Acc:  0.957645\n",
      "            : Test Loss :1.136467695236206 Test Accuracy : 0.9564399719238281 \n",
      "****************************************************************************************************\n",
      "Iteration  33 : Loss =  1.1133075   Acc:  0.9582083\n",
      "            : Test Loss :1.1238594055175781 Test Accuracy : 0.9572799801826477 \n",
      "****************************************************************************************************\n",
      "Iteration  34 : Loss =  1.1010576   Acc:  0.95874834\n",
      "            : Test Loss :1.1125171184539795 Test Accuracy : 0.9571899771690369 \n",
      "****************************************************************************************************\n",
      "Iteration  35 : Loss =  1.0889392   Acc:  0.9591867\n",
      "            : Test Loss :1.0998443365097046 Test Accuracy : 0.9584199786186218 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  36 : Loss =  1.0764686   Acc:  0.95988333\n",
      "            : Test Loss :1.0878310203552246 Test Accuracy : 0.9588099718093872 \n",
      "****************************************************************************************************\n",
      "Iteration  37 : Loss =  1.0644958   Acc:  0.96061\n",
      "            : Test Loss :1.0782725811004639 Test Accuracy : 0.9591000080108643 \n",
      "****************************************************************************************************\n",
      "Iteration  38 : Loss =  1.0537715   Acc:  0.96105665\n",
      "            : Test Loss :1.0675220489501953 Test Accuracy : 0.9593799710273743 \n",
      "****************************************************************************************************\n",
      "Iteration  39 : Loss =  1.0438988   Acc:  0.96133333\n",
      "            : Test Loss :1.0571296215057373 Test Accuracy : 0.9600300192832947 \n",
      "****************************************************************************************************\n",
      "Iteration  40 : Loss =  1.0323342   Acc:  0.96194\n",
      "            : Test Loss :1.046372413635254 Test Accuracy : 0.9606500267982483 \n",
      "****************************************************************************************************\n",
      "Iteration  41 : Loss =  1.0217232   Acc:  0.9624183\n",
      "            : Test Loss :1.0374393463134766 Test Accuracy : 0.9607700109481812 \n",
      "****************************************************************************************************\n",
      "Iteration  42 : Loss =  1.0129476   Acc:  0.96272\n",
      "            : Test Loss :1.0300993919372559 Test Accuracy : 0.9612399935722351 \n",
      "****************************************************************************************************\n",
      "Iteration  43 : Loss =  1.0041066   Acc:  0.96298\n",
      "            : Test Loss :1.0197457075119019 Test Accuracy : 0.9615100026130676 \n",
      "****************************************************************************************************\n",
      "Iteration  44 : Loss =  0.9943969   Acc:  0.9633333\n",
      "            : Test Loss :1.010875940322876 Test Accuracy : 0.9617699980735779 \n",
      "****************************************************************************************************\n",
      "Iteration  45 : Loss =  0.9848043   Acc:  0.96375\n",
      "            : Test Loss :1.003418207168579 Test Accuracy : 0.9623600244522095 \n",
      "****************************************************************************************************\n",
      "Iteration  46 : Loss =  0.97672355   Acc:  0.96411836\n",
      "            : Test Loss :0.9952718019485474 Test Accuracy : 0.9625899791717529 \n",
      "****************************************************************************************************\n",
      "Iteration  47 : Loss =  0.9690401   Acc:  0.9641917\n",
      "            : Test Loss :0.9877042770385742 Test Accuracy : 0.9628000259399414 \n",
      "****************************************************************************************************\n",
      "Iteration  48 : Loss =  0.96023464   Acc:  0.9646033\n",
      "            : Test Loss :0.9788743853569031 Test Accuracy : 0.9629999995231628 \n",
      "****************************************************************************************************\n",
      "Iteration  49 : Loss =  0.95166177   Acc:  0.9648967\n",
      "            : Test Loss :0.9715016484260559 Test Accuracy : 0.9632200002670288 \n",
      "****************************************************************************************************\n",
      "Iteration  50 : Loss =  0.94409233   Acc:  0.96516\n",
      "            : Test Loss :0.9650546908378601 Test Accuracy : 0.9634799957275391 \n",
      "****************************************************************************************************\n",
      "Iteration  51 : Loss =  0.93691295   Acc:  0.9653933\n",
      "            : Test Loss :0.956973135471344 Test Accuracy : 0.9636300206184387 \n",
      "****************************************************************************************************\n",
      "Iteration  52 : Loss =  0.9292889   Acc:  0.965525\n",
      "            : Test Loss :0.949650764465332 Test Accuracy : 0.9639800190925598 \n",
      "****************************************************************************************************\n",
      "Iteration  53 : Loss =  0.92137814   Acc:  0.96592\n",
      "            : Test Loss :0.9424139261245728 Test Accuracy : 0.9641600251197815 \n",
      "****************************************************************************************************\n",
      "Iteration  54 : Loss =  0.9140466   Acc:  0.966145\n",
      "            : Test Loss :0.9357732534408569 Test Accuracy : 0.9643999934196472 \n",
      "****************************************************************************************************\n",
      "Iteration  55 : Loss =  0.9074265   Acc:  0.9662517\n",
      "            : Test Loss :0.9300223588943481 Test Accuracy : 0.9645100235939026 \n",
      "****************************************************************************************************\n",
      "Iteration  56 : Loss =  0.9009442   Acc:  0.9664417\n",
      "            : Test Loss :0.9229139089584351 Test Accuracy : 0.964680016040802 \n",
      "****************************************************************************************************\n",
      "Iteration  57 : Loss =  0.894297   Acc:  0.96655166\n",
      "            : Test Loss :0.9166882038116455 Test Accuracy : 0.9650200009346008 \n",
      "****************************************************************************************************\n",
      "Iteration  58 : Loss =  0.8874143   Acc:  0.96691835\n",
      "            : Test Loss :0.9098612070083618 Test Accuracy : 0.9650899767875671 \n",
      "****************************************************************************************************\n",
      "Iteration  59 : Loss =  0.88078415   Acc:  0.96705836\n",
      "            : Test Loss :0.9040434956550598 Test Accuracy : 0.9652699828147888 \n",
      "****************************************************************************************************\n",
      "Iteration  60 : Loss =  0.8746436   Acc:  0.9673667\n",
      "            : Test Loss :0.8985030651092529 Test Accuracy : 0.9652799963951111 \n",
      "****************************************************************************************************\n",
      "Iteration  61 : Loss =  0.8690512   Acc:  0.967345\n",
      "            : Test Loss :0.8939513564109802 Test Accuracy : 0.9653199911117554 \n",
      "****************************************************************************************************\n",
      "Iteration  62 : Loss =  0.8642005   Acc:  0.96748\n",
      "            : Test Loss :0.8895257115364075 Test Accuracy : 0.9652699828147888 \n",
      "****************************************************************************************************\n",
      "Iteration  63 : Loss =  0.8600477   Acc:  0.9673967\n",
      "            : Test Loss :0.8870109915733337 Test Accuracy : 0.9653300046920776 \n",
      "****************************************************************************************************\n",
      "Iteration  64 : Loss =  0.8565179   Acc:  0.96767664\n",
      "            : Test Loss :0.8794564008712769 Test Accuracy : 0.9651899933815002 \n",
      "****************************************************************************************************\n",
      "Iteration  65 : Loss =  0.85032654   Acc:  0.96743333\n",
      "            : Test Loss :0.8738404512405396 Test Accuracy : 0.9654399752616882 \n",
      "****************************************************************************************************\n",
      "Iteration  66 : Loss =  0.84320104   Acc:  0.9679833\n",
      "            : Test Loss :0.868218183517456 Test Accuracy : 0.9657999873161316 \n",
      "****************************************************************************************************\n",
      "Iteration  67 : Loss =  0.838481   Acc:  0.9679483\n",
      "            : Test Loss :0.8617309331893921 Test Accuracy : 0.9662200212478638 \n",
      "****************************************************************************************************\n",
      "Iteration  68 : Loss =  0.8317524   Acc:  0.968325\n",
      "            : Test Loss :0.8563711643218994 Test Accuracy : 0.9664300084114075 \n",
      "****************************************************************************************************\n",
      "Iteration  69 : Loss =  0.82594657   Acc:  0.96869665\n",
      "            : Test Loss :0.8535233736038208 Test Accuracy : 0.9660800099372864 \n",
      "****************************************************************************************************\n",
      "Iteration  70 : Loss =  0.8237717   Acc:  0.96835\n",
      "            : Test Loss :0.8484060764312744 Test Accuracy : 0.9662500023841858 \n",
      "****************************************************************************************************\n",
      "Iteration  71 : Loss =  0.8178941   Acc:  0.96858835\n",
      "            : Test Loss :0.8408910632133484 Test Accuracy : 0.9670000076293945 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  72 : Loss =  0.8105194   Acc:  0.9691783\n",
      "            : Test Loss :0.838040292263031 Test Accuracy : 0.966759979724884 \n",
      "****************************************************************************************************\n",
      "Iteration  73 : Loss =  0.8079139   Acc:  0.96904\n",
      "            : Test Loss :0.8348680734634399 Test Accuracy : 0.9665499925613403 \n",
      "****************************************************************************************************\n",
      "Iteration  74 : Loss =  0.80402774   Acc:  0.96895\n",
      "            : Test Loss :0.827804684638977 Test Accuracy : 0.9672399759292603 \n",
      "****************************************************************************************************\n",
      "Iteration  75 : Loss =  0.7973104   Acc:  0.9693767\n",
      "            : Test Loss :0.8241174221038818 Test Accuracy : 0.9672600030899048 \n",
      "****************************************************************************************************\n",
      "Iteration  76 : Loss =  0.7933342   Acc:  0.96953666\n",
      "            : Test Loss :0.8209611773490906 Test Accuracy : 0.9671599864959717 \n",
      "****************************************************************************************************\n",
      "Iteration  77 : Loss =  0.78997916   Acc:  0.9693933\n",
      "            : Test Loss :0.8154999613761902 Test Accuracy : 0.967270016670227 \n",
      "****************************************************************************************************\n",
      "Iteration  78 : Loss =  0.78484523   Acc:  0.96961164\n",
      "            : Test Loss :0.8122192621231079 Test Accuracy : 0.9673200249671936 \n",
      "****************************************************************************************************\n",
      "Iteration  79 : Loss =  0.7810167   Acc:  0.96983\n",
      "            : Test Loss :0.8077506422996521 Test Accuracy : 0.967519998550415 \n",
      "****************************************************************************************************\n",
      "Iteration  80 : Loss =  0.77679205   Acc:  0.96975666\n",
      "            : Test Loss :0.8027040362358093 Test Accuracy : 0.9678500294685364 \n",
      "****************************************************************************************************\n",
      "Iteration  81 : Loss =  0.7717104   Acc:  0.97001165\n",
      "            : Test Loss :0.7998443841934204 Test Accuracy : 0.9678099751472473 \n",
      "****************************************************************************************************\n",
      "Iteration  82 : Loss =  0.7685076   Acc:  0.97016335\n",
      "            : Test Loss :0.7960554361343384 Test Accuracy : 0.967739999294281 \n",
      "****************************************************************************************************\n",
      "Iteration  83 : Loss =  0.7649556   Acc:  0.97000664\n",
      "            : Test Loss :0.7915741801261902 Test Accuracy : 0.9678500294685364 \n",
      "****************************************************************************************************\n",
      "Iteration  84 : Loss =  0.7601272   Acc:  0.97034\n",
      "            : Test Loss :0.7877200841903687 Test Accuracy : 0.9679099917411804 \n",
      "****************************************************************************************************\n",
      "Iteration  85 : Loss =  0.7563691   Acc:  0.97036666\n",
      "            : Test Loss :0.7843104600906372 Test Accuracy : 0.9679499864578247 \n",
      "****************************************************************************************************\n",
      "Iteration  86 : Loss =  0.752856   Acc:  0.97036666\n",
      "            : Test Loss :0.7803158164024353 Test Accuracy : 0.9682300090789795 \n",
      "****************************************************************************************************\n",
      "Iteration  87 : Loss =  0.74861467   Acc:  0.97058165\n",
      "            : Test Loss :0.7767969965934753 Test Accuracy : 0.9680500030517578 \n",
      "****************************************************************************************************\n",
      "Iteration  88 : Loss =  0.7451665   Acc:  0.97065336\n",
      "            : Test Loss :0.7738316059112549 Test Accuracy : 0.9682599902153015 \n",
      "****************************************************************************************************\n",
      "Iteration  89 : Loss =  0.7420464   Acc:  0.97058\n",
      "            : Test Loss :0.7699106931686401 Test Accuracy : 0.9681800007820129 \n",
      "****************************************************************************************************\n",
      "Iteration  90 : Loss =  0.7381541   Acc:  0.9707983\n",
      "            : Test Loss :0.7664880156517029 Test Accuracy : 0.968529999256134 \n",
      "****************************************************************************************************\n",
      "Iteration  91 : Loss =  0.734447   Acc:  0.97090167\n",
      "            : Test Loss :0.7631030082702637 Test Accuracy : 0.9684699773788452 \n",
      "****************************************************************************************************\n",
      "Iteration  92 : Loss =  0.73123574   Acc:  0.9708617\n",
      "            : Test Loss :0.7597464323043823 Test Accuracy : 0.968720018863678 \n",
      "****************************************************************************************************\n",
      "Iteration  93 : Loss =  0.72759795   Acc:  0.9710817\n",
      "            : Test Loss :0.7559822797775269 Test Accuracy : 0.9686499834060669 \n",
      "****************************************************************************************************\n",
      "Iteration  94 : Loss =  0.7238865   Acc:  0.97116166\n",
      "            : Test Loss :0.7530527114868164 Test Accuracy : 0.9688100218772888 \n",
      "****************************************************************************************************\n",
      "Iteration  95 : Loss =  0.72075236   Acc:  0.9712167\n",
      "            : Test Loss :0.7499272227287292 Test Accuracy : 0.9687600135803223 \n",
      "****************************************************************************************************\n",
      "Iteration  96 : Loss =  0.7175611   Acc:  0.97129166\n",
      "            : Test Loss :0.7465758323669434 Test Accuracy : 0.9688799977302551 \n",
      "****************************************************************************************************\n",
      "Iteration  97 : Loss =  0.71410286   Acc:  0.97142833\n",
      "            : Test Loss :0.7435727119445801 Test Accuracy : 0.9689099788665771 \n",
      "****************************************************************************************************\n",
      "Iteration  98 : Loss =  0.7111261   Acc:  0.971395\n",
      "            : Test Loss :0.7414258718490601 Test Accuracy : 0.9689599871635437 \n",
      "****************************************************************************************************\n",
      "Iteration  99 : Loss =  0.70862657   Acc:  0.97149\n",
      "            : Test Loss :0.7391531467437744 Test Accuracy : 0.9688599705696106 \n",
      "****************************************************************************************************\n",
      "Iteration  100 : Loss =  0.7065693   Acc:  0.9713333\n",
      "            : Test Loss :0.7381182909011841 Test Accuracy : 0.9689300060272217 \n",
      "****************************************************************************************************\n",
      "Iteration  101 : Loss =  0.7050259   Acc:  0.97139835\n",
      "            : Test Loss :0.7390007972717285 Test Accuracy : 0.9684600234031677 \n",
      "****************************************************************************************************\n",
      "Iteration  102 : Loss =  0.7061248   Acc:  0.970985\n",
      "            : Test Loss :0.7342196702957153 Test Accuracy : 0.9689599871635437 \n",
      "****************************************************************************************************\n",
      "Iteration  103 : Loss =  0.7008938   Acc:  0.97130334\n",
      "            : Test Loss :0.7284266948699951 Test Accuracy : 0.9691600203514099 \n",
      "****************************************************************************************************\n",
      "Iteration  104 : Loss =  0.69550854   Acc:  0.97162664\n",
      "            : Test Loss :0.7230448722839355 Test Accuracy : 0.9694399833679199 \n",
      "****************************************************************************************************\n",
      "Iteration  105 : Loss =  0.6898827   Acc:  0.97203165\n",
      "            : Test Loss :0.7218675017356873 Test Accuracy : 0.9694100022315979 \n",
      "****************************************************************************************************\n",
      "Iteration  106 : Loss =  0.68852997   Acc:  0.97187835\n",
      "            : Test Loss :0.7220875024795532 Test Accuracy : 0.9690600037574768 \n",
      "****************************************************************************************************\n",
      "Iteration  107 : Loss =  0.6888182   Acc:  0.97165334\n",
      "            : Test Loss :0.7181414365768433 Test Accuracy : 0.9694499969482422 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  108 : Loss =  0.6845   Acc:  0.9719717\n",
      "            : Test Loss :0.7132257223129272 Test Accuracy : 0.9696300029754639 \n",
      "****************************************************************************************************\n",
      "Iteration  109 : Loss =  0.67977977   Acc:  0.97213\n",
      "            : Test Loss :0.7103757262229919 Test Accuracy : 0.9696400165557861 \n",
      "****************************************************************************************************\n",
      "Iteration  110 : Loss =  0.6766664   Acc:  0.97233\n",
      "            : Test Loss :0.709456205368042 Test Accuracy : 0.9696300029754639 \n",
      "****************************************************************************************************\n",
      "Iteration  111 : Loss =  0.6754977   Acc:  0.97226167\n",
      "            : Test Loss :0.7080680131912231 Test Accuracy : 0.9695199728012085 \n",
      "****************************************************************************************************\n",
      "Iteration  112 : Loss =  0.6742713   Acc:  0.97215664\n",
      "            : Test Loss :0.7043415307998657 Test Accuracy : 0.9697399735450745 \n",
      "****************************************************************************************************\n",
      "Iteration  113 : Loss =  0.67023396   Acc:  0.97242\n",
      "            : Test Loss :0.7003985643386841 Test Accuracy : 0.969980001449585 \n",
      "****************************************************************************************************\n",
      "Iteration  114 : Loss =  0.6663561   Acc:  0.972625\n",
      "            : Test Loss :0.6986230611801147 Test Accuracy : 0.9699599742889404 \n",
      "****************************************************************************************************\n",
      "Iteration  115 : Loss =  0.6645323   Acc:  0.97261834\n",
      "            : Test Loss :0.6976528763771057 Test Accuracy : 0.9700000286102295 \n",
      "****************************************************************************************************\n",
      "Iteration  116 : Loss =  0.66332114   Acc:  0.972575\n",
      "            : Test Loss :0.6952346563339233 Test Accuracy : 0.9698899984359741 \n",
      "****************************************************************************************************\n",
      "Iteration  117 : Loss =  0.6609612   Acc:  0.97252\n",
      "            : Test Loss :0.6917873620986938 Test Accuracy : 0.9700599908828735 \n",
      "****************************************************************************************************\n",
      "Iteration  118 : Loss =  0.6572932   Acc:  0.97281164\n",
      "            : Test Loss :0.6891435384750366 Test Accuracy : 0.9701300263404846 \n",
      "****************************************************************************************************\n",
      "Iteration  119 : Loss =  0.65454865   Acc:  0.97291166\n",
      "            : Test Loss :0.6874110698699951 Test Accuracy : 0.9702699780464172 \n",
      "****************************************************************************************************\n",
      "Iteration  120 : Loss =  0.65281427   Acc:  0.97281164\n",
      "            : Test Loss :0.685843288898468 Test Accuracy : 0.9700899720191956 \n",
      "****************************************************************************************************\n",
      "Iteration  121 : Loss =  0.6510061   Acc:  0.97294164\n",
      "            : Test Loss :0.6837486028671265 Test Accuracy : 0.9699699878692627 \n",
      "****************************************************************************************************\n",
      "Iteration  122 : Loss =  0.6488661   Acc:  0.9729033\n",
      "            : Test Loss :0.6810370683670044 Test Accuracy : 0.970300018787384 \n",
      "****************************************************************************************************\n",
      "Iteration  123 : Loss =  0.64611346   Acc:  0.97303\n",
      "            : Test Loss :0.6786096096038818 Test Accuracy : 0.9704399704933167 \n",
      "****************************************************************************************************\n",
      "Iteration  124 : Loss =  0.6435363   Acc:  0.97319835\n",
      "            : Test Loss :0.6762756109237671 Test Accuracy : 0.9707199931144714 \n",
      "****************************************************************************************************\n",
      "Iteration  125 : Loss =  0.64118326   Acc:  0.97319\n",
      "            : Test Loss :0.6744560599327087 Test Accuracy : 0.9705299735069275 \n",
      "****************************************************************************************************\n",
      "Iteration  126 : Loss =  0.6392213   Acc:  0.9732533\n",
      "            : Test Loss :0.6729336380958557 Test Accuracy : 0.9703500270843506 \n",
      "****************************************************************************************************\n",
      "Iteration  127 : Loss =  0.63758796   Acc:  0.9732467\n",
      "            : Test Loss :0.6710769534111023 Test Accuracy : 0.9704599976539612 \n",
      "****************************************************************************************************\n",
      "Iteration  128 : Loss =  0.6356656   Acc:  0.973305\n",
      "            : Test Loss :0.6691623330116272 Test Accuracy : 0.9704099893569946 \n",
      "****************************************************************************************************\n",
      "Iteration  129 : Loss =  0.63353425   Acc:  0.9733717\n",
      "            : Test Loss :0.6665749549865723 Test Accuracy : 0.9707000255584717 \n",
      "****************************************************************************************************\n",
      "Iteration  130 : Loss =  0.6309488   Acc:  0.97343165\n",
      "            : Test Loss :0.6642498970031738 Test Accuracy : 0.9708399772644043 \n",
      "****************************************************************************************************\n",
      "Iteration  131 : Loss =  0.62855315   Acc:  0.9734567\n",
      "            : Test Loss :0.6622584462165833 Test Accuracy : 0.9709100127220154 \n",
      "****************************************************************************************************\n",
      "Iteration  132 : Loss =  0.6264299   Acc:  0.97358\n",
      "            : Test Loss :0.6604321002960205 Test Accuracy : 0.9708600044250488 \n",
      "****************************************************************************************************\n",
      "Iteration  133 : Loss =  0.6245366   Acc:  0.97360665\n",
      "            : Test Loss :0.658782422542572 Test Accuracy : 0.9708600044250488 \n",
      "****************************************************************************************************\n",
      "Iteration  134 : Loss =  0.62272   Acc:  0.97366834\n",
      "            : Test Loss :0.6570250988006592 Test Accuracy : 0.9708899855613708 \n",
      "****************************************************************************************************\n",
      "Iteration  135 : Loss =  0.6209334   Acc:  0.97365665\n",
      "            : Test Loss :0.6556369066238403 Test Accuracy : 0.9707300066947937 \n",
      "****************************************************************************************************\n",
      "Iteration  136 : Loss =  0.6193346   Acc:  0.9737433\n",
      "            : Test Loss :0.6541538834571838 Test Accuracy : 0.971019983291626 \n",
      "****************************************************************************************************\n",
      "Iteration  137 : Loss =  0.6177913   Acc:  0.97377336\n",
      "            : Test Loss :0.6534477472305298 Test Accuracy : 0.970740020275116 \n",
      "****************************************************************************************************\n",
      "Iteration  138 : Loss =  0.6169355   Acc:  0.9736267\n",
      "            : Test Loss :0.6522401571273804 Test Accuracy : 0.9708999991416931 \n",
      "****************************************************************************************************\n",
      "Iteration  139 : Loss =  0.61567914   Acc:  0.97369\n",
      "            : Test Loss :0.652130663394928 Test Accuracy : 0.9708999991416931 \n",
      "****************************************************************************************************\n",
      "Iteration  140 : Loss =  0.61544406   Acc:  0.97350836\n",
      "            : Test Loss :0.6494535207748413 Test Accuracy : 0.9709600210189819 \n",
      "****************************************************************************************************\n",
      "Iteration  141 : Loss =  0.61273277   Acc:  0.97374\n",
      "            : Test Loss :0.6471027135848999 Test Accuracy : 0.9711400270462036 \n",
      "****************************************************************************************************\n",
      "Iteration  142 : Loss =  0.610302   Acc:  0.9738017\n",
      "            : Test Loss :0.6432909965515137 Test Accuracy : 0.9711599946022034 \n",
      "****************************************************************************************************\n",
      "Iteration  143 : Loss =  0.60647607   Acc:  0.9741\n",
      "            : Test Loss :0.640619158744812 Test Accuracy : 0.9714900255203247 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  144 : Loss =  0.6037262   Acc:  0.97411835\n",
      "            : Test Loss :0.6392908096313477 Test Accuracy : 0.9713699817657471 \n",
      "****************************************************************************************************\n",
      "Iteration  145 : Loss =  0.6022287   Acc:  0.9741333\n",
      "            : Test Loss :0.6386817097663879 Test Accuracy : 0.9712799787521362 \n",
      "****************************************************************************************************\n",
      "Iteration  146 : Loss =  0.60148835   Acc:  0.974205\n",
      "            : Test Loss :0.6385625004768372 Test Accuracy : 0.9712100028991699 \n",
      "****************************************************************************************************\n",
      "Iteration  147 : Loss =  0.6011741   Acc:  0.97401667\n",
      "            : Test Loss :0.6369011402130127 Test Accuracy : 0.9712200164794922 \n",
      "****************************************************************************************************\n",
      "Iteration  148 : Loss =  0.59944856   Acc:  0.97415835\n",
      "            : Test Loss :0.6354577541351318 Test Accuracy : 0.9711599946022034 \n",
      "****************************************************************************************************\n",
      "Iteration  149 : Loss =  0.59782135   Acc:  0.97405833\n",
      "            : Test Loss :0.6324442625045776 Test Accuracy : 0.9713699817657471 \n",
      "****************************************************************************************************\n",
      "Iteration  150 : Loss =  0.5949092   Acc:  0.9743633\n",
      "            : Test Loss :0.6306570172309875 Test Accuracy : 0.9713699817657471 \n",
      "****************************************************************************************************\n",
      "Iteration  151 : Loss =  0.59291625   Acc:  0.97434336\n",
      "            : Test Loss :0.6296561360359192 Test Accuracy : 0.9715800285339355 \n",
      "****************************************************************************************************\n",
      "Iteration  152 : Loss =  0.5919885   Acc:  0.9742733\n",
      "            : Test Loss :0.6303315162658691 Test Accuracy : 0.9712200164794922 \n",
      "****************************************************************************************************\n",
      "Iteration  153 : Loss =  0.5923803   Acc:  0.97421664\n",
      "            : Test Loss :0.6300890445709229 Test Accuracy : 0.9712700247764587 \n",
      "****************************************************************************************************\n",
      "Iteration  154 : Loss =  0.5922047   Acc:  0.97405\n",
      "            : Test Loss :0.6293187141418457 Test Accuracy : 0.9712700247764587 \n",
      "****************************************************************************************************\n",
      "Iteration  155 : Loss =  0.59110034   Acc:  0.97410166\n",
      "            : Test Loss :0.624587893486023 Test Accuracy : 0.9715999960899353 \n",
      "****************************************************************************************************\n",
      "Iteration  156 : Loss =  0.58650863   Acc:  0.97436\n",
      "            : Test Loss :0.6210595369338989 Test Accuracy : 0.9716100096702576 \n",
      "****************************************************************************************************\n",
      "Iteration  157 : Loss =  0.58279926   Acc:  0.97467834\n",
      "            : Test Loss :0.6202911138534546 Test Accuracy : 0.9715399742126465 \n",
      "****************************************************************************************************\n",
      "Iteration  158 : Loss =  0.5819232   Acc:  0.97475165\n",
      "            : Test Loss :0.6208193898200989 Test Accuracy : 0.9718899726867676 \n",
      "****************************************************************************************************\n",
      "Iteration  159 : Loss =  0.58235085   Acc:  0.9745167\n",
      "            : Test Loss :0.6198952198028564 Test Accuracy : 0.9716100096702576 \n",
      "****************************************************************************************************\n",
      "Iteration  160 : Loss =  0.58132565   Acc:  0.97459334\n",
      "            : Test Loss :0.6165207624435425 Test Accuracy : 0.9720100164413452 \n",
      "****************************************************************************************************\n",
      "Iteration  161 : Loss =  0.5778902   Acc:  0.97472167\n",
      "            : Test Loss :0.6139799952507019 Test Accuracy : 0.9718999862670898 \n",
      "****************************************************************************************************\n",
      "Iteration  162 : Loss =  0.5752218   Acc:  0.9748883\n",
      "            : Test Loss :0.6134631633758545 Test Accuracy : 0.9718599915504456 \n",
      "****************************************************************************************************\n",
      "Iteration  163 : Loss =  0.57459384   Acc:  0.9749733\n",
      "            : Test Loss :0.6133350729942322 Test Accuracy : 0.972029983997345 \n",
      "****************************************************************************************************\n",
      "Iteration  164 : Loss =  0.5744083   Acc:  0.97475\n",
      "            : Test Loss :0.6121208071708679 Test Accuracy : 0.9718599915504456 \n",
      "****************************************************************************************************\n",
      "Iteration  165 : Loss =  0.57304466   Acc:  0.974885\n",
      "            : Test Loss :0.60943603515625 Test Accuracy : 0.9721099734306335 \n",
      "****************************************************************************************************\n",
      "Iteration  166 : Loss =  0.57031476   Acc:  0.97495836\n",
      "            : Test Loss :0.6075125932693481 Test Accuracy : 0.972029983997345 \n",
      "****************************************************************************************************\n",
      "Iteration  167 : Loss =  0.5682862   Acc:  0.97507834\n",
      "            : Test Loss :0.606894850730896 Test Accuracy : 0.9719600081443787 \n",
      "****************************************************************************************************\n",
      "Iteration  168 : Loss =  0.56752586   Acc:  0.97510165\n",
      "            : Test Loss :0.606377899646759 Test Accuracy : 0.9720500111579895 \n",
      "****************************************************************************************************\n",
      "Iteration  169 : Loss =  0.566984   Acc:  0.97497\n",
      "            : Test Loss :0.6054109334945679 Test Accuracy : 0.9719499945640564 \n",
      "****************************************************************************************************\n",
      "Iteration  170 : Loss =  0.56582546   Acc:  0.97505665\n",
      "            : Test Loss :0.6037284135818481 Test Accuracy : 0.9720399975776672 \n",
      "****************************************************************************************************\n",
      "Iteration  171 : Loss =  0.56409955   Acc:  0.97510165\n",
      "            : Test Loss :0.6035352349281311 Test Accuracy : 0.9719600081443787 \n",
      "****************************************************************************************************\n",
      "Iteration  172 : Loss =  0.56365335   Acc:  0.97507334\n",
      "            : Test Loss :0.6057922840118408 Test Accuracy : 0.9718499779701233 \n",
      "****************************************************************************************************\n",
      "Iteration  173 : Loss =  0.56595606   Acc:  0.97493166\n",
      "            : Test Loss :0.6167605519294739 Test Accuracy : 0.9710299968719482 \n",
      "****************************************************************************************************\n",
      "Iteration  174 : Loss =  0.5762397   Acc:  0.9739867\n",
      "            : Test Loss :0.6136112809181213 Test Accuracy : 0.9709299802780151 \n",
      "****************************************************************************************************\n",
      "Iteration  175 : Loss =  0.5734806   Acc:  0.9739917\n",
      "            : Test Loss :0.6102454662322998 Test Accuracy : 0.9713000059127808 \n",
      "****************************************************************************************************\n",
      "Iteration  176 : Loss =  0.5698698   Acc:  0.97428334\n",
      "            : Test Loss :0.5969780683517456 Test Accuracy : 0.9721699953079224 \n",
      "****************************************************************************************************\n",
      "Iteration  177 : Loss =  0.5573243   Acc:  0.975295\n",
      "            : Test Loss :0.6021861433982849 Test Accuracy : 0.9719099998474121 \n",
      "****************************************************************************************************\n",
      "Iteration  178 : Loss =  0.5623552   Acc:  0.9748783\n",
      "            : Test Loss :0.6124765872955322 Test Accuracy : 0.9710900187492371 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  179 : Loss =  0.5718961   Acc:  0.974045\n",
      "            : Test Loss :0.5943878889083862 Test Accuracy : 0.9720699787139893 \n",
      "****************************************************************************************************\n",
      "Iteration  180 : Loss =  0.55441403   Acc:  0.9753733\n",
      "            : Test Loss :0.6019735336303711 Test Accuracy : 0.971809983253479 \n",
      "****************************************************************************************************\n",
      "Iteration  181 : Loss =  0.56181514   Acc:  0.9748\n",
      "            : Test Loss :0.6143583059310913 Test Accuracy : 0.9708300232887268 \n",
      "****************************************************************************************************\n",
      "Iteration  182 : Loss =  0.57336605   Acc:  0.97387666\n",
      "            : Test Loss :0.5902697443962097 Test Accuracy : 0.9725000262260437 \n",
      "****************************************************************************************************\n",
      "Iteration  183 : Loss =  0.54974055   Acc:  0.9755167\n",
      "            : Test Loss :0.6131359338760376 Test Accuracy : 0.9706000089645386 \n",
      "****************************************************************************************************\n",
      "Iteration  184 : Loss =  0.5721814   Acc:  0.97355336\n",
      "            : Test Loss :0.6208549737930298 Test Accuracy : 0.9705399870872498 \n",
      "****************************************************************************************************\n",
      "Iteration  185 : Loss =  0.5794954   Acc:  0.9733533\n",
      "            : Test Loss :0.5946537852287292 Test Accuracy : 0.9721800088882446 \n",
      "****************************************************************************************************\n",
      "Iteration  186 : Loss =  0.5536615   Acc:  0.9750133\n",
      "            : Test Loss :0.6357406377792358 Test Accuracy : 0.9684000015258789 \n",
      "****************************************************************************************************\n",
      "Iteration  187 : Loss =  0.59404194   Acc:  0.971455\n",
      "            : Test Loss :0.6039104461669922 Test Accuracy : 0.9712600111961365 \n",
      "****************************************************************************************************\n",
      "Iteration  188 : Loss =  0.5631492   Acc:  0.974345\n",
      "            : Test Loss :0.6133085489273071 Test Accuracy : 0.9710100293159485 \n",
      "****************************************************************************************************\n",
      "Iteration  189 : Loss =  0.57217073   Acc:  0.9737717\n",
      "            : Test Loss :0.5951268672943115 Test Accuracy : 0.971780002117157 \n",
      "****************************************************************************************************\n",
      "Iteration  190 : Loss =  0.55436945   Acc:  0.97480834\n",
      "            : Test Loss :0.5989165306091309 Test Accuracy : 0.9717000126838684 \n",
      "****************************************************************************************************\n",
      "Iteration  191 : Loss =  0.5583169   Acc:  0.97448164\n",
      "            : Test Loss :0.5989681482315063 Test Accuracy : 0.971530020236969 \n",
      "****************************************************************************************************\n",
      "Iteration  192 : Loss =  0.558375   Acc:  0.97462165\n",
      "            : Test Loss :0.597743034362793 Test Accuracy : 0.9714199900627136 \n",
      "****************************************************************************************************\n",
      "Iteration  193 : Loss =  0.5566467   Acc:  0.97451335\n",
      "            : Test Loss :0.5906496644020081 Test Accuracy : 0.9720500111579895 \n",
      "****************************************************************************************************\n",
      "Iteration  194 : Loss =  0.54961133   Acc:  0.9749967\n",
      "            : Test Loss :0.5895991921424866 Test Accuracy : 0.9720799922943115 \n",
      "****************************************************************************************************\n",
      "Iteration  195 : Loss =  0.5487892   Acc:  0.975145\n",
      "            : Test Loss :0.5900360345840454 Test Accuracy : 0.9719799757003784 \n",
      "****************************************************************************************************\n",
      "Iteration  196 : Loss =  0.5489363   Acc:  0.9750017\n",
      "            : Test Loss :0.5890398025512695 Test Accuracy : 0.9720500111579895 \n",
      "****************************************************************************************************\n",
      "Iteration  197 : Loss =  0.5474539   Acc:  0.97500664\n",
      "            : Test Loss :0.5841339826583862 Test Accuracy : 0.9723899960517883 \n",
      "****************************************************************************************************\n",
      "Iteration  198 : Loss =  0.5427871   Acc:  0.97549\n",
      "            : Test Loss :0.5837575793266296 Test Accuracy : 0.9722899794578552 \n",
      "****************************************************************************************************\n",
      "Iteration  199 : Loss =  0.5426321   Acc:  0.9755433\n",
      "            : Test Loss :0.5830114483833313 Test Accuracy : 0.972540020942688 \n",
      "****************************************************************************************************\n",
      "Iteration  200 : Loss =  0.54143316   Acc:  0.9753867\n",
      "            : Test Loss :0.5827630758285522 Test Accuracy : 0.9723899960517883 \n",
      "****************************************************************************************************\n",
      "Iteration  201 : Loss =  0.5408056   Acc:  0.975355\n",
      "            : Test Loss :0.5787827968597412 Test Accuracy : 0.9725499749183655 \n",
      "****************************************************************************************************\n",
      "Iteration  202 : Loss =  0.53724575   Acc:  0.9758217\n",
      "            : Test Loss :0.5789141654968262 Test Accuracy : 0.9723700284957886 \n",
      "****************************************************************************************************\n",
      "Iteration  203 : Loss =  0.537429   Acc:  0.97576165\n",
      "            : Test Loss :0.5777392387390137 Test Accuracy : 0.9728900194168091 \n",
      "****************************************************************************************************\n",
      "Iteration  204 : Loss =  0.53571296   Acc:  0.9757183\n",
      "            : Test Loss :0.5777521729469299 Test Accuracy : 0.9726799726486206 \n",
      "****************************************************************************************************\n",
      "Iteration  205 : Loss =  0.53545856   Acc:  0.9755933\n",
      "            : Test Loss :0.5745507478713989 Test Accuracy : 0.9725300073623657 \n",
      "****************************************************************************************************\n",
      "Iteration  206 : Loss =  0.5327507   Acc:  0.975975\n",
      "            : Test Loss :0.5744795203208923 Test Accuracy : 0.972540020942688 \n",
      "****************************************************************************************************\n",
      "Iteration  207 : Loss =  0.53272355   Acc:  0.97599334\n",
      "            : Test Loss :0.5735230445861816 Test Accuracy : 0.9729099869728088 \n",
      "****************************************************************************************************\n",
      "Iteration  208 : Loss =  0.5311259   Acc:  0.9759517\n",
      "            : Test Loss :0.573380708694458 Test Accuracy : 0.9728999733924866 \n",
      "****************************************************************************************************\n",
      "Iteration  209 : Loss =  0.53077304   Acc:  0.97590834\n",
      "            : Test Loss :0.5709269046783447 Test Accuracy : 0.9726300239562988 \n",
      "****************************************************************************************************\n",
      "Iteration  210 : Loss =  0.52888954   Acc:  0.97617\n",
      "            : Test Loss :0.5704883337020874 Test Accuracy : 0.9728000164031982 \n",
      "****************************************************************************************************\n",
      "Iteration  211 : Loss =  0.52840114   Acc:  0.97617334\n",
      "            : Test Loss :0.5700527429580688 Test Accuracy : 0.9729999899864197 \n",
      "****************************************************************************************************\n",
      "Iteration  212 : Loss =  0.52733845   Acc:  0.9761083\n",
      "            : Test Loss :0.5692977905273438 Test Accuracy : 0.9729800224304199 \n",
      "****************************************************************************************************\n",
      "Iteration  213 : Loss =  0.5265208   Acc:  0.97611\n",
      "            : Test Loss :0.5678470730781555 Test Accuracy : 0.9729999899864197 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  214 : Loss =  0.5255011   Acc:  0.97627336\n",
      "            : Test Loss :0.5668022632598877 Test Accuracy : 0.9729800224304199 \n",
      "****************************************************************************************************\n",
      "Iteration  215 : Loss =  0.5243883   Acc:  0.97631\n",
      "            : Test Loss :0.5669001936912537 Test Accuracy : 0.9729400277137756 \n",
      "****************************************************************************************************\n",
      "Iteration  216 : Loss =  0.5239197   Acc:  0.976245\n",
      "            : Test Loss :0.5655995011329651 Test Accuracy : 0.9731400012969971 \n",
      "****************************************************************************************************\n",
      "Iteration  217 : Loss =  0.52263623   Acc:  0.976285\n",
      "            : Test Loss :0.5649755597114563 Test Accuracy : 0.9730100035667419 \n",
      "****************************************************************************************************\n",
      "Iteration  218 : Loss =  0.5223053   Acc:  0.97637165\n",
      "            : Test Loss :0.5636513233184814 Test Accuracy : 0.9731600284576416 \n",
      "****************************************************************************************************\n",
      "Iteration  219 : Loss =  0.5207597   Acc:  0.97642833\n",
      "            : Test Loss :0.563987135887146 Test Accuracy : 0.9729599952697754 \n",
      "****************************************************************************************************\n",
      "Iteration  220 : Loss =  0.52072775   Acc:  0.976345\n",
      "            : Test Loss :0.5623244047164917 Test Accuracy : 0.9731500148773193 \n",
      "****************************************************************************************************\n",
      "Iteration  221 : Loss =  0.51912755   Acc:  0.9765\n",
      "            : Test Loss :0.5623047947883606 Test Accuracy : 0.9731699824333191 \n",
      "****************************************************************************************************\n",
      "Iteration  222 : Loss =  0.5191953   Acc:  0.97645664\n",
      "            : Test Loss :0.560762345790863 Test Accuracy : 0.9732400178909302 \n",
      "****************************************************************************************************\n",
      "Iteration  223 : Loss =  0.5174582   Acc:  0.9765983\n",
      "            : Test Loss :0.5612202882766724 Test Accuracy : 0.9731400012969971 \n",
      "****************************************************************************************************\n",
      "Iteration  224 : Loss =  0.51761204   Acc:  0.976515\n",
      "            : Test Loss :0.5594807863235474 Test Accuracy : 0.9732099771499634 \n",
      "****************************************************************************************************\n",
      "Iteration  225 : Loss =  0.51598686   Acc:  0.9766167\n",
      "            : Test Loss :0.5596290826797485 Test Accuracy : 0.9732900261878967 \n",
      "****************************************************************************************************\n",
      "Iteration  226 : Loss =  0.51611227   Acc:  0.97656333\n",
      "            : Test Loss :0.5581949353218079 Test Accuracy : 0.9731699824333191 \n",
      "****************************************************************************************************\n",
      "Iteration  227 : Loss =  0.51456505   Acc:  0.97663665\n",
      "            : Test Loss :0.5583693981170654 Test Accuracy : 0.9731600284576416 \n",
      "****************************************************************************************************\n",
      "Iteration  228 : Loss =  0.5144764   Acc:  0.97659\n",
      "            : Test Loss :0.5569754242897034 Test Accuracy : 0.9732800126075745 \n",
      "****************************************************************************************************\n",
      "Iteration  229 : Loss =  0.5132081   Acc:  0.9767417\n",
      "            : Test Loss :0.5567376613616943 Test Accuracy : 0.9733499884605408 \n",
      "****************************************************************************************************\n",
      "Iteration  230 : Loss =  0.51291025   Acc:  0.97671664\n",
      "            : Test Loss :0.5558823347091675 Test Accuracy : 0.9732400178909302 \n",
      "****************************************************************************************************\n",
      "Iteration  231 : Loss =  0.5119573   Acc:  0.97675335\n",
      "            : Test Loss :0.5555692911148071 Test Accuracy : 0.9732999801635742 \n",
      "****************************************************************************************************\n",
      "Iteration  232 : Loss =  0.5114119   Acc:  0.97676665\n",
      "            : Test Loss :0.5547969341278076 Test Accuracy : 0.9733499884605408 \n",
      "****************************************************************************************************\n",
      "Iteration  233 : Loss =  0.5108587   Acc:  0.97677\n",
      "            : Test Loss :0.5544897317886353 Test Accuracy : 0.9733700156211853 \n",
      "****************************************************************************************************\n",
      "Iteration  234 : Loss =  0.510288   Acc:  0.97682166\n",
      "            : Test Loss :0.5546029806137085 Test Accuracy : 0.9731699824333191 \n",
      "****************************************************************************************************\n",
      "Iteration  235 : Loss =  0.51040435   Acc:  0.9767017\n",
      "            : Test Loss :0.5551239252090454 Test Accuracy : 0.9732800126075745 \n",
      "****************************************************************************************************\n",
      "Iteration  236 : Loss =  0.5105034   Acc:  0.9767183\n",
      "            : Test Loss :0.5563931465148926 Test Accuracy : 0.9730700254440308 \n",
      "****************************************************************************************************\n",
      "Iteration  237 : Loss =  0.51237977   Acc:  0.97644836\n",
      "            : Test Loss :0.557289719581604 Test Accuracy : 0.9731900095939636 \n",
      "****************************************************************************************************\n",
      "Iteration  238 : Loss =  0.512447   Acc:  0.9763833\n",
      "            : Test Loss :0.5567774772644043 Test Accuracy : 0.9728900194168091 \n",
      "****************************************************************************************************\n",
      "Iteration  239 : Loss =  0.51239187   Acc:  0.9763467\n",
      "            : Test Loss :0.5534350872039795 Test Accuracy : 0.9733800292015076 \n",
      "****************************************************************************************************\n",
      "Iteration  240 : Loss =  0.50863975   Acc:  0.97670335\n",
      "            : Test Loss :0.5501822829246521 Test Accuracy : 0.9734100103378296 \n",
      "****************************************************************************************************\n",
      "Iteration  241 : Loss =  0.5056783   Acc:  0.9769383\n",
      "            : Test Loss :0.5497216582298279 Test Accuracy : 0.9733700156211853 \n",
      "****************************************************************************************************\n",
      "Iteration  242 : Loss =  0.5049331   Acc:  0.97698\n",
      "            : Test Loss :0.5515779852867126 Test Accuracy : 0.9734200239181519 \n",
      "****************************************************************************************************\n",
      "Iteration  243 : Loss =  0.50639033   Acc:  0.97678334\n",
      "            : Test Loss :0.5529615879058838 Test Accuracy : 0.9729999899864197 \n",
      "****************************************************************************************************\n",
      "Iteration  244 : Loss =  0.50826293   Acc:  0.9765133\n",
      "            : Test Loss :0.5520700216293335 Test Accuracy : 0.9734200239181519 \n",
      "****************************************************************************************************\n",
      "Iteration  245 : Loss =  0.50664455   Acc:  0.9767\n",
      "            : Test Loss :0.5488522052764893 Test Accuracy : 0.9733700156211853 \n",
      "****************************************************************************************************\n",
      "Iteration  246 : Loss =  0.5037509   Acc:  0.97686166\n",
      "            : Test Loss :0.5466355085372925 Test Accuracy : 0.9734500050544739 \n",
      "****************************************************************************************************\n",
      "Iteration  247 : Loss =  0.50146353   Acc:  0.97710836\n",
      "            : Test Loss :0.5473619103431702 Test Accuracy : 0.9734899997711182 \n",
      "****************************************************************************************************\n",
      "Iteration  248 : Loss =  0.5019608   Acc:  0.9770283\n",
      "            : Test Loss :0.5481482744216919 Test Accuracy : 0.9733899831771851 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  249 : Loss =  0.5028752   Acc:  0.97679836\n",
      "            : Test Loss :0.547921895980835 Test Accuracy : 0.9735699892044067 \n",
      "****************************************************************************************************\n",
      "Iteration  250 : Loss =  0.5021588   Acc:  0.97695\n",
      "            : Test Loss :0.5456954836845398 Test Accuracy : 0.9735199809074402 \n",
      "****************************************************************************************************\n",
      "Iteration  251 : Loss =  0.50029206   Acc:  0.9770733\n",
      "            : Test Loss :0.5442049503326416 Test Accuracy : 0.9735299944877625 \n",
      "****************************************************************************************************\n",
      "Iteration  252 : Loss =  0.49850821   Acc:  0.9772417\n",
      "            : Test Loss :0.5441569089889526 Test Accuracy : 0.9737799763679504 \n",
      "****************************************************************************************************\n",
      "Iteration  253 : Loss =  0.49827915   Acc:  0.977255\n",
      "            : Test Loss :0.5443780422210693 Test Accuracy : 0.9734899997711182 \n",
      "****************************************************************************************************\n",
      "Iteration  254 : Loss =  0.49869198   Acc:  0.97713333\n",
      "            : Test Loss :0.5445486307144165 Test Accuracy : 0.9736199975013733 \n",
      "****************************************************************************************************\n",
      "Iteration  255 : Loss =  0.4984473   Acc:  0.977145\n",
      "            : Test Loss :0.5429731607437134 Test Accuracy : 0.9735400080680847 \n",
      "****************************************************************************************************\n",
      "Iteration  256 : Loss =  0.49710238   Acc:  0.977185\n",
      "            : Test Loss :0.5418992638587952 Test Accuracy : 0.9738100171089172 \n",
      "****************************************************************************************************\n",
      "Iteration  257 : Loss =  0.49570358   Acc:  0.97734\n",
      "            : Test Loss :0.541214108467102 Test Accuracy : 0.9736199975013733 \n",
      "****************************************************************************************************\n",
      "Iteration  258 : Loss =  0.49504045   Acc:  0.97734666\n",
      "            : Test Loss :0.5411179661750793 Test Accuracy : 0.9736599922180176 \n",
      "****************************************************************************************************\n",
      "Iteration  259 : Loss =  0.49503657   Acc:  0.977285\n",
      "            : Test Loss :0.5415284633636475 Test Accuracy : 0.973829984664917 \n",
      "****************************************************************************************************\n",
      "Iteration  260 : Loss =  0.49497315   Acc:  0.9772867\n",
      "            : Test Loss :0.5407024621963501 Test Accuracy : 0.9736300110816956 \n",
      "****************************************************************************************************\n",
      "Iteration  261 : Loss =  0.494441   Acc:  0.97726834\n",
      "            : Test Loss :0.5399609208106995 Test Accuracy : 0.9737399816513062 \n",
      "****************************************************************************************************\n",
      "Iteration  262 : Loss =  0.49339873   Acc:  0.977395\n",
      "            : Test Loss :0.5387855172157288 Test Accuracy : 0.9736700057983398 \n",
      "****************************************************************************************************\n",
      "Iteration  263 : Loss =  0.49229902   Acc:  0.97742\n",
      "            : Test Loss :0.5382320284843445 Test Accuracy : 0.973800003528595 \n",
      "****************************************************************************************************\n",
      "Iteration  264 : Loss =  0.49156415   Acc:  0.977485\n",
      "            : Test Loss :0.538047730922699 Test Accuracy : 0.9738699793815613 \n",
      "****************************************************************************************************\n",
      "Iteration  265 : Loss =  0.49125493   Acc:  0.97748\n",
      "            : Test Loss :0.537775456905365 Test Accuracy : 0.9737300276756287 \n",
      "****************************************************************************************************\n",
      "Iteration  266 : Loss =  0.49106705   Acc:  0.97742665\n",
      "            : Test Loss :0.5379360914230347 Test Accuracy : 0.9739900231361389 \n",
      "****************************************************************************************************\n",
      "Iteration  267 : Loss =  0.49082905   Acc:  0.97747\n",
      "            : Test Loss :0.5372220873832703 Test Accuracy : 0.9737100005149841 \n",
      "****************************************************************************************************\n",
      "Iteration  268 : Loss =  0.4904548   Acc:  0.977415\n",
      "            : Test Loss :0.5369900465011597 Test Accuracy : 0.9739000201225281 \n",
      "****************************************************************************************************\n",
      "Iteration  269 : Loss =  0.48975867   Acc:  0.977515\n",
      "            : Test Loss :0.5359916687011719 Test Accuracy : 0.9736999869346619 \n",
      "****************************************************************************************************\n",
      "Iteration  270 : Loss =  0.48898017   Acc:  0.9774633\n",
      "            : Test Loss :0.535362958908081 Test Accuracy : 0.9740200042724609 \n",
      "****************************************************************************************************\n",
      "Iteration  271 : Loss =  0.48807043   Acc:  0.97762334\n",
      "            : Test Loss :0.5345129370689392 Test Accuracy : 0.9738600254058838 \n",
      "****************************************************************************************************\n",
      "Iteration  272 : Loss =  0.48729864   Acc:  0.97760165\n",
      "            : Test Loss :0.5340716242790222 Test Accuracy : 0.9739400148391724 \n",
      "****************************************************************************************************\n",
      "Iteration  273 : Loss =  0.48665762   Acc:  0.9776217\n",
      "            : Test Loss :0.5336848497390747 Test Accuracy : 0.9739400148391724 \n",
      "****************************************************************************************************\n",
      "Iteration  274 : Loss =  0.48616076   Acc:  0.97765166\n",
      "            : Test Loss :0.5332579612731934 Test Accuracy : 0.9738799929618835 \n",
      "****************************************************************************************************\n",
      "Iteration  275 : Loss =  0.48579058   Acc:  0.97766\n",
      "            : Test Loss :0.533263087272644 Test Accuracy : 0.974120020866394 \n",
      "****************************************************************************************************\n",
      "Iteration  276 : Loss =  0.48549718   Acc:  0.97772336\n",
      "            : Test Loss :0.5329319834709167 Test Accuracy : 0.9738199710845947 \n",
      "****************************************************************************************************\n",
      "Iteration  277 : Loss =  0.4854061   Acc:  0.9776033\n",
      "            : Test Loss :0.5334895849227905 Test Accuracy : 0.9739300012588501 \n",
      "****************************************************************************************************\n",
      "Iteration  278 : Loss =  0.4854889   Acc:  0.97766\n",
      "            : Test Loss :0.5335292220115662 Test Accuracy : 0.9738699793815613 \n",
      "****************************************************************************************************\n",
      "Iteration  279 : Loss =  0.48600048   Acc:  0.97745335\n",
      "            : Test Loss :0.5348109006881714 Test Accuracy : 0.9737499952316284 \n",
      "****************************************************************************************************\n",
      "Iteration  280 : Loss =  0.48646882   Acc:  0.97752166\n",
      "            : Test Loss :0.5350443720817566 Test Accuracy : 0.9735900163650513 \n",
      "****************************************************************************************************\n",
      "Iteration  281 : Loss =  0.48748863   Acc:  0.9772767\n",
      "            : Test Loss :0.5354137420654297 Test Accuracy : 0.9737200140953064 \n",
      "****************************************************************************************************\n",
      "Iteration  282 : Loss =  0.48690447   Acc:  0.9774017\n",
      "            : Test Loss :0.5334759950637817 Test Accuracy : 0.9737499952316284 \n",
      "****************************************************************************************************\n",
      "Iteration  283 : Loss =  0.48565415   Acc:  0.9773717\n",
      "            : Test Loss :0.5312445163726807 Test Accuracy : 0.9740899801254272 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  284 : Loss =  0.48284066   Acc:  0.9777217\n",
      "            : Test Loss :0.528977632522583 Test Accuracy : 0.9740599989891052 \n",
      "****************************************************************************************************\n",
      "Iteration  285 : Loss =  0.4807827   Acc:  0.97780836\n",
      "            : Test Loss :0.5285831093788147 Test Accuracy : 0.9739300012588501 \n",
      "****************************************************************************************************\n",
      "Iteration  286 : Loss =  0.48026502   Acc:  0.977855\n",
      "            : Test Loss :0.5297030210494995 Test Accuracy : 0.9740999937057495 \n",
      "****************************************************************************************************\n",
      "Iteration  287 : Loss =  0.48098195   Acc:  0.97780836\n",
      "            : Test Loss :0.5305989384651184 Test Accuracy : 0.9738500118255615 \n",
      "****************************************************************************************************\n",
      "Iteration  288 : Loss =  0.48227912   Acc:  0.97757167\n",
      "            : Test Loss :0.5317047834396362 Test Accuracy : 0.9739099740982056 \n",
      "****************************************************************************************************\n",
      "Iteration  289 : Loss =  0.48258388   Acc:  0.977625\n",
      "            : Test Loss :0.5305715799331665 Test Accuracy : 0.973800003528595 \n",
      "****************************************************************************************************\n",
      "Iteration  290 : Loss =  0.48221314   Acc:  0.9775233\n",
      "            : Test Loss :0.5290126800537109 Test Accuracy : 0.9740399718284607 \n",
      "****************************************************************************************************\n",
      "Iteration  291 : Loss =  0.47994822   Acc:  0.9778067\n",
      "            : Test Loss :0.5264041423797607 Test Accuracy : 0.9739800095558167 \n",
      "****************************************************************************************************\n",
      "Iteration  292 : Loss =  0.47775805   Acc:  0.97790664\n",
      "            : Test Loss :0.5256580114364624 Test Accuracy : 0.9741799831390381 \n",
      "****************************************************************************************************\n",
      "Iteration  293 : Loss =  0.47678587   Acc:  0.977985\n",
      "            : Test Loss :0.526448130607605 Test Accuracy : 0.974049985408783 \n",
      "****************************************************************************************************\n",
      "Iteration  294 : Loss =  0.47722718   Acc:  0.977955\n",
      "            : Test Loss :0.526968777179718 Test Accuracy : 0.974049985408783 \n",
      "****************************************************************************************************\n",
      "Iteration  295 : Loss =  0.47813052   Acc:  0.9777617\n",
      "            : Test Loss :0.5277513265609741 Test Accuracy : 0.9739800095558167 \n",
      "****************************************************************************************************\n",
      "Iteration  296 : Loss =  0.47814134   Acc:  0.97785\n",
      "            : Test Loss :0.5264132022857666 Test Accuracy : 0.9740399718284607 \n",
      "****************************************************************************************************\n",
      "Iteration  297 : Loss =  0.4774453   Acc:  0.9777783\n",
      "            : Test Loss :0.5252353549003601 Test Accuracy : 0.9741899967193604 \n",
      "****************************************************************************************************\n",
      "Iteration  298 : Loss =  0.47569388   Acc:  0.9779933\n",
      "            : Test Loss :0.5234947204589844 Test Accuracy : 0.9741500020027161 \n",
      "****************************************************************************************************\n",
      "Iteration  299 : Loss =  0.47420624   Acc:  0.9780617\n",
      "            : Test Loss :0.522964596748352 Test Accuracy : 0.9742500185966492 \n",
      "****************************************************************************************************\n",
      "Iteration  300 : Loss =  0.47363603   Acc:  0.9781017\n",
      "            : Test Loss :0.5235913395881653 Test Accuracy : 0.9742299914360046 \n",
      "****************************************************************************************************\n",
      "Iteration  301 : Loss =  0.47385263   Acc:  0.9780767\n",
      "            : Test Loss :0.5236236453056335 Test Accuracy : 0.9741100072860718 \n",
      "****************************************************************************************************\n",
      "Iteration  302 : Loss =  0.4742459   Acc:  0.9779933\n",
      "            : Test Loss :0.5240910649299622 Test Accuracy : 0.9741500020027161 \n",
      "****************************************************************************************************\n",
      "Iteration  303 : Loss =  0.474105   Acc:  0.9780383\n",
      "            : Test Loss :0.5231482982635498 Test Accuracy : 0.9740999937057495 \n",
      "****************************************************************************************************\n",
      "Iteration  304 : Loss =  0.47358054   Acc:  0.97802335\n",
      "            : Test Loss :0.5225697755813599 Test Accuracy : 0.9742299914360046 \n",
      "****************************************************************************************************\n",
      "Iteration  305 : Loss =  0.47246343   Acc:  0.97813\n",
      "            : Test Loss :0.5211036801338196 Test Accuracy : 0.9742100238800049 \n",
      "****************************************************************************************************\n",
      "Iteration  306 : Loss =  0.47134098   Acc:  0.9781517\n",
      "            : Test Loss :0.5205353498458862 Test Accuracy : 0.9744300246238708 \n",
      "****************************************************************************************************\n",
      "Iteration  307 : Loss =  0.47048187   Acc:  0.9782583\n",
      "            : Test Loss :0.5201689004898071 Test Accuracy : 0.9744099974632263 \n",
      "****************************************************************************************************\n",
      "Iteration  308 : Loss =  0.47007057   Acc:  0.97831666\n",
      "            : Test Loss :0.5199938416481018 Test Accuracy : 0.9742100238800049 \n",
      "****************************************************************************************************\n",
      "Iteration  309 : Loss =  0.46999395   Acc:  0.97823167\n",
      "            : Test Loss :0.5204293131828308 Test Accuracy : 0.9742900133132935 \n",
      "****************************************************************************************************\n",
      "Iteration  310 : Loss =  0.47000736   Acc:  0.97824836\n",
      "            : Test Loss :0.5200685858726501 Test Accuracy : 0.9742100238800049 \n",
      "****************************************************************************************************\n",
      "Iteration  311 : Loss =  0.4700112   Acc:  0.9782183\n",
      "            : Test Loss :0.5204616785049438 Test Accuracy : 0.9743300080299377 \n",
      "****************************************************************************************************\n",
      "Iteration  312 : Loss =  0.46977302   Acc:  0.978245\n",
      "            : Test Loss :0.5196329951286316 Test Accuracy : 0.9743000268936157 \n",
      "****************************************************************************************************\n",
      "Iteration  313 : Loss =  0.46947765   Acc:  0.97820836\n",
      "            : Test Loss :0.5196762681007385 Test Accuracy : 0.9744200110435486 \n",
      "****************************************************************************************************\n",
      "Iteration  314 : Loss =  0.46884334   Acc:  0.97829336\n",
      "            : Test Loss :0.5183869004249573 Test Accuracy : 0.9743099808692932 \n",
      "****************************************************************************************************\n",
      "Iteration  315 : Loss =  0.46811688   Acc:  0.978285\n",
      "            : Test Loss :0.5181333422660828 Test Accuracy : 0.9744799733161926 \n",
      "****************************************************************************************************\n",
      "Iteration  316 : Loss =  0.46722507   Acc:  0.97841835\n",
      "            : Test Loss :0.517005980014801 Test Accuracy : 0.9743499755859375 \n",
      "****************************************************************************************************\n",
      "Iteration  317 : Loss =  0.46643022   Acc:  0.978385\n",
      "            : Test Loss :0.5166938900947571 Test Accuracy : 0.9744899868965149 \n",
      "****************************************************************************************************\n",
      "Iteration  318 : Loss =  0.46579462   Acc:  0.9784733\n",
      "            : Test Loss :0.5162606835365295 Test Accuracy : 0.9744099974632263 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  319 : Loss =  0.465334   Acc:  0.97846836\n",
      "            : Test Loss :0.5159183740615845 Test Accuracy : 0.9745399951934814 \n",
      "****************************************************************************************************\n",
      "Iteration  320 : Loss =  0.46500498   Acc:  0.978415\n",
      "            : Test Loss :0.5160255432128906 Test Accuracy : 0.9746599793434143 \n",
      "****************************************************************************************************\n",
      "Iteration  321 : Loss =  0.46477968   Acc:  0.9785233\n",
      "            : Test Loss :0.515631914138794 Test Accuracy : 0.9744300246238708 \n",
      "****************************************************************************************************\n",
      "Iteration  322 : Loss =  0.4646949   Acc:  0.978495\n",
      "            : Test Loss :0.5162943601608276 Test Accuracy : 0.9745200276374817 \n",
      "****************************************************************************************************\n",
      "Iteration  323 : Loss =  0.46476656   Acc:  0.97849\n",
      "            : Test Loss :0.5160712599754333 Test Accuracy : 0.974370002746582 \n",
      "****************************************************************************************************\n",
      "Iteration  324 : Loss =  0.46518093   Acc:  0.97840333\n",
      "            : Test Loss :0.5176560878753662 Test Accuracy : 0.97434002161026 \n",
      "****************************************************************************************************\n",
      "Iteration  325 : Loss =  0.46579045   Acc:  0.9783483\n",
      "            : Test Loss :0.5178894996643066 Test Accuracy : 0.9743000268936157 \n",
      "****************************************************************************************************\n",
      "Iteration  326 : Loss =  0.4670664   Acc:  0.97815835\n",
      "            : Test Loss :0.5200504064559937 Test Accuracy : 0.9742000102996826 \n",
      "****************************************************************************************************\n",
      "Iteration  327 : Loss =  0.46784496   Acc:  0.978145\n",
      "            : Test Loss :0.5197102427482605 Test Accuracy : 0.9741299748420715 \n",
      "****************************************************************************************************\n",
      "Iteration  328 : Loss =  0.46876526   Acc:  0.97799164\n",
      "            : Test Loss :0.5191261768341064 Test Accuracy : 0.9743000268936157 \n",
      "****************************************************************************************************\n",
      "Iteration  329 : Loss =  0.466963   Acc:  0.97816\n",
      "            : Test Loss :0.515694797039032 Test Accuracy : 0.97434002161026 \n",
      "****************************************************************************************************\n",
      "Iteration  330 : Loss =  0.46429867   Acc:  0.97829336\n",
      "            : Test Loss :0.5132650136947632 Test Accuracy : 0.9746099710464478 \n",
      "****************************************************************************************************\n",
      "Iteration  331 : Loss =  0.46162224   Acc:  0.978615\n",
      "            : Test Loss :0.5131397247314453 Test Accuracy : 0.9746800065040588 \n",
      "****************************************************************************************************\n",
      "Iteration  332 : Loss =  0.46095416   Acc:  0.97863\n",
      "            : Test Loss :0.5133779644966125 Test Accuracy : 0.9744099974632263 \n",
      "****************************************************************************************************\n",
      "Iteration  333 : Loss =  0.46201402   Acc:  0.978555\n",
      "            : Test Loss :0.5157699584960938 Test Accuracy : 0.9744200110435486 \n",
      "****************************************************************************************************\n",
      "Iteration  334 : Loss =  0.4630049   Acc:  0.97843665\n",
      "            : Test Loss :0.5149465799331665 Test Accuracy : 0.9745000004768372 \n",
      "****************************************************************************************************\n",
      "Iteration  335 : Loss =  0.46331972   Acc:  0.978345\n",
      "            : Test Loss :0.5145148038864136 Test Accuracy : 0.974560022354126 \n",
      "****************************************************************************************************\n",
      "Iteration  336 : Loss =  0.4618955   Acc:  0.97851\n",
      "            : Test Loss :0.5125064253807068 Test Accuracy : 0.974560022354126 \n",
      "****************************************************************************************************\n",
      "Iteration  337 : Loss =  0.46032113   Acc:  0.97849834\n",
      "            : Test Loss :0.5113298892974854 Test Accuracy : 0.9747099876403809 \n",
      "****************************************************************************************************\n",
      "Iteration  338 : Loss =  0.45932603   Acc:  0.97869164\n",
      "            : Test Loss :0.5121076703071594 Test Accuracy : 0.9745399951934814 \n",
      "****************************************************************************************************\n",
      "Iteration  339 : Loss =  0.45928574   Acc:  0.978695\n",
      "            : Test Loss :0.5112485289573669 Test Accuracy : 0.9745299816131592 \n",
      "****************************************************************************************************\n",
      "Iteration  340 : Loss =  0.4594382   Acc:  0.97862\n",
      "            : Test Loss :0.5117535591125488 Test Accuracy : 0.9745299816131592 \n",
      "****************************************************************************************************\n",
      "Iteration  341 : Loss =  0.4587188   Acc:  0.978715\n",
      "            : Test Loss :0.510195791721344 Test Accuracy : 0.97461998462677 \n",
      "****************************************************************************************************\n",
      "Iteration  342 : Loss =  0.457856   Acc:  0.97870666\n",
      "            : Test Loss :0.5100452303886414 Test Accuracy : 0.9749000072479248 \n",
      "****************************************************************************************************\n",
      "Iteration  343 : Loss =  0.4574011   Acc:  0.97879\n",
      "            : Test Loss :0.510543704032898 Test Accuracy : 0.974590003490448 \n",
      "****************************************************************************************************\n",
      "Iteration  344 : Loss =  0.45754606   Acc:  0.97872\n",
      "            : Test Loss :0.5098750591278076 Test Accuracy : 0.9747800230979919 \n",
      "****************************************************************************************************\n",
      "Iteration  345 : Loss =  0.45747998   Acc:  0.9787317\n",
      "            : Test Loss :0.5097408890724182 Test Accuracy : 0.9746999740600586 \n",
      "****************************************************************************************************\n",
      "Iteration  346 : Loss =  0.45662576   Acc:  0.9787833\n",
      "            : Test Loss :0.5080983638763428 Test Accuracy : 0.9749500155448914 \n",
      "****************************************************************************************************\n",
      "Iteration  347 : Loss =  0.45540524   Acc:  0.97885835\n",
      "            : Test Loss :0.5074777603149414 Test Accuracy : 0.9747800230979919 \n",
      "****************************************************************************************************\n",
      "Iteration  348 : Loss =  0.4545719   Acc:  0.97895336\n",
      "            : Test Loss :0.507676362991333 Test Accuracy : 0.9747400283813477 \n",
      "****************************************************************************************************\n",
      "Iteration  349 : Loss =  0.4544878   Acc:  0.9788667\n",
      "            : Test Loss :0.507568895816803 Test Accuracy : 0.9749299883842468 \n",
      "****************************************************************************************************\n",
      "Iteration  350 : Loss =  0.45472473   Acc:  0.9788617\n",
      "            : Test Loss :0.5080331563949585 Test Accuracy : 0.9747700095176697 \n",
      "****************************************************************************************************\n",
      "Iteration  351 : Loss =  0.45469785   Acc:  0.97886\n",
      "            : Test Loss :0.507429301738739 Test Accuracy : 0.9748799800872803 \n",
      "****************************************************************************************************\n",
      "Iteration  352 : Loss =  0.45425624   Acc:  0.97884834\n",
      "            : Test Loss :0.5067893266677856 Test Accuracy : 0.9748600125312805 \n",
      "****************************************************************************************************\n",
      "Iteration  353 : Loss =  0.45365828   Acc:  0.97887665\n",
      "            : Test Loss :0.5067043304443359 Test Accuracy : 0.9749900102615356 \n",
      "****************************************************************************************************\n",
      "Iteration  354 : Loss =  0.4531598   Acc:  0.978945\n",
      "            : Test Loss :0.5060506463050842 Test Accuracy : 0.974810004234314 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  355 : Loss =  0.4529692   Acc:  0.9789067\n",
      "            : Test Loss :0.506716787815094 Test Accuracy : 0.9747400283813477 \n",
      "****************************************************************************************************\n",
      "Iteration  356 : Loss =  0.452909   Acc:  0.97893333\n",
      "            : Test Loss :0.5062159299850464 Test Accuracy : 0.9746699929237366 \n",
      "****************************************************************************************************\n",
      "Iteration  357 : Loss =  0.45290244   Acc:  0.97892\n",
      "            : Test Loss :0.5065804123878479 Test Accuracy : 0.9749400019645691 \n",
      "****************************************************************************************************\n",
      "Iteration  358 : Loss =  0.45268798   Acc:  0.97891164\n",
      "            : Test Loss :0.5059588551521301 Test Accuracy : 0.9746099710464478 \n",
      "****************************************************************************************************\n",
      "Iteration  359 : Loss =  0.45242256   Acc:  0.9788833\n",
      "            : Test Loss :0.5060790777206421 Test Accuracy : 0.9748700261116028 \n",
      "****************************************************************************************************\n",
      "Iteration  360 : Loss =  0.45212615   Acc:  0.978925\n",
      "            : Test Loss :0.5057416558265686 Test Accuracy : 0.9747099876403809 \n",
      "****************************************************************************************************\n",
      "Iteration  361 : Loss =  0.4521349   Acc:  0.97893167\n",
      "            : Test Loss :0.5064459443092346 Test Accuracy : 0.9749900102615356 \n",
      "****************************************************************************************************\n",
      "Iteration  362 : Loss =  0.45234466   Acc:  0.97887164\n",
      "            : Test Loss :0.5067053437232971 Test Accuracy : 0.9746900200843811 \n",
      "****************************************************************************************************\n",
      "Iteration  363 : Loss =  0.45306122   Acc:  0.978815\n",
      "            : Test Loss :0.5075072646141052 Test Accuracy : 0.9748200178146362 \n",
      "****************************************************************************************************\n",
      "Iteration  364 : Loss =  0.45318323   Acc:  0.97879\n",
      "            : Test Loss :0.5071267485618591 Test Accuracy : 0.974590003490448 \n",
      "****************************************************************************************************\n",
      "Iteration  365 : Loss =  0.4533974   Acc:  0.97875834\n",
      "            : Test Loss :0.5065757036209106 Test Accuracy : 0.9749500155448914 \n",
      "****************************************************************************************************\n",
      "Iteration  366 : Loss =  0.45211947   Acc:  0.9788383\n",
      "            : Test Loss :0.5046969652175903 Test Accuracy : 0.9747099876403809 \n",
      "****************************************************************************************************\n",
      "Iteration  367 : Loss =  0.4507655   Acc:  0.978965\n",
      "            : Test Loss :0.5036856532096863 Test Accuracy : 0.9751099944114685 \n",
      "****************************************************************************************************\n",
      "Iteration  368 : Loss =  0.44918475   Acc:  0.97909665\n",
      "            : Test Loss :0.5025521516799927 Test Accuracy : 0.9749199748039246 \n",
      "****************************************************************************************************\n",
      "Iteration  369 : Loss =  0.44830292   Acc:  0.97912335\n",
      "            : Test Loss :0.5026447772979736 Test Accuracy : 0.9749900102615356 \n",
      "****************************************************************************************************\n",
      "Iteration  370 : Loss =  0.448061   Acc:  0.979195\n",
      "            : Test Loss :0.5027799010276794 Test Accuracy : 0.9750999808311462 \n",
      "****************************************************************************************************\n",
      "Iteration  371 : Loss =  0.44824135   Acc:  0.9790983\n",
      "            : Test Loss :0.5028916597366333 Test Accuracy : 0.9748200178146362 \n",
      "****************************************************************************************************\n",
      "Iteration  372 : Loss =  0.44825548   Acc:  0.9791333\n",
      "            : Test Loss :0.5027856826782227 Test Accuracy : 0.9752399921417236 \n",
      "****************************************************************************************************\n",
      "Iteration  373 : Loss =  0.44805127   Acc:  0.97911835\n",
      "            : Test Loss :0.5023788809776306 Test Accuracy : 0.9748499989509583 \n",
      "****************************************************************************************************\n",
      "Iteration  374 : Loss =  0.44777662   Acc:  0.9790817\n",
      "            : Test Loss :0.5023187398910522 Test Accuracy : 0.9751600027084351 \n",
      "****************************************************************************************************\n",
      "Iteration  375 : Loss =  0.44737625   Acc:  0.979115\n",
      "            : Test Loss :0.5018812417984009 Test Accuracy : 0.9748499989509583 \n",
      "****************************************************************************************************\n",
      "Iteration  376 : Loss =  0.44732624   Acc:  0.9791667\n",
      "            : Test Loss :0.5028942227363586 Test Accuracy : 0.9749799966812134 \n",
      "****************************************************************************************************\n",
      "Iteration  377 : Loss =  0.44759458   Acc:  0.979105\n",
      "            : Test Loss :0.5030889511108398 Test Accuracy : 0.9748700261116028 \n",
      "****************************************************************************************************\n",
      "Iteration  378 : Loss =  0.44853854   Acc:  0.9789633\n",
      "            : Test Loss :0.5052285194396973 Test Accuracy : 0.97461998462677 \n",
      "****************************************************************************************************\n",
      "Iteration  379 : Loss =  0.44953752   Acc:  0.9790717\n",
      "            : Test Loss :0.505386471748352 Test Accuracy : 0.9746099710464478 \n",
      "****************************************************************************************************\n",
      "Iteration  380 : Loss =  0.45081747   Acc:  0.978695\n",
      "            : Test Loss :0.5054765939712524 Test Accuracy : 0.9747499823570251 \n",
      "****************************************************************************************************\n",
      "Iteration  381 : Loss =  0.44970596   Acc:  0.97901833\n",
      "            : Test Loss :0.502485990524292 Test Accuracy : 0.9748600125312805 \n",
      "****************************************************************************************************\n",
      "Iteration  382 : Loss =  0.44769758   Acc:  0.9789417\n",
      "            : Test Loss :0.5001294612884521 Test Accuracy : 0.9751999974250793 \n",
      "****************************************************************************************************\n",
      "Iteration  383 : Loss =  0.44484887   Acc:  0.9793017\n",
      "            : Test Loss :0.4990673065185547 Test Accuracy : 0.9750499725341797 \n",
      "****************************************************************************************************\n",
      "Iteration  384 : Loss =  0.44365168   Acc:  0.9793367\n",
      "            : Test Loss :0.4991469979286194 Test Accuracy : 0.9752100110054016 \n",
      "****************************************************************************************************\n",
      "Iteration  385 : Loss =  0.44416595   Acc:  0.9793417\n",
      "            : Test Loss :0.5010870695114136 Test Accuracy : 0.9749199748039246 \n",
      "****************************************************************************************************\n",
      "Iteration  386 : Loss =  0.4451064   Acc:  0.97940165\n",
      "            : Test Loss :0.5010859966278076 Test Accuracy : 0.9747999906539917 \n",
      "****************************************************************************************************\n",
      "Iteration  387 : Loss =  0.44586456   Acc:  0.97901833\n",
      "            : Test Loss :0.5011648535728455 Test Accuracy : 0.9750699996948242 \n",
      "****************************************************************************************************\n",
      "Iteration  388 : Loss =  0.44515234   Acc:  0.979305\n",
      "            : Test Loss :0.5001680254936218 Test Accuracy : 0.974839985370636 \n",
      "****************************************************************************************************\n",
      "Iteration  389 : Loss =  0.44446483   Acc:  0.979125\n",
      "            : Test Loss :0.4992074966430664 Test Accuracy : 0.9750900268554688 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  390 : Loss =  0.44361418   Acc:  0.9793417\n",
      "            : Test Loss :0.4993092715740204 Test Accuracy : 0.9747800230979919 \n",
      "****************************************************************************************************\n",
      "Iteration  391 : Loss =  0.44321755   Acc:  0.97929335\n",
      "            : Test Loss :0.4984961152076721 Test Accuracy : 0.9753000140190125 \n",
      "****************************************************************************************************\n",
      "Iteration  392 : Loss =  0.44292402   Acc:  0.97934\n",
      "            : Test Loss :0.49864619970321655 Test Accuracy : 0.9749400019645691 \n",
      "****************************************************************************************************\n",
      "Iteration  393 : Loss =  0.4425797   Acc:  0.97941333\n",
      "            : Test Loss :0.49850940704345703 Test Accuracy : 0.9752399921417236 \n",
      "****************************************************************************************************\n",
      "Iteration  394 : Loss =  0.44235715   Acc:  0.979325\n",
      "            : Test Loss :0.4981737732887268 Test Accuracy : 0.9749199748039246 \n",
      "****************************************************************************************************\n",
      "Iteration  395 : Loss =  0.44256437   Acc:  0.97934\n",
      "            : Test Loss :0.5001752972602844 Test Accuracy : 0.9750999808311462 \n",
      "****************************************************************************************************\n",
      "Iteration  396 : Loss =  0.44332176   Acc:  0.97920334\n",
      "            : Test Loss :0.4987261891365051 Test Accuracy : 0.9748799800872803 \n",
      "****************************************************************************************************\n",
      "Iteration  397 : Loss =  0.44352782   Acc:  0.97932166\n",
      "            : Test Loss :0.49955660104751587 Test Accuracy : 0.9751399755477905 \n",
      "****************************************************************************************************\n",
      "Iteration  398 : Loss =  0.44260404   Acc:  0.97923166\n",
      "            : Test Loss :0.4968857169151306 Test Accuracy : 0.975059986114502 \n",
      "****************************************************************************************************\n",
      "Iteration  399 : Loss =  0.44132322   Acc:  0.97937167\n",
      "            : Test Loss :0.4969007968902588 Test Accuracy : 0.9753000140190125 \n",
      "****************************************************************************************************\n",
      "Iteration  400 : Loss =  0.44041753   Acc:  0.97941834\n",
      "            : Test Loss :0.4973323941230774 Test Accuracy : 0.9748600125312805 \n",
      "****************************************************************************************************\n",
      "Iteration  401 : Loss =  0.44078183   Acc:  0.97938335\n",
      "            : Test Loss :0.4977613091468811 Test Accuracy : 0.9752299785614014 \n",
      "****************************************************************************************************\n",
      "Iteration  402 : Loss =  0.44144705   Acc:  0.979415\n",
      "            : Test Loss :0.49848872423171997 Test Accuracy : 0.9747700095176697 \n",
      "****************************************************************************************************\n",
      "Iteration  403 : Loss =  0.44156122   Acc:  0.9792517\n",
      "            : Test Loss :0.49759626388549805 Test Accuracy : 0.9750900268554688 \n",
      "****************************************************************************************************\n",
      "Iteration  404 : Loss =  0.44090402   Acc:  0.97941\n",
      "            : Test Loss :0.4968503415584564 Test Accuracy : 0.9750800132751465 \n",
      "****************************************************************************************************\n",
      "Iteration  405 : Loss =  0.44046128   Acc:  0.9792867\n",
      "            : Test Loss :0.4972674250602722 Test Accuracy : 0.9750000238418579 \n",
      "****************************************************************************************************\n",
      "Iteration  406 : Loss =  0.44009984   Acc:  0.97947836\n",
      "            : Test Loss :0.4955832064151764 Test Accuracy : 0.9754199981689453 \n",
      "****************************************************************************************************\n",
      "Iteration  407 : Loss =  0.43969664   Acc:  0.97941\n",
      "            : Test Loss :0.4956433176994324 Test Accuracy : 0.975130021572113 \n",
      "****************************************************************************************************\n",
      "Iteration  408 : Loss =  0.43833733   Acc:  0.97953665\n",
      "            : Test Loss :0.4935394525527954 Test Accuracy : 0.9752200245857239 \n",
      "****************************************************************************************************\n",
      "Iteration  409 : Loss =  0.43711185   Acc:  0.97968\n",
      "            : Test Loss :0.49374493956565857 Test Accuracy : 0.975409984588623 \n",
      "****************************************************************************************************\n",
      "Iteration  410 : Loss =  0.43676051   Acc:  0.97963333\n",
      "            : Test Loss :0.4945586323738098 Test Accuracy : 0.9751200079917908 \n",
      "****************************************************************************************************\n",
      "Iteration  411 : Loss =  0.43730712   Acc:  0.9797\n",
      "            : Test Loss :0.4947494864463806 Test Accuracy : 0.9753900170326233 \n",
      "****************************************************************************************************\n",
      "Iteration  412 : Loss =  0.43794978   Acc:  0.97954834\n",
      "            : Test Loss :0.4952155351638794 Test Accuracy : 0.9751399755477905 \n",
      "****************************************************************************************************\n",
      "Iteration  413 : Loss =  0.43779725   Acc:  0.97968\n",
      "            : Test Loss :0.4947180151939392 Test Accuracy : 0.9754400253295898 \n",
      "****************************************************************************************************\n",
      "Iteration  414 : Loss =  0.43752617   Acc:  0.97944665\n",
      "            : Test Loss :0.493902325630188 Test Accuracy : 0.9752699732780457 \n",
      "****************************************************************************************************\n",
      "Iteration  415 : Loss =  0.43687212   Acc:  0.97964835\n",
      "            : Test Loss :0.4946085214614868 Test Accuracy : 0.9753000140190125 \n",
      "****************************************************************************************************\n",
      "Iteration  416 : Loss =  0.43685555   Acc:  0.979495\n",
      "            : Test Loss :0.4930683970451355 Test Accuracy : 0.9751499891281128 \n",
      "****************************************************************************************************\n",
      "Iteration  417 : Loss =  0.43630773   Acc:  0.97973335\n",
      "            : Test Loss :0.49354439973831177 Test Accuracy : 0.9756100177764893 \n",
      "****************************************************************************************************\n",
      "Iteration  418 : Loss =  0.43561924   Acc:  0.97968334\n",
      "            : Test Loss :0.49204736948013306 Test Accuracy : 0.9752200245857239 \n",
      "****************************************************************************************************\n",
      "Iteration  419 : Loss =  0.4349643   Acc:  0.979705\n",
      "            : Test Loss :0.49276506900787354 Test Accuracy : 0.9754499793052673 \n",
      "****************************************************************************************************\n",
      "Iteration  420 : Loss =  0.43495905   Acc:  0.97974664\n",
      "            : Test Loss :0.4936675727367401 Test Accuracy : 0.9750999808311462 \n",
      "****************************************************************************************************\n",
      "Iteration  421 : Loss =  0.43610716   Acc:  0.97955334\n",
      "            : Test Loss :0.4961203336715698 Test Accuracy : 0.9751600027084351 \n",
      "****************************************************************************************************\n",
      "Iteration  422 : Loss =  0.43816948   Acc:  0.97939\n",
      "            : Test Loss :0.49942547082901 Test Accuracy : 0.9744399785995483 \n",
      "****************************************************************************************************\n",
      "Iteration  423 : Loss =  0.44187444   Acc:  0.9789817\n",
      "            : Test Loss :0.5041909217834473 Test Accuracy : 0.9742900133132935 \n",
      "****************************************************************************************************\n",
      "Iteration  424 : Loss =  0.44556847   Acc:  0.9786983\n",
      "            : Test Loss :0.5059694647789001 Test Accuracy : 0.9741100072860718 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  425 : Loss =  0.44899243   Acc:  0.9783367\n",
      "            : Test Loss :0.5031728744506836 Test Accuracy : 0.9745299816131592 \n",
      "****************************************************************************************************\n",
      "Iteration  426 : Loss =  0.44439256   Acc:  0.97883\n",
      "            : Test Loss :0.493549644947052 Test Accuracy : 0.9753199815750122 \n",
      "****************************************************************************************************\n",
      "Iteration  427 : Loss =  0.43680257   Acc:  0.979505\n",
      "            : Test Loss :0.49041739106178284 Test Accuracy : 0.9754400253295898 \n",
      "****************************************************************************************************\n",
      "Iteration  428 : Loss =  0.4326269   Acc:  0.979815\n",
      "            : Test Loss :0.49468016624450684 Test Accuracy : 0.9752799868583679 \n",
      "****************************************************************************************************\n",
      "Iteration  429 : Loss =  0.43628198   Acc:  0.97956\n",
      "            : Test Loss :0.49813312292099 Test Accuracy : 0.9747300148010254 \n",
      "****************************************************************************************************\n",
      "Iteration  430 : Loss =  0.44086152   Acc:  0.9790533\n",
      "            : Test Loss :0.4966362714767456 Test Accuracy : 0.9751399755477905 \n",
      "****************************************************************************************************\n",
      "Iteration  431 : Loss =  0.4375565   Acc:  0.97948\n",
      "            : Test Loss :0.4902840852737427 Test Accuracy : 0.9752500057220459 \n",
      "****************************************************************************************************\n",
      "Iteration  432 : Loss =  0.43250352   Acc:  0.97981167\n",
      "            : Test Loss :0.48973768949508667 Test Accuracy : 0.9754700064659119 \n",
      "****************************************************************************************************\n",
      "Iteration  433 : Loss =  0.4315825   Acc:  0.9798617\n",
      "            : Test Loss :0.49365925788879395 Test Accuracy : 0.9752299785614014 \n",
      "****************************************************************************************************\n",
      "Iteration  434 : Loss =  0.43470454   Acc:  0.97973835\n",
      "            : Test Loss :0.4939648509025574 Test Accuracy : 0.9751399755477905 \n",
      "****************************************************************************************************\n",
      "Iteration  435 : Loss =  0.4360667   Acc:  0.9794367\n",
      "            : Test Loss :0.4919435679912567 Test Accuracy : 0.9754899740219116 \n",
      "****************************************************************************************************\n",
      "Iteration  436 : Loss =  0.43293715   Acc:  0.9798483\n",
      "            : Test Loss :0.4888561964035034 Test Accuracy : 0.9756500124931335 \n",
      "****************************************************************************************************\n",
      "Iteration  437 : Loss =  0.43031114   Acc:  0.97998834\n",
      "            : Test Loss :0.4894576668739319 Test Accuracy : 0.9752600193023682 \n",
      "****************************************************************************************************\n",
      "Iteration  438 : Loss =  0.43127865   Acc:  0.97983664\n",
      "            : Test Loss :0.4923667013645172 Test Accuracy : 0.9753299951553345 \n",
      "****************************************************************************************************\n",
      "Iteration  439 : Loss =  0.4330457   Acc:  0.97973335\n",
      "            : Test Loss :0.49117666482925415 Test Accuracy : 0.9752900004386902 \n",
      "****************************************************************************************************\n",
      "Iteration  440 : Loss =  0.4329457   Acc:  0.97974336\n",
      "            : Test Loss :0.48963379859924316 Test Accuracy : 0.9755499958992004 \n",
      "****************************************************************************************************\n",
      "Iteration  441 : Loss =  0.43058097   Acc:  0.97989166\n",
      "            : Test Loss :0.48865121603012085 Test Accuracy : 0.9754899740219116 \n",
      "****************************************************************************************************\n",
      "Iteration  442 : Loss =  0.4297832   Acc:  0.980005\n",
      "            : Test Loss :0.4897997975349426 Test Accuracy : 0.9753400087356567 \n",
      "****************************************************************************************************\n",
      "Iteration  443 : Loss =  0.43125224   Acc:  0.979705\n",
      "            : Test Loss :0.4920046329498291 Test Accuracy : 0.9753299951553345 \n",
      "****************************************************************************************************\n",
      "Iteration  444 : Loss =  0.43254524   Acc:  0.97985667\n",
      "            : Test Loss :0.49206236004829407 Test Accuracy : 0.9751200079917908 \n",
      "****************************************************************************************************\n",
      "Iteration  445 : Loss =  0.43300444   Acc:  0.97955835\n",
      "            : Test Loss :0.491385817527771 Test Accuracy : 0.9753199815750122 \n",
      "****************************************************************************************************\n",
      "Iteration  446 : Loss =  0.4323995   Acc:  0.97986335\n",
      "            : Test Loss :0.49450600147247314 Test Accuracy : 0.9752200245857239 \n",
      "****************************************************************************************************\n",
      "Iteration  447 : Loss =  0.43477708   Acc:  0.979355\n",
      "            : Test Loss :0.4939826726913452 Test Accuracy : 0.9747599959373474 \n",
      "****************************************************************************************************\n",
      "Iteration  448 : Loss =  0.43574932   Acc:  0.979485\n",
      "            : Test Loss :0.4963313937187195 Test Accuracy : 0.9750800132751465 \n",
      "****************************************************************************************************\n",
      "Iteration  449 : Loss =  0.4365508   Acc:  0.9792733\n",
      "            : Test Loss :0.4914979040622711 Test Accuracy : 0.9750000238418579 \n",
      "****************************************************************************************************\n",
      "Iteration  450 : Loss =  0.43299952   Acc:  0.9796733\n",
      "            : Test Loss :0.4881436824798584 Test Accuracy : 0.9757999777793884 \n",
      "****************************************************************************************************\n",
      "Iteration  451 : Loss =  0.4293642   Acc:  0.979895\n",
      "            : Test Loss :0.4874765872955322 Test Accuracy : 0.9754300117492676 \n",
      "****************************************************************************************************\n",
      "Iteration  452 : Loss =  0.42794728   Acc:  0.98010665\n",
      "            : Test Loss :0.4873396158218384 Test Accuracy : 0.9752799868583679 \n",
      "****************************************************************************************************\n",
      "Iteration  453 : Loss =  0.428716   Acc:  0.97998166\n",
      "            : Test Loss :0.48953020572662354 Test Accuracy : 0.9756199717521667 \n",
      "****************************************************************************************************\n",
      "Iteration  454 : Loss =  0.42940533   Acc:  0.97985166\n",
      "            : Test Loss :0.4892653822898865 Test Accuracy : 0.9753400087356567 \n",
      "****************************************************************************************************\n",
      "Iteration  455 : Loss =  0.42970008   Acc:  0.97995335\n",
      "            : Test Loss :0.4902847409248352 Test Accuracy : 0.9755600094795227 \n",
      "****************************************************************************************************\n",
      "Iteration  456 : Loss =  0.4306519   Acc:  0.97964835\n",
      "            : Test Loss :0.49085718393325806 Test Accuracy : 0.9752900004386902 \n",
      "****************************************************************************************************\n",
      "Iteration  457 : Loss =  0.43060327   Acc:  0.97993165\n",
      "            : Test Loss :0.48995500802993774 Test Accuracy : 0.9753100275993347 \n",
      "****************************************************************************************************\n",
      "Iteration  458 : Loss =  0.4304994   Acc:  0.9796517\n",
      "            : Test Loss :0.48830193281173706 Test Accuracy : 0.9755899906158447 \n",
      "****************************************************************************************************\n",
      "Iteration  459 : Loss =  0.42849547   Acc:  0.9800583\n",
      "            : Test Loss :0.48701465129852295 Test Accuracy : 0.9754499793052673 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  460 : Loss =  0.4271108   Acc:  0.980005\n",
      "            : Test Loss :0.48537737131118774 Test Accuracy : 0.9757599830627441 \n",
      "****************************************************************************************************\n",
      "Iteration  461 : Loss =  0.42634284   Acc:  0.98016334\n",
      "            : Test Loss :0.4860846996307373 Test Accuracy : 0.9755799770355225 \n",
      "****************************************************************************************************\n",
      "Iteration  462 : Loss =  0.42586747   Acc:  0.98023164\n",
      "            : Test Loss :0.4851672649383545 Test Accuracy : 0.9757699966430664 \n",
      "****************************************************************************************************\n",
      "Iteration  463 : Loss =  0.42567533   Acc:  0.98016834\n",
      "            : Test Loss :0.4858550727367401 Test Accuracy : 0.9757699966430664 \n",
      "****************************************************************************************************\n",
      "Iteration  464 : Loss =  0.425923   Acc:  0.9802\n",
      "            : Test Loss :0.48717212677001953 Test Accuracy : 0.9754899740219116 \n",
      "****************************************************************************************************\n",
      "Iteration  465 : Loss =  0.42677933   Acc:  0.979935\n",
      "            : Test Loss :0.4863417148590088 Test Accuracy : 0.9757000207901001 \n",
      "****************************************************************************************************\n",
      "Iteration  466 : Loss =  0.4268543   Acc:  0.98015165\n",
      "            : Test Loss :0.48711884021759033 Test Accuracy : 0.9755799770355225 \n",
      "****************************************************************************************************\n",
      "Iteration  467 : Loss =  0.42648116   Acc:  0.98004335\n",
      "            : Test Loss :0.4849107265472412 Test Accuracy : 0.9756600260734558 \n",
      "****************************************************************************************************\n",
      "Iteration  468 : Loss =  0.42519063   Acc:  0.980235\n",
      "            : Test Loss :0.4847031235694885 Test Accuracy : 0.9757400155067444 \n",
      "****************************************************************************************************\n",
      "Iteration  469 : Loss =  0.424492   Acc:  0.9801817\n",
      "            : Test Loss :0.48462867736816406 Test Accuracy : 0.9755600094795227 \n",
      "****************************************************************************************************\n",
      "Iteration  470 : Loss =  0.42423597   Acc:  0.98030335\n",
      "            : Test Loss :0.48405537009239197 Test Accuracy : 0.9759200215339661 \n",
      "****************************************************************************************************\n",
      "Iteration  471 : Loss =  0.4240602   Acc:  0.98027164\n",
      "            : Test Loss :0.4842604994773865 Test Accuracy : 0.975629985332489 \n",
      "****************************************************************************************************\n",
      "Iteration  472 : Loss =  0.42353073   Acc:  0.9803\n",
      "            : Test Loss :0.48324698209762573 Test Accuracy : 0.9758700132369995 \n",
      "****************************************************************************************************\n",
      "Iteration  473 : Loss =  0.42292202   Acc:  0.98036\n",
      "            : Test Loss :0.4831783175468445 Test Accuracy : 0.9758399724960327 \n",
      "****************************************************************************************************\n",
      "Iteration  474 : Loss =  0.42271185   Acc:  0.98032\n",
      "            : Test Loss :0.48365187644958496 Test Accuracy : 0.9758300185203552 \n",
      "****************************************************************************************************\n",
      "Iteration  475 : Loss =  0.42293018   Acc:  0.98037165\n",
      "            : Test Loss :0.4836132824420929 Test Accuracy : 0.9758300185203552 \n",
      "****************************************************************************************************\n",
      "Iteration  476 : Loss =  0.42326254   Acc:  0.9802383\n",
      "            : Test Loss :0.4841589629650116 Test Accuracy : 0.9756399989128113 \n",
      "****************************************************************************************************\n",
      "Iteration  477 : Loss =  0.42333296   Acc:  0.98039\n",
      "            : Test Loss :0.4841485023498535 Test Accuracy : 0.9757999777793884 \n",
      "****************************************************************************************************\n",
      "Iteration  478 : Loss =  0.42339423   Acc:  0.9802117\n",
      "            : Test Loss :0.4840664863586426 Test Accuracy : 0.9756799936294556 \n",
      "****************************************************************************************************\n",
      "Iteration  479 : Loss =  0.42345065   Acc:  0.980275\n",
      "            : Test Loss :0.4854440689086914 Test Accuracy : 0.9757199883460999 \n",
      "****************************************************************************************************\n",
      "Iteration  480 : Loss =  0.4241343   Acc:  0.9801\n",
      "            : Test Loss :0.48477697372436523 Test Accuracy : 0.9754899740219116 \n",
      "****************************************************************************************************\n",
      "Iteration  481 : Loss =  0.42438895   Acc:  0.98019\n",
      "            : Test Loss :0.4867250621318817 Test Accuracy : 0.9757099747657776 \n",
      "****************************************************************************************************\n",
      "Iteration  482 : Loss =  0.42524832   Acc:  0.97999\n",
      "            : Test Loss :0.4850054979324341 Test Accuracy : 0.9754800200462341 \n",
      "****************************************************************************************************\n",
      "Iteration  483 : Loss =  0.42453495   Acc:  0.980245\n",
      "            : Test Loss :0.4857461452484131 Test Accuracy : 0.9757199883460999 \n",
      "****************************************************************************************************\n",
      "Iteration  484 : Loss =  0.42457113   Acc:  0.98002833\n",
      "            : Test Loss :0.48418235778808594 Test Accuracy : 0.9757900238037109 \n",
      "****************************************************************************************************\n",
      "Iteration  485 : Loss =  0.4232005   Acc:  0.9803117\n",
      "            : Test Loss :0.4833188056945801 Test Accuracy : 0.9756699800491333 \n",
      "****************************************************************************************************\n",
      "Iteration  486 : Loss =  0.4225356   Acc:  0.98025334\n",
      "            : Test Loss :0.48314639925956726 Test Accuracy : 0.975820004940033 \n",
      "****************************************************************************************************\n",
      "Iteration  487 : Loss =  0.42162198   Acc:  0.98046\n",
      "            : Test Loss :0.4817785322666168 Test Accuracy : 0.975820004940033 \n",
      "****************************************************************************************************\n",
      "Iteration  488 : Loss =  0.42094386   Acc:  0.98040664\n",
      "            : Test Loss :0.48218464851379395 Test Accuracy : 0.9761099815368652 \n",
      "****************************************************************************************************\n",
      "Iteration  489 : Loss =  0.42045367   Acc:  0.9804867\n",
      "            : Test Loss :0.4815950393676758 Test Accuracy : 0.9756799936294556 \n",
      "****************************************************************************************************\n",
      "Iteration  490 : Loss =  0.4203606   Acc:  0.980435\n",
      "            : Test Loss :0.4822486937046051 Test Accuracy : 0.9758800268173218 \n",
      "****************************************************************************************************\n",
      "Iteration  491 : Loss =  0.42061517   Acc:  0.9804017\n",
      "            : Test Loss :0.48273202776908875 Test Accuracy : 0.9756500124931335 \n",
      "****************************************************************************************************\n",
      "Iteration  492 : Loss =  0.42118034   Acc:  0.98046833\n",
      "            : Test Loss :0.4835277795791626 Test Accuracy : 0.9758700132369995 \n",
      "****************************************************************************************************\n",
      "Iteration  493 : Loss =  0.42187542   Acc:  0.98026\n",
      "            : Test Loss :0.48400306701660156 Test Accuracy : 0.9755600094795227 \n",
      "****************************************************************************************************\n",
      "Iteration  494 : Loss =  0.42243996   Acc:  0.98033667\n",
      "            : Test Loss :0.48532572388648987 Test Accuracy : 0.9757900238037109 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  495 : Loss =  0.42341614   Acc:  0.98004\n",
      "            : Test Loss :0.484564870595932 Test Accuracy : 0.9754400253295898 \n",
      "****************************************************************************************************\n",
      "Iteration  496 : Loss =  0.4232897   Acc:  0.9801933\n",
      "            : Test Loss :0.48613792657852173 Test Accuracy : 0.9757599830627441 \n",
      "****************************************************************************************************\n",
      "Iteration  497 : Loss =  0.42394847   Acc:  0.979935\n",
      "            : Test Loss :0.4832655191421509 Test Accuracy : 0.975570023059845 \n",
      "****************************************************************************************************\n",
      "Iteration  498 : Loss =  0.4221112   Acc:  0.98037165\n",
      "            : Test Loss :0.4831889271736145 Test Accuracy : 0.9757800102233887 \n",
      "****************************************************************************************************\n",
      "Iteration  499 : Loss =  0.42101392   Acc:  0.98028666\n",
      "            : Test Loss :0.4810278117656708 Test Accuracy : 0.9758999943733215 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "    # load the training and test data    \n",
    "    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "    # reshape the feature data\n",
    "    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "    te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "    # noramlise feature data\n",
    "    tr_x = tr_x / 255.0\n",
    "    te_x = te_x / 255.0\n",
    "\n",
    "    # one hot encode the training labels and get the transpose\n",
    "    tr_y = np_utils.to_categorical(tr_y,10)\n",
    "    tr_y = tr_y.T\n",
    "\n",
    "    # one hot encode the test labels and get the transpose\n",
    "    te_y = np_utils.to_categorical(te_y,10)\n",
    "    te_y = te_y.T\n",
    "\n",
    "    return tr_x, tr_y, te_x, te_y\n",
    "     \n",
    "    \n",
    "\n",
    "\n",
    "def softmax(y_pred):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return tf.exp(y_pred) / tf.reduce_sum(tf.exp(y_pred), axis=0) \n",
    "\n",
    "\n",
    "\n",
    "def forward_pass(x, w3_T, w2_T, w1_T, b):\n",
    "    \n",
    "#     w3_T = tf.transpose(w3)\n",
    "#     w2_T = tf.transpose(w2)\n",
    "#     w1_T = tf.transpose(w1)\n",
    "    \n",
    "    \n",
    "    \"\"\"Layer 1 - With 300 ReLu Neurons\"\"\"\n",
    "    # We need to mutliply the flattened INPUT by the weights of this layer and add bias\n",
    "    y_pred_layer1 = tf.matmul(w1_T, x) + b\n",
    "    y_pred_relu_layer1 = tf.maximum(y_pred_layer1, 0)\n",
    "    \n",
    "    \n",
    "    \"\"\"Layer 2 - With 100 ReLu Neurons\"\"\"\n",
    "    # We need to mutliply the output of the previous layer by the weights of this layer and add bias\n",
    "    y_pred_layer2 = tf.matmul(w2_T, y_pred_relu_layer1) + b\n",
    "    y_pred_relu_layer2 = tf.maximum(y_pred_layer2, 0)\n",
    "    \n",
    "    \n",
    "    \"\"\"Layer 3 - Softmax Layer with 10 neurons\"\"\"\n",
    "    # We need to mutliply the output of the previous layer by the weights of this layer and add bias\n",
    "    y_pred_layer3 = tf.matmul(w3_T, y_pred_relu_layer2) + b\n",
    "    \n",
    "    '''Pipe the results through the softmax activiation function. '''\n",
    "    y_pred_softmax = softmax(y_pred_layer3)\n",
    "    \n",
    "\n",
    "    return y_pred_softmax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def cross_entropy_wth_L2(y, y_pred, w3, w2, w1, regularization_rate):\n",
    "    \n",
    "    regularization_factor = regularization_rate / 2\n",
    "    \n",
    "    loss = - tf.reduce_sum( y * tf.math.log(y_pred), axis=0 )\n",
    "     \n",
    "    regularlization = regularization_factor * (  tf.reduce_sum(tf.square(w1))+  tf.reduce_sum(tf.square(w2))\n",
    "                                               + tf.reduce_sum(tf.square(w3))  )\n",
    "    \n",
    "    out = tf.reduce_mean(loss) + regularlization\n",
    "                                             \n",
    "    return out\n",
    "\n",
    "def cross_entropy_wth_L1(y, y_pred, w3, w2, w1, regularization_rate):\n",
    "    \n",
    "    loss = - tf.reduce_sum( y * tf.math.log(y_pred), axis=0 )\n",
    "    \n",
    "    regularization_factor = regularization_rate / 2    \n",
    "    \n",
    "    regularlization = regularization_factor * (  tf.reduce_sum(tf.abs(w1))+  tf.reduce_sum(tf.abs(w2))\n",
    "                                               + tf.reduce_sum(tf.abs(w3))  )\n",
    "    \n",
    "    out = tf.reduce_mean(loss) + regularlization\n",
    "                                             \n",
    "    return out\n",
    "\n",
    "def cross_entropy(y, y_pred, w3, w2, w1, regularization_rate):\n",
    "\n",
    "    y_pred_ = tf.clip_by_value(y_pred, 1e-10, 1.0)\n",
    "    \n",
    "    return cross_entropy_wth_L1(y, y_pred, w3, w2, w1, regularization_rate)\n",
    "    \n",
    "\n",
    "def calculate_accuracy(y_pred_sigmoid, y):\n",
    "  \n",
    "    \n",
    "    \n",
    "    # Round the predictions by the logistical unit to either 1 or 0\n",
    "    predictions = tf.round(y_pred_sigmoid)\n",
    "\n",
    "    # tf.equal will return a boolean array: True if prediction correct, False otherwise\n",
    "    # tf.cast converts the resulting boolean array to a numerical array \n",
    "    # 1 if True (correct prediction), 0 if False (incorrect prediction)\n",
    "    predictions_correct = tf.cast(tf.equal(predictions, y), tf.float32)\n",
    "\n",
    "    # Finally, we just determine the mean value of predictions_correct\n",
    "    accuracy = tf.reduce_mean(predictions_correct)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def main():\n",
    "    list_of_test_accuracies=[]\n",
    "    list_of_train_accuracies=[]\n",
    "    list_of_test_loss=[]\n",
    "    list_of_train_loss=[]\n",
    "    \n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    num_Iterations = 500\n",
    "    regularization_rate = 0.002\n",
    "    \n",
    "    \n",
    "    adam_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    tr_x, tr_y, te_x, te_y  = load_data()\n",
    "\n",
    "    tr_x = tf.cast(tr_x, tf.float32)\n",
    "    te_x = tf.cast(te_x, tf.float32)\n",
    "    tr_y = tf.cast(tr_y, tf.float32)\n",
    "    te_y = tf.cast(te_y, tf.float32)\n",
    "    \n",
    "#     print('tr_x shape ==>',tr_x.shape)\n",
    "#     print('Transpose of tr_x ==>',tf.transpose(tr_x).shape)\n",
    "#     print('tr_y shape ==>',tr_y.shape)\n",
    "#     print('te_x shape ==>',te_x.shape)\n",
    "#     print('te_y shape ==>',te_y.shape)\n",
    "\n",
    "    # We need a coefficient for each of the features and a single bias value\n",
    "    w1 = tf.Variable(tf.random.normal([ 300,784], mean=0.0, stddev=0.05))\n",
    "    w2 = tf.Variable(tf.random.normal([ 100,300], mean=0.0, stddev=0.05))\n",
    "    w3 = tf.Variable(tf.random.normal([ 10,100], mean=0.0, stddev=0.05))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('w1 shape ==>',w1.shape)\n",
    "    print('w2 shape ==>',w2.shape)\n",
    "    print('w3 shape ==>',w3.shape)\n",
    "    \n",
    "    b = tf.Variable([0.])\n",
    "    \n",
    "    # Iterate our training loop\n",
    "    for i in range(num_Iterations):\n",
    "      \n",
    "        # Create an instance of GradientTape to monitor the forward pass\n",
    "        # and calcualte the gradients for each of the variables m and c\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = forward_pass(tf.transpose(tr_x), w3,w2,w1, b)\n",
    "\n",
    "\n",
    "            #         print('y_pred shape ==>',y_pred.shape)\n",
    "            #         print('tr_y shape ==>',tr_y.shape)\n",
    "\n",
    "            currentLoss = cross_entropy(tr_y, y_pred, w3, w2, w1, regularization_rate)\n",
    "            list_of_train_loss.append(currentLoss)\n",
    "        gradients = tape.gradient(currentLoss, [w1, w2, w3, b])\n",
    "        accuracy = calculate_accuracy(y_pred, tr_y)\n",
    "        list_of_train_accuracies.append(accuracy)\n",
    "    \n",
    "        print (\"Iteration \", i, \": Loss = \",currentLoss.numpy(), \"  Acc: \", accuracy.numpy())\n",
    "\n",
    "        adam_optimizer.apply_gradients(zip(gradients, [w1,w2,w3,b]))\n",
    "        \n",
    "        \"\"\"Lets see how the model performs with the newly upadted w and b on the test data\"\"\"\n",
    "        te_y_pred = forward_pass(tf.transpose(te_x), w3, w2, w1, b)\n",
    "        current_Test_Loss = cross_entropy(te_y, te_y_pred,  w3, w2, w1, regularization_rate)\n",
    "        list_of_test_loss.append(current_Test_Loss.numpy())\n",
    "\n",
    "        test_accuracy = calculate_accuracy(te_y_pred, te_y) \n",
    "        list_of_test_accuracies.append(test_accuracy.numpy())\n",
    "\n",
    "        print (\"            : Test Loss :{0} Test Accuracy : {1} \" .format(current_Test_Loss,test_accuracy) )\n",
    "        \n",
    "        print(\"*\"*100)\n",
    "    \n",
    "    \n",
    "    \"\"\"Plotting the performance\"\"\"\n",
    "    configuration = '300 ReLu 100 Relu 10 softmax neurons with L2 ' +' Reg Rate:' +str(regularization_rate)\n",
    "\n",
    "    plt.title(\"Training loss vs Val Loss \\n\"+configuration+' Epochs:'+str (num_Iterations)) \n",
    "    plt.plot(list_of_test_loss, label=\"Val Loss\")\n",
    "    plt.plot(list_of_train_loss, label=\"Train Loss\")\n",
    "    plt.plot(list_of_train_accuracies, label=\"Train Acc\")\n",
    "    plt.plot(list_of_test_accuracies, label=\"Val Acc\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    \n",
    "main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = tf.constant([[5 ,7], [3,5]])\n",
    "w = tf.constant([ [2,3],[8,5]  ])\n",
    "\n",
    "print(tf.matmul(tf.transpose(w),x))\n",
    "# tf.abs(x)  # [5.25594902, 6.60492229]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[5 ,7], [3,5]])\n",
    "w = tf.constant([ [2,3],[8,5]  ])\n",
    "\n",
    "print(tf.matmul(w,tf.transpose(x)))\n",
    "# tf.abs(x)  # [5.25594902, 6.60492229]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[5 ,7], [3,5]])\n",
    "w = tf.constant([ [2,3],[8,5]  ])\n",
    "\n",
    "print(tf.matmul(x,tf.transpose(w)))\n",
    "# tf.abs(x)  # [5.25594902, 6.60492229]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[5 ,-7], [-3,5]])\n",
    "w = tf.constant([ [2,3],[8,5]  ])\n",
    "\n",
    "print(tf.maximum(x,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
