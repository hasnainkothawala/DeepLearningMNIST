{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 shape ==> (300, 784)\n",
      "w2 shape ==> (10, 300)\n",
      "Iteration  0 : Loss =  2.3478048   Acc:  0.9\n",
      "            : Test Loss :2.0400261878967285 Test Accuracy : 0.8999999761581421 \n",
      "****************************************************************************************************\n",
      "Iteration  1 : Loss =  2.0397692   Acc:  0.9\n",
      "            : Test Loss :1.8280320167541504 Test Accuracy : 0.8999999761581421 \n",
      "****************************************************************************************************\n",
      "Iteration  2 : Loss =  1.8254197   Acc:  0.9000083\n",
      "            : Test Loss :1.6480752229690552 Test Accuracy : 0.9020199775695801 \n",
      "****************************************************************************************************\n",
      "Iteration  3 : Loss =  1.6430532   Acc:  0.90183836\n",
      "            : Test Loss :1.4925801753997803 Test Accuracy : 0.9068300127983093 \n",
      "****************************************************************************************************\n",
      "Iteration  4 : Loss =  1.4857314   Acc:  0.907155\n",
      "            : Test Loss :1.3640527725219727 Test Accuracy : 0.9142299890518188 \n",
      "****************************************************************************************************\n",
      "Iteration  5 : Loss =  1.3557317   Acc:  0.9144983\n",
      "            : Test Loss :1.2546420097351074 Test Accuracy : 0.920989990234375 \n",
      "****************************************************************************************************\n",
      "Iteration  6 : Loss =  1.2453132   Acc:  0.9215083\n",
      "            : Test Loss :1.158212423324585 Test Accuracy : 0.9282400012016296 \n",
      "****************************************************************************************************\n",
      "Iteration  7 : Loss =  1.1479995   Acc:  0.92832667\n",
      "            : Test Loss :1.0780762434005737 Test Accuracy : 0.9326099753379822 \n",
      "****************************************************************************************************\n",
      "Iteration  8 : Loss =  1.0669818   Acc:  0.93328\n",
      "            : Test Loss :1.0138509273529053 Test Accuracy : 0.9361199736595154 \n",
      "****************************************************************************************************\n",
      "Iteration  9 : Loss =  1.0018762   Acc:  0.936735\n",
      "            : Test Loss :0.9607650637626648 Test Accuracy : 0.9386699795722961 \n",
      "****************************************************************************************************\n",
      "Iteration  10 : Loss =  0.94778043   Acc:  0.9389517\n",
      "            : Test Loss :0.915782630443573 Test Accuracy : 0.9401500225067139 \n",
      "****************************************************************************************************\n",
      "Iteration  11 : Loss =  0.901659   Acc:  0.9404783\n",
      "            : Test Loss :0.8777761459350586 Test Accuracy : 0.9413999915122986 \n",
      "****************************************************************************************************\n",
      "Iteration  12 : Loss =  0.8625748   Acc:  0.9419683\n",
      "            : Test Loss :0.8455602526664734 Test Accuracy : 0.9426299929618835 \n",
      "****************************************************************************************************\n",
      "Iteration  13 : Loss =  0.8295561   Acc:  0.943585\n",
      "            : Test Loss :0.8186351656913757 Test Accuracy : 0.9439799785614014 \n",
      "****************************************************************************************************\n",
      "Iteration  14 : Loss =  0.80204093   Acc:  0.94513166\n",
      "            : Test Loss :0.7953135371208191 Test Accuracy : 0.9451500177383423 \n",
      "****************************************************************************************************\n",
      "Iteration  15 : Loss =  0.7782146   Acc:  0.94645834\n",
      "            : Test Loss :0.7743465900421143 Test Accuracy : 0.9464600086212158 \n",
      "****************************************************************************************************\n",
      "Iteration  16 : Loss =  0.75678515   Acc:  0.9477233\n",
      "            : Test Loss :0.7564612030982971 Test Accuracy : 0.9476100206375122 \n",
      "****************************************************************************************************\n",
      "Iteration  17 : Loss =  0.738516   Acc:  0.9491133\n",
      "            : Test Loss :0.740612268447876 Test Accuracy : 0.9486500024795532 \n",
      "****************************************************************************************************\n",
      "Iteration  18 : Loss =  0.72247523   Acc:  0.95037335\n",
      "            : Test Loss :0.7260206937789917 Test Accuracy : 0.949999988079071 \n",
      "****************************************************************************************************\n",
      "Iteration  19 : Loss =  0.70768434   Acc:  0.9515667\n",
      "            : Test Loss :0.7127420902252197 Test Accuracy : 0.9508799910545349 \n",
      "****************************************************************************************************\n",
      "Iteration  20 : Loss =  0.69391567   Acc:  0.952455\n",
      "            : Test Loss :0.7004336714744568 Test Accuracy : 0.9513300061225891 \n",
      "****************************************************************************************************\n",
      "Iteration  21 : Loss =  0.6807967   Acc:  0.9530567\n",
      "            : Test Loss :0.6897400617599487 Test Accuracy : 0.9518300294876099 \n",
      "****************************************************************************************************\n",
      "Iteration  22 : Loss =  0.66920286   Acc:  0.95370835\n",
      "            : Test Loss :0.6793307662010193 Test Accuracy : 0.9527999758720398 \n",
      "****************************************************************************************************\n",
      "Iteration  23 : Loss =  0.65800524   Acc:  0.954395\n",
      "            : Test Loss :0.6695629954338074 Test Accuracy : 0.953819990158081 \n",
      "****************************************************************************************************\n",
      "Iteration  24 : Loss =  0.64756286   Acc:  0.9553317\n",
      "            : Test Loss :0.6599603295326233 Test Accuracy : 0.954479992389679 \n",
      "****************************************************************************************************\n",
      "Iteration  25 : Loss =  0.6373815   Acc:  0.9564\n",
      "            : Test Loss :0.6510635018348694 Test Accuracy : 0.9555299878120422 \n",
      "****************************************************************************************************\n",
      "Iteration  26 : Loss =  0.62801623   Acc:  0.95728\n",
      "            : Test Loss :0.6424647569656372 Test Accuracy : 0.9561799764633179 \n",
      "****************************************************************************************************\n",
      "Iteration  27 : Loss =  0.6190827   Acc:  0.9581\n",
      "            : Test Loss :0.6341824531555176 Test Accuracy : 0.9568099975585938 \n",
      "****************************************************************************************************\n",
      "Iteration  28 : Loss =  0.6105987   Acc:  0.95874834\n",
      "            : Test Loss :0.6264123320579529 Test Accuracy : 0.9573500156402588 \n",
      "****************************************************************************************************\n",
      "Iteration  29 : Loss =  0.6025686   Acc:  0.9593\n",
      "            : Test Loss :0.6192997097969055 Test Accuracy : 0.9577100276947021 \n",
      "****************************************************************************************************\n",
      "Iteration  30 : Loss =  0.59500885   Acc:  0.9597\n",
      "            : Test Loss :0.6126257181167603 Test Accuracy : 0.9582800269126892 \n",
      "****************************************************************************************************\n",
      "Iteration  31 : Loss =  0.58785635   Acc:  0.9602417\n",
      "            : Test Loss :0.6059590578079224 Test Accuracy : 0.9588299989700317 \n",
      "****************************************************************************************************\n",
      "Iteration  32 : Loss =  0.5809262   Acc:  0.96074\n",
      "            : Test Loss :0.5996893644332886 Test Accuracy : 0.9594900012016296 \n",
      "****************************************************************************************************\n",
      "Iteration  33 : Loss =  0.574409   Acc:  0.96140665\n",
      "            : Test Loss :0.593904972076416 Test Accuracy : 0.9600099921226501 \n",
      "****************************************************************************************************\n",
      "Iteration  34 : Loss =  0.56823057   Acc:  0.96194834\n",
      "            : Test Loss :0.5883926749229431 Test Accuracy : 0.9604200124740601 \n",
      "****************************************************************************************************\n",
      "Iteration  35 : Loss =  0.56244636   Acc:  0.9624633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            : Test Loss :0.582893431186676 Test Accuracy : 0.9608100056648254 \n",
      "****************************************************************************************************\n",
      "Iteration  36 : Loss =  0.5568812   Acc:  0.9628633\n",
      "            : Test Loss :0.5777478218078613 Test Accuracy : 0.9609500169754028 \n",
      "****************************************************************************************************\n",
      "Iteration  37 : Loss =  0.5515304   Acc:  0.96321166\n",
      "            : Test Loss :0.5729818344116211 Test Accuracy : 0.9612100124359131 \n",
      "****************************************************************************************************\n",
      "Iteration  38 : Loss =  0.5464709   Acc:  0.9634633\n",
      "            : Test Loss :0.5682604908943176 Test Accuracy : 0.9612399935722351 \n",
      "****************************************************************************************************\n",
      "Iteration  39 : Loss =  0.5416501   Acc:  0.96375\n",
      "            : Test Loss :0.5637748837471008 Test Accuracy : 0.9617400169372559 \n",
      "****************************************************************************************************\n",
      "Iteration  40 : Loss =  0.53706187   Acc:  0.9640717\n",
      "            : Test Loss :0.5595608353614807 Test Accuracy : 0.9618600010871887 \n",
      "****************************************************************************************************\n",
      "Iteration  41 : Loss =  0.5326145   Acc:  0.96439165\n",
      "            : Test Loss :0.5554807782173157 Test Accuracy : 0.9621000289916992 \n",
      "****************************************************************************************************\n",
      "Iteration  42 : Loss =  0.5284001   Acc:  0.96471834\n",
      "            : Test Loss :0.5515225529670715 Test Accuracy : 0.9625599980354309 \n",
      "****************************************************************************************************\n",
      "Iteration  43 : Loss =  0.5243609   Acc:  0.965025\n",
      "            : Test Loss :0.5478951334953308 Test Accuracy : 0.9629899859428406 \n",
      "****************************************************************************************************\n",
      "Iteration  44 : Loss =  0.5205026   Acc:  0.9652733\n",
      "            : Test Loss :0.5444086790084839 Test Accuracy : 0.9631800055503845 \n",
      "****************************************************************************************************\n",
      "Iteration  45 : Loss =  0.51669943   Acc:  0.96548665\n",
      "            : Test Loss :0.5411194562911987 Test Accuracy : 0.9634299874305725 \n",
      "****************************************************************************************************\n",
      "Iteration  46 : Loss =  0.51307386   Acc:  0.9657033\n",
      "            : Test Loss :0.5379860401153564 Test Accuracy : 0.9636099934577942 \n",
      "****************************************************************************************************\n",
      "Iteration  47 : Loss =  0.50959456   Acc:  0.96590835\n",
      "            : Test Loss :0.5349183082580566 Test Accuracy : 0.9636800289154053 \n",
      "****************************************************************************************************\n",
      "Iteration  48 : Loss =  0.5062392   Acc:  0.96610165\n",
      "            : Test Loss :0.5320059657096863 Test Accuracy : 0.963949978351593 \n",
      "****************************************************************************************************\n",
      "Iteration  49 : Loss =  0.5030123   Acc:  0.96633\n",
      "            : Test Loss :0.5291892886161804 Test Accuracy : 0.9640600085258484 \n",
      "****************************************************************************************************\n",
      "Iteration  50 : Loss =  0.49987754   Acc:  0.96653664\n",
      "            : Test Loss :0.5264133810997009 Test Accuracy : 0.9642599821090698 \n",
      "****************************************************************************************************\n",
      "Iteration  51 : Loss =  0.49685588   Acc:  0.966765\n",
      "            : Test Loss :0.523780882358551 Test Accuracy : 0.9645500183105469 \n",
      "****************************************************************************************************\n",
      "Iteration  52 : Loss =  0.49388757   Acc:  0.9669783\n",
      "            : Test Loss :0.5212252736091614 Test Accuracy : 0.9646700024604797 \n",
      "****************************************************************************************************\n",
      "Iteration  53 : Loss =  0.49099612   Acc:  0.9672033\n",
      "            : Test Loss :0.5186590552330017 Test Accuracy : 0.9648000001907349 \n",
      "****************************************************************************************************\n",
      "Iteration  54 : Loss =  0.48818037   Acc:  0.96735334\n",
      "            : Test Loss :0.5162861347198486 Test Accuracy : 0.9650300145149231 \n",
      "****************************************************************************************************\n",
      "Iteration  55 : Loss =  0.48543477   Acc:  0.9675183\n",
      "            : Test Loss :0.5139539241790771 Test Accuracy : 0.9650499820709229 \n",
      "****************************************************************************************************\n",
      "Iteration  56 : Loss =  0.48276457   Acc:  0.9676333\n",
      "            : Test Loss :0.5115980505943298 Test Accuracy : 0.9652299880981445 \n",
      "****************************************************************************************************\n",
      "Iteration  57 : Loss =  0.48013982   Acc:  0.967795\n",
      "            : Test Loss :0.5093645453453064 Test Accuracy : 0.9653599858283997 \n",
      "****************************************************************************************************\n",
      "Iteration  58 : Loss =  0.47758734   Acc:  0.967995\n",
      "            : Test Loss :0.5071470737457275 Test Accuracy : 0.9654800295829773 \n",
      "****************************************************************************************************\n",
      "Iteration  59 : Loss =  0.4751001   Acc:  0.96816164\n",
      "            : Test Loss :0.5049936771392822 Test Accuracy : 0.9657300114631653 \n",
      "****************************************************************************************************\n",
      "Iteration  60 : Loss =  0.4726534   Acc:  0.968345\n",
      "            : Test Loss :0.5028575658798218 Test Accuracy : 0.9658100008964539 \n",
      "****************************************************************************************************\n",
      "Iteration  61 : Loss =  0.4702615   Acc:  0.96853334\n",
      "            : Test Loss :0.5007624626159668 Test Accuracy : 0.9660099744796753 \n",
      "****************************************************************************************************\n",
      "Iteration  62 : Loss =  0.4679268   Acc:  0.96865\n",
      "            : Test Loss :0.49870914220809937 Test Accuracy : 0.9662399888038635 \n",
      "****************************************************************************************************\n",
      "Iteration  63 : Loss =  0.4656498   Acc:  0.96886665\n",
      "            : Test Loss :0.4966352880001068 Test Accuracy : 0.9663800001144409 \n",
      "****************************************************************************************************\n",
      "Iteration  64 : Loss =  0.46341875   Acc:  0.9690383\n",
      "            : Test Loss :0.494687020778656 Test Accuracy : 0.9665200114250183 \n",
      "****************************************************************************************************\n",
      "Iteration  65 : Loss =  0.46122637   Acc:  0.96918166\n",
      "            : Test Loss :0.4926644563674927 Test Accuracy : 0.9666500091552734 \n",
      "****************************************************************************************************\n",
      "Iteration  66 : Loss =  0.45908374   Acc:  0.96933836\n",
      "            : Test Loss :0.49078336358070374 Test Accuracy : 0.9667900204658508 \n",
      "****************************************************************************************************\n",
      "Iteration  67 : Loss =  0.4569938   Acc:  0.969435\n",
      "            : Test Loss :0.4888593852519989 Test Accuracy : 0.9668499827384949 \n",
      "****************************************************************************************************\n",
      "Iteration  68 : Loss =  0.4549398   Acc:  0.96958\n",
      "            : Test Loss :0.4871308505535126 Test Accuracy : 0.9670299887657166 \n",
      "****************************************************************************************************\n",
      "Iteration  69 : Loss =  0.45292872   Acc:  0.96970665\n",
      "            : Test Loss :0.4852730333805084 Test Accuracy : 0.9670199751853943 \n",
      "****************************************************************************************************\n",
      "Iteration  70 : Loss =  0.45096686   Acc:  0.96987\n",
      "            : Test Loss :0.48370248079299927 Test Accuracy : 0.9674000144004822 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  71 : Loss =  0.44905448   Acc:  0.96996164\n",
      "            : Test Loss :0.4819035530090332 Test Accuracy : 0.9673399925231934 \n",
      "****************************************************************************************************\n",
      "Iteration  72 : Loss =  0.44721916   Acc:  0.970105\n",
      "            : Test Loss :0.48057469725608826 Test Accuracy : 0.967739999294281 \n",
      "****************************************************************************************************\n",
      "Iteration  73 : Loss =  0.4454759   Acc:  0.970175\n",
      "            : Test Loss :0.4788926839828491 Test Accuracy : 0.9676100015640259 \n",
      "****************************************************************************************************\n",
      "Iteration  74 : Loss =  0.4438567   Acc:  0.9702783\n",
      "            : Test Loss :0.4777567386627197 Test Accuracy : 0.9678800106048584 \n",
      "****************************************************************************************************\n",
      "Iteration  75 : Loss =  0.4421742   Acc:  0.97040164\n",
      "            : Test Loss :0.4758421778678894 Test Accuracy : 0.9677900075912476 \n",
      "****************************************************************************************************\n",
      "Iteration  76 : Loss =  0.44043854   Acc:  0.97052336\n",
      "            : Test Loss :0.4742845594882965 Test Accuracy : 0.9681000113487244 \n",
      "****************************************************************************************************\n",
      "Iteration  77 : Loss =  0.43841228   Acc:  0.9706633\n",
      "            : Test Loss :0.472258597612381 Test Accuracy : 0.9681299924850464 \n",
      "****************************************************************************************************\n",
      "Iteration  78 : Loss =  0.4364098   Acc:  0.970785\n",
      "            : Test Loss :0.47072821855545044 Test Accuracy : 0.9682400226593018 \n",
      "****************************************************************************************************\n",
      "Iteration  79 : Loss =  0.43467256   Acc:  0.97093165\n",
      "            : Test Loss :0.46954989433288574 Test Accuracy : 0.9683899879455566 \n",
      "****************************************************************************************************\n",
      "Iteration  80 : Loss =  0.43321374   Acc:  0.9710233\n",
      "            : Test Loss :0.46817177534103394 Test Accuracy : 0.9683300256729126 \n",
      "****************************************************************************************************\n",
      "Iteration  81 : Loss =  0.43180892   Acc:  0.97108835\n",
      "            : Test Loss :0.46694183349609375 Test Accuracy : 0.9684600234031677 \n",
      "****************************************************************************************************\n",
      "Iteration  82 : Loss =  0.43019557   Acc:  0.97109336\n",
      "            : Test Loss :0.4652245044708252 Test Accuracy : 0.9685199856758118 \n",
      "****************************************************************************************************\n",
      "Iteration  83 : Loss =  0.42844465   Acc:  0.9713167\n",
      "            : Test Loss :0.4637983739376068 Test Accuracy : 0.9687100052833557 \n",
      "****************************************************************************************************\n",
      "Iteration  84 : Loss =  0.42674786   Acc:  0.9714183\n",
      "            : Test Loss :0.4625137746334076 Test Accuracy : 0.968779981136322 \n",
      "****************************************************************************************************\n",
      "Iteration  85 : Loss =  0.4252518   Acc:  0.9715033\n",
      "            : Test Loss :0.46127840876579285 Test Accuracy : 0.968779981136322 \n",
      "****************************************************************************************************\n",
      "Iteration  86 : Loss =  0.4238922   Acc:  0.97163\n",
      "            : Test Loss :0.46023744344711304 Test Accuracy : 0.968940019607544 \n",
      "****************************************************************************************************\n",
      "Iteration  87 : Loss =  0.42251137   Acc:  0.97169\n",
      "            : Test Loss :0.45883071422576904 Test Accuracy : 0.9689499735832214 \n",
      "****************************************************************************************************\n",
      "Iteration  88 : Loss =  0.42104706   Acc:  0.97178\n",
      "            : Test Loss :0.4575839340686798 Test Accuracy : 0.9691100120544434 \n",
      "****************************************************************************************************\n",
      "Iteration  89 : Loss =  0.4194757   Acc:  0.97186667\n",
      "            : Test Loss :0.45618730783462524 Test Accuracy : 0.9690799713134766 \n",
      "****************************************************************************************************\n",
      "Iteration  90 : Loss =  0.41793385   Acc:  0.97195834\n",
      "            : Test Loss :0.4549722671508789 Test Accuracy : 0.9690899848937988 \n",
      "****************************************************************************************************\n",
      "Iteration  91 : Loss =  0.41649348   Acc:  0.972085\n",
      "            : Test Loss :0.4538682699203491 Test Accuracy : 0.969290018081665 \n",
      "****************************************************************************************************\n",
      "Iteration  92 : Loss =  0.41515175   Acc:  0.972185\n",
      "            : Test Loss :0.4527209997177124 Test Accuracy : 0.9693999886512756 \n",
      "****************************************************************************************************\n",
      "Iteration  93 : Loss =  0.41386113   Acc:  0.97223336\n",
      "            : Test Loss :0.451691597700119 Test Accuracy : 0.9695799946784973 \n",
      "****************************************************************************************************\n",
      "Iteration  94 : Loss =  0.41255462   Acc:  0.97235334\n",
      "            : Test Loss :0.4504771828651428 Test Accuracy : 0.969539999961853 \n",
      "****************************************************************************************************\n",
      "Iteration  95 : Loss =  0.41121954   Acc:  0.97242\n",
      "            : Test Loss :0.44936269521713257 Test Accuracy : 0.9696400165557861 \n",
      "****************************************************************************************************\n",
      "Iteration  96 : Loss =  0.40983075   Acc:  0.97252333\n",
      "            : Test Loss :0.4481155276298523 Test Accuracy : 0.9696999788284302 \n",
      "****************************************************************************************************\n",
      "Iteration  97 : Loss =  0.40843892   Acc:  0.97268164\n",
      "            : Test Loss :0.4469830095767975 Test Accuracy : 0.9699400067329407 \n",
      "****************************************************************************************************\n",
      "Iteration  98 : Loss =  0.40706766   Acc:  0.97275335\n",
      "            : Test Loss :0.4458464980125427 Test Accuracy : 0.9700000286102295 \n",
      "****************************************************************************************************\n",
      "Iteration  99 : Loss =  0.4057418   Acc:  0.972815\n",
      "            : Test Loss :0.44477540254592896 Test Accuracy : 0.9700400233268738 \n",
      "****************************************************************************************************\n",
      "Iteration  100 : Loss =  0.40446115   Acc:  0.97291666\n",
      "            : Test Loss :0.44375455379486084 Test Accuracy : 0.9702799916267395 \n",
      "****************************************************************************************************\n",
      "Iteration  101 : Loss =  0.40321815   Acc:  0.97298336\n",
      "            : Test Loss :0.4427279233932495 Test Accuracy : 0.9700300097465515 \n",
      "****************************************************************************************************\n",
      "Iteration  102 : Loss =  0.40200463   Acc:  0.97305834\n",
      "            : Test Loss :0.44177481532096863 Test Accuracy : 0.9704599976539612 \n",
      "****************************************************************************************************\n",
      "Iteration  103 : Loss =  0.40081525   Acc:  0.9731633\n",
      "            : Test Loss :0.44079676270484924 Test Accuracy : 0.9702600240707397 \n",
      "****************************************************************************************************\n",
      "Iteration  104 : Loss =  0.39966607   Acc:  0.9732467\n",
      "            : Test Loss :0.43990805745124817 Test Accuracy : 0.970579981803894 \n",
      "****************************************************************************************************\n",
      "Iteration  105 : Loss =  0.39853406   Acc:  0.9732917\n",
      "            : Test Loss :0.43901294469833374 Test Accuracy : 0.9703999757766724 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  106 : Loss =  0.39747038   Acc:  0.97339\n",
      "            : Test Loss :0.43814024329185486 Test Accuracy : 0.9707199931144714 \n",
      "****************************************************************************************************\n",
      "Iteration  107 : Loss =  0.3963684   Acc:  0.97344166\n",
      "            : Test Loss :0.4372495114803314 Test Accuracy : 0.9704800248146057 \n",
      "****************************************************************************************************\n",
      "Iteration  108 : Loss =  0.39530075   Acc:  0.97360164\n",
      "            : Test Loss :0.4362058639526367 Test Accuracy : 0.97079998254776 \n",
      "****************************************************************************************************\n",
      "Iteration  109 : Loss =  0.3940589   Acc:  0.97362334\n",
      "            : Test Loss :0.4351092278957367 Test Accuracy : 0.9706599712371826 \n",
      "****************************************************************************************************\n",
      "Iteration  110 : Loss =  0.39276868   Acc:  0.9737167\n",
      "            : Test Loss :0.43390923738479614 Test Accuracy : 0.9709699749946594 \n",
      "****************************************************************************************************\n",
      "Iteration  111 : Loss =  0.3913886   Acc:  0.97383666\n",
      "            : Test Loss :0.4328133165836334 Test Accuracy : 0.9708799719810486 \n",
      "****************************************************************************************************\n",
      "Iteration  112 : Loss =  0.3900921   Acc:  0.97388667\n",
      "            : Test Loss :0.431838721036911 Test Accuracy : 0.9710100293159485 \n",
      "****************************************************************************************************\n",
      "Iteration  113 : Loss =  0.38892552   Acc:  0.973955\n",
      "            : Test Loss :0.43098315596580505 Test Accuracy : 0.9710299968719482 \n",
      "****************************************************************************************************\n",
      "Iteration  114 : Loss =  0.3878824   Acc:  0.974065\n",
      "            : Test Loss :0.43023568391799927 Test Accuracy : 0.9710299968719482 \n",
      "****************************************************************************************************\n",
      "Iteration  115 : Loss =  0.38691327   Acc:  0.9741283\n",
      "            : Test Loss :0.42942196130752563 Test Accuracy : 0.9712700247764587 \n",
      "****************************************************************************************************\n",
      "Iteration  116 : Loss =  0.38594753   Acc:  0.97419\n",
      "            : Test Loss :0.4286864697933197 Test Accuracy : 0.9712299704551697 \n",
      "****************************************************************************************************\n",
      "Iteration  117 : Loss =  0.38497704   Acc:  0.97427166\n",
      "            : Test Loss :0.42774268984794617 Test Accuracy : 0.9714000225067139 \n",
      "****************************************************************************************************\n",
      "Iteration  118 : Loss =  0.38390508   Acc:  0.9743083\n",
      "            : Test Loss :0.4268774390220642 Test Accuracy : 0.9713600277900696 \n",
      "****************************************************************************************************\n",
      "Iteration  119 : Loss =  0.3828114   Acc:  0.9743867\n",
      "            : Test Loss :0.42581844329833984 Test Accuracy : 0.9715999960899353 \n",
      "****************************************************************************************************\n",
      "Iteration  120 : Loss =  0.38162777   Acc:  0.9744383\n",
      "            : Test Loss :0.4248623549938202 Test Accuracy : 0.9716200232505798 \n",
      "****************************************************************************************************\n",
      "Iteration  121 : Loss =  0.3804746   Acc:  0.97454\n",
      "            : Test Loss :0.4239056706428528 Test Accuracy : 0.9718300104141235 \n",
      "****************************************************************************************************\n",
      "Iteration  122 : Loss =  0.3793578   Acc:  0.97466\n",
      "            : Test Loss :0.4230160713195801 Test Accuracy : 0.971780002117157 \n",
      "****************************************************************************************************\n",
      "Iteration  123 : Loss =  0.37831047   Acc:  0.9746417\n",
      "            : Test Loss :0.4222261607646942 Test Accuracy : 0.9717000126838684 \n",
      "****************************************************************************************************\n",
      "Iteration  124 : Loss =  0.3773224   Acc:  0.97471833\n",
      "            : Test Loss :0.4214060604572296 Test Accuracy : 0.972000002861023 \n",
      "****************************************************************************************************\n",
      "Iteration  125 : Loss =  0.3763742   Acc:  0.9747767\n",
      "            : Test Loss :0.4207232892513275 Test Accuracy : 0.9719200134277344 \n",
      "****************************************************************************************************\n",
      "Iteration  126 : Loss =  0.37545392   Acc:  0.97487\n",
      "            : Test Loss :0.4199129045009613 Test Accuracy : 0.9720600247383118 \n",
      "****************************************************************************************************\n",
      "Iteration  127 : Loss =  0.37454063   Acc:  0.97493\n",
      "            : Test Loss :0.4192950129508972 Test Accuracy : 0.972029983997345 \n",
      "****************************************************************************************************\n",
      "Iteration  128 : Loss =  0.37365624   Acc:  0.97503835\n",
      "            : Test Loss :0.4184788167476654 Test Accuracy : 0.9721900224685669 \n",
      "****************************************************************************************************\n",
      "Iteration  129 : Loss =  0.37274817   Acc:  0.97503334\n",
      "            : Test Loss :0.4178984761238098 Test Accuracy : 0.9721400141716003 \n",
      "****************************************************************************************************\n",
      "Iteration  130 : Loss =  0.37188807   Acc:  0.9751483\n",
      "            : Test Loss :0.4170399308204651 Test Accuracy : 0.9722300171852112 \n",
      "****************************************************************************************************\n",
      "Iteration  131 : Loss =  0.37094635   Acc:  0.97513336\n",
      "            : Test Loss :0.41642001271247864 Test Accuracy : 0.9722099900245667 \n",
      "****************************************************************************************************\n",
      "Iteration  132 : Loss =  0.37004584   Acc:  0.97526\n",
      "            : Test Loss :0.41548067331314087 Test Accuracy : 0.972320020198822 \n",
      "****************************************************************************************************\n",
      "Iteration  133 : Loss =  0.36901602   Acc:  0.97521335\n",
      "            : Test Loss :0.41470834612846375 Test Accuracy : 0.9724799990653992 \n",
      "****************************************************************************************************\n",
      "Iteration  134 : Loss =  0.36801055   Acc:  0.97541165\n",
      "            : Test Loss :0.41378456354141235 Test Accuracy : 0.97257000207901 \n",
      "****************************************************************************************************\n",
      "Iteration  135 : Loss =  0.3669491   Acc:  0.975335\n",
      "            : Test Loss :0.41295990347862244 Test Accuracy : 0.9726300239562988 \n",
      "****************************************************************************************************\n",
      "Iteration  136 : Loss =  0.36593667   Acc:  0.9754867\n",
      "            : Test Loss :0.4121973514556885 Test Accuracy : 0.9725800156593323 \n",
      "****************************************************************************************************\n",
      "Iteration  137 : Loss =  0.36497298   Acc:  0.9754267\n",
      "            : Test Loss :0.41142818331718445 Test Accuracy : 0.9726600050926208 \n",
      "****************************************************************************************************\n",
      "Iteration  138 : Loss =  0.36407074   Acc:  0.9755717\n",
      "            : Test Loss :0.41085800528526306 Test Accuracy : 0.9727399945259094 \n",
      "****************************************************************************************************\n",
      "Iteration  139 : Loss =  0.36322585   Acc:  0.975595\n",
      "            : Test Loss :0.41014087200164795 Test Accuracy : 0.9727699756622314 \n",
      "****************************************************************************************************\n",
      "Iteration  140 : Loss =  0.36243644   Acc:  0.97568834\n",
      "            : Test Loss :0.409748911857605 Test Accuracy : 0.9728000164031982 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  141 : Loss =  0.36170202   Acc:  0.97575164\n",
      "            : Test Loss :0.40905478596687317 Test Accuracy : 0.972760021686554 \n",
      "****************************************************************************************************\n",
      "Iteration  142 : Loss =  0.3610321   Acc:  0.97575\n",
      "            : Test Loss :0.4088873565196991 Test Accuracy : 0.9727699756622314 \n",
      "****************************************************************************************************\n",
      "Iteration  143 : Loss =  0.36043248   Acc:  0.9758933\n",
      "            : Test Loss :0.4081627428531647 Test Accuracy : 0.9728599786758423 \n",
      "****************************************************************************************************\n",
      "Iteration  144 : Loss =  0.35983932   Acc:  0.9758\n",
      "            : Test Loss :0.40798965096473694 Test Accuracy : 0.9728000164031982 \n",
      "****************************************************************************************************\n",
      "Iteration  145 : Loss =  0.359154   Acc:  0.976005\n",
      "            : Test Loss :0.4068566858768463 Test Accuracy : 0.9729200005531311 \n",
      "****************************************************************************************************\n",
      "Iteration  146 : Loss =  0.3582124   Acc:  0.97590333\n",
      "            : Test Loss :0.40609729290008545 Test Accuracy : 0.9729099869728088 \n",
      "****************************************************************************************************\n",
      "Iteration  147 : Loss =  0.3569969   Acc:  0.97610664\n",
      "            : Test Loss :0.4047667384147644 Test Accuracy : 0.9730299711227417 \n",
      "****************************************************************************************************\n",
      "Iteration  148 : Loss =  0.35572827   Acc:  0.97608\n",
      "            : Test Loss :0.403991162776947 Test Accuracy : 0.9731600284576416 \n",
      "****************************************************************************************************\n",
      "Iteration  149 : Loss =  0.35470384   Acc:  0.9761183\n",
      "            : Test Loss :0.40350252389907837 Test Accuracy : 0.9732000231742859 \n",
      "****************************************************************************************************\n",
      "Iteration  150 : Loss =  0.354001   Acc:  0.97619665\n",
      "            : Test Loss :0.4029877781867981 Test Accuracy : 0.9731299877166748 \n",
      "****************************************************************************************************\n",
      "Iteration  151 : Loss =  0.35348174   Acc:  0.976195\n",
      "            : Test Loss :0.40282225608825684 Test Accuracy : 0.9730100035667419 \n",
      "****************************************************************************************************\n",
      "Iteration  152 : Loss =  0.35291088   Acc:  0.9763117\n",
      "            : Test Loss :0.4019491672515869 Test Accuracy : 0.9731500148773193 \n",
      "****************************************************************************************************\n",
      "Iteration  153 : Loss =  0.35215834   Acc:  0.9762717\n",
      "            : Test Loss :0.4013857841491699 Test Accuracy : 0.9730100035667419 \n",
      "****************************************************************************************************\n",
      "Iteration  154 : Loss =  0.35119393   Acc:  0.97642\n",
      "            : Test Loss :0.40034088492393494 Test Accuracy : 0.9732599854469299 \n",
      "****************************************************************************************************\n",
      "Iteration  155 : Loss =  0.35018098   Acc:  0.97638166\n",
      "            : Test Loss :0.399690181016922 Test Accuracy : 0.9732099771499634 \n",
      "****************************************************************************************************\n",
      "Iteration  156 : Loss =  0.34930655   Acc:  0.97651833\n",
      "            : Test Loss :0.3991977870464325 Test Accuracy : 0.9732499718666077 \n",
      "****************************************************************************************************\n",
      "Iteration  157 : Loss =  0.34862354   Acc:  0.97652334\n",
      "            : Test Loss :0.3986651301383972 Test Accuracy : 0.9733399748802185 \n",
      "****************************************************************************************************\n",
      "Iteration  158 : Loss =  0.34806505   Acc:  0.97655\n",
      "            : Test Loss :0.3984232544898987 Test Accuracy : 0.973330020904541 \n",
      "****************************************************************************************************\n",
      "Iteration  159 : Loss =  0.34746596   Acc:  0.9766267\n",
      "            : Test Loss :0.397647887468338 Test Accuracy : 0.9733800292015076 \n",
      "****************************************************************************************************\n",
      "Iteration  160 : Loss =  0.34675142   Acc:  0.976645\n",
      "            : Test Loss :0.397095650434494 Test Accuracy : 0.9733999967575073 \n",
      "****************************************************************************************************\n",
      "Iteration  161 : Loss =  0.3458659   Acc:  0.97674\n",
      "            : Test Loss :0.396274209022522 Test Accuracy : 0.9734500050544739 \n",
      "****************************************************************************************************\n",
      "Iteration  162 : Loss =  0.34498549   Acc:  0.9767633\n",
      "            : Test Loss :0.3956840932369232 Test Accuracy : 0.9733800292015076 \n",
      "****************************************************************************************************\n",
      "Iteration  163 : Loss =  0.34424883   Acc:  0.97682333\n",
      "            : Test Loss :0.3954305648803711 Test Accuracy : 0.9733999967575073 \n",
      "****************************************************************************************************\n",
      "Iteration  164 : Loss =  0.3437051   Acc:  0.97691333\n",
      "            : Test Loss :0.3948926031589508 Test Accuracy : 0.9734500050544739 \n",
      "****************************************************************************************************\n",
      "Iteration  165 : Loss =  0.34320852   Acc:  0.9768633\n",
      "            : Test Loss :0.3948133885860443 Test Accuracy : 0.9733800292015076 \n",
      "****************************************************************************************************\n",
      "Iteration  166 : Loss =  0.34269425   Acc:  0.9770167\n",
      "            : Test Loss :0.39400163292884827 Test Accuracy : 0.9735400080680847 \n",
      "****************************************************************************************************\n",
      "Iteration  167 : Loss =  0.34199572   Acc:  0.97695667\n",
      "            : Test Loss :0.39375096559524536 Test Accuracy : 0.9734200239181519 \n",
      "****************************************************************************************************\n",
      "Iteration  168 : Loss =  0.34133098   Acc:  0.9771\n",
      "            : Test Loss :0.39297783374786377 Test Accuracy : 0.9735299944877625 \n",
      "****************************************************************************************************\n",
      "Iteration  169 : Loss =  0.34057006   Acc:  0.9770783\n",
      "            : Test Loss :0.3926386833190918 Test Accuracy : 0.9736400246620178 \n",
      "****************************************************************************************************\n",
      "Iteration  170 : Loss =  0.33997548   Acc:  0.97713834\n",
      "            : Test Loss :0.39206627011299133 Test Accuracy : 0.9736300110816956 \n",
      "****************************************************************************************************\n",
      "Iteration  171 : Loss =  0.3392342   Acc:  0.9771583\n",
      "            : Test Loss :0.3913072645664215 Test Accuracy : 0.9736999869346619 \n",
      "****************************************************************************************************\n",
      "Iteration  172 : Loss =  0.33835804   Acc:  0.97718835\n",
      "            : Test Loss :0.3905217945575714 Test Accuracy : 0.9737300276756287 \n",
      "****************************************************************************************************\n",
      "Iteration  173 : Loss =  0.3373615   Acc:  0.9772267\n",
      "            : Test Loss :0.3898058235645294 Test Accuracy : 0.973829984664917 \n",
      "****************************************************************************************************\n",
      "Iteration  174 : Loss =  0.3365126   Acc:  0.9773\n",
      "            : Test Loss :0.38938969373703003 Test Accuracy : 0.9738500118255615 \n",
      "****************************************************************************************************\n",
      "Iteration  175 : Loss =  0.3359254   Acc:  0.9773217\n",
      "            : Test Loss :0.3891310691833496 Test Accuracy : 0.9738500118255615 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  176 : Loss =  0.33547714   Acc:  0.977345\n",
      "            : Test Loss :0.3887844979763031 Test Accuracy : 0.9739099740982056 \n",
      "****************************************************************************************************\n",
      "Iteration  177 : Loss =  0.33497694   Acc:  0.9774\n",
      "            : Test Loss :0.3882797956466675 Test Accuracy : 0.9738500118255615 \n",
      "****************************************************************************************************\n",
      "Iteration  178 : Loss =  0.33431575   Acc:  0.9774117\n",
      "            : Test Loss :0.3877931535243988 Test Accuracy : 0.9740200042724609 \n",
      "****************************************************************************************************\n",
      "Iteration  179 : Loss =  0.333605   Acc:  0.97757167\n",
      "            : Test Loss :0.387128084897995 Test Accuracy : 0.9739800095558167 \n",
      "****************************************************************************************************\n",
      "Iteration  180 : Loss =  0.33291653   Acc:  0.97755665\n",
      "            : Test Loss :0.3869435787200928 Test Accuracy : 0.9740700125694275 \n",
      "****************************************************************************************************\n",
      "Iteration  181 : Loss =  0.33235186   Acc:  0.9776083\n",
      "            : Test Loss :0.3862806260585785 Test Accuracy : 0.9740099906921387 \n",
      "****************************************************************************************************\n",
      "Iteration  182 : Loss =  0.3317868   Acc:  0.9775133\n",
      "            : Test Loss :0.38605213165283203 Test Accuracy : 0.9742199778556824 \n",
      "****************************************************************************************************\n",
      "Iteration  183 : Loss =  0.33112064   Acc:  0.9776483\n",
      "            : Test Loss :0.38523101806640625 Test Accuracy : 0.9740899801254272 \n",
      "****************************************************************************************************\n",
      "Iteration  184 : Loss =  0.3303751   Acc:  0.97761667\n",
      "            : Test Loss :0.38482871651649475 Test Accuracy : 0.9742799997329712 \n",
      "****************************************************************************************************\n",
      "Iteration  185 : Loss =  0.32962337   Acc:  0.9777383\n",
      "            : Test Loss :0.3842105567455292 Test Accuracy : 0.9742199778556824 \n",
      "****************************************************************************************************\n",
      "Iteration  186 : Loss =  0.32894874   Acc:  0.9777217\n",
      "            : Test Loss :0.3838179111480713 Test Accuracy : 0.9742799997329712 \n",
      "****************************************************************************************************\n",
      "Iteration  187 : Loss =  0.32835734   Acc:  0.97780335\n",
      "            : Test Loss :0.3834269642829895 Test Accuracy : 0.9744099974632263 \n",
      "****************************************************************************************************\n",
      "Iteration  188 : Loss =  0.32778215   Acc:  0.9777833\n",
      "            : Test Loss :0.3829341530799866 Test Accuracy : 0.9742299914360046 \n",
      "****************************************************************************************************\n",
      "Iteration  189 : Loss =  0.3271728   Acc:  0.9778583\n",
      "            : Test Loss :0.38251304626464844 Test Accuracy : 0.9744200110435486 \n",
      "****************************************************************************************************\n",
      "Iteration  190 : Loss =  0.32652298   Acc:  0.9779\n",
      "            : Test Loss :0.3819974660873413 Test Accuracy : 0.9743899703025818 \n",
      "****************************************************************************************************\n",
      "Iteration  191 : Loss =  0.32586646   Acc:  0.9779483\n",
      "            : Test Loss :0.3815702497959137 Test Accuracy : 0.9744600057601929 \n",
      "****************************************************************************************************\n",
      "Iteration  192 : Loss =  0.32524258   Acc:  0.97800833\n",
      "            : Test Loss :0.38120192289352417 Test Accuracy : 0.974560022354126 \n",
      "****************************************************************************************************\n",
      "Iteration  193 : Loss =  0.32465932   Acc:  0.978065\n",
      "            : Test Loss :0.38078489899635315 Test Accuracy : 0.9744300246238708 \n",
      "****************************************************************************************************\n",
      "Iteration  194 : Loss =  0.3240976   Acc:  0.97805166\n",
      "            : Test Loss :0.3804740607738495 Test Accuracy : 0.9745699763298035 \n",
      "****************************************************************************************************\n",
      "Iteration  195 : Loss =  0.32353052   Acc:  0.97810334\n",
      "            : Test Loss :0.3800056576728821 Test Accuracy : 0.9745000004768372 \n",
      "****************************************************************************************************\n",
      "Iteration  196 : Loss =  0.32294402   Acc:  0.9781233\n",
      "            : Test Loss :0.37964358925819397 Test Accuracy : 0.97461998462677 \n",
      "****************************************************************************************************\n",
      "Iteration  197 : Loss =  0.32233962   Acc:  0.9781917\n",
      "            : Test Loss :0.3791671395301819 Test Accuracy : 0.9745500087738037 \n",
      "****************************************************************************************************\n",
      "Iteration  198 : Loss =  0.3217286   Acc:  0.978215\n",
      "            : Test Loss :0.3787591755390167 Test Accuracy : 0.9746500253677368 \n",
      "****************************************************************************************************\n",
      "Iteration  199 : Loss =  0.32112738   Acc:  0.97827166\n",
      "            : Test Loss :0.3783796727657318 Test Accuracy : 0.9746299982070923 \n",
      "****************************************************************************************************\n",
      "Iteration  200 : Loss =  0.32054922   Acc:  0.9783317\n",
      "            : Test Loss :0.3779403567314148 Test Accuracy : 0.9746000170707703 \n",
      "****************************************************************************************************\n",
      "Iteration  201 : Loss =  0.32000807   Acc:  0.97835\n",
      "            : Test Loss :0.3777780830860138 Test Accuracy : 0.9746900200843811 \n",
      "****************************************************************************************************\n",
      "Iteration  202 : Loss =  0.31953594   Acc:  0.9784117\n",
      "            : Test Loss :0.3774173855781555 Test Accuracy : 0.9745500087738037 \n",
      "****************************************************************************************************\n",
      "Iteration  203 : Loss =  0.319205   Acc:  0.9784217\n",
      "            : Test Loss :0.37798872590065 Test Accuracy : 0.9745500087738037 \n",
      "****************************************************************************************************\n",
      "Iteration  204 : Loss =  0.3192746   Acc:  0.97840333\n",
      "            : Test Loss :0.37846288084983826 Test Accuracy : 0.9744499921798706 \n",
      "****************************************************************************************************\n",
      "Iteration  205 : Loss =  0.32001355   Acc:  0.97831666\n",
      "            : Test Loss :0.38210150599479675 Test Accuracy : 0.9742000102996826 \n",
      "****************************************************************************************************\n",
      "Iteration  206 : Loss =  0.32271394   Acc:  0.9782183\n",
      "            : Test Loss :0.38222017884254456 Test Accuracy : 0.9741500020027161 \n",
      "****************************************************************************************************\n",
      "Iteration  207 : Loss =  0.32350248   Acc:  0.97787833\n",
      "            : Test Loss :0.3837333917617798 Test Accuracy : 0.9742000102996826 \n",
      "****************************************************************************************************\n",
      "Iteration  208 : Loss =  0.32396242   Acc:  0.97808665\n",
      "            : Test Loss :0.3759368062019348 Test Accuracy : 0.9747400283813477 \n",
      "****************************************************************************************************\n",
      "Iteration  209 : Loss =  0.3169341   Acc:  0.9785233\n",
      "            : Test Loss :0.3748815655708313 Test Accuracy : 0.9747200012207031 \n",
      "****************************************************************************************************\n",
      "Iteration  210 : Loss =  0.31573647   Acc:  0.97862\n",
      "            : Test Loss :0.37932276725769043 Test Accuracy : 0.974399983882904 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  211 : Loss =  0.31933987   Acc:  0.9784283\n",
      "            : Test Loss :0.3766774833202362 Test Accuracy : 0.9744799733161926 \n",
      "****************************************************************************************************\n",
      "Iteration  212 : Loss =  0.3172138   Acc:  0.97842\n",
      "            : Test Loss :0.37370380759239197 Test Accuracy : 0.9748700261116028 \n",
      "****************************************************************************************************\n",
      "Iteration  213 : Loss =  0.31379777   Acc:  0.97872335\n",
      "            : Test Loss :0.3741007447242737 Test Accuracy : 0.9747899770736694 \n",
      "****************************************************************************************************\n",
      "Iteration  214 : Loss =  0.31391406   Acc:  0.97870165\n",
      "            : Test Loss :0.37517020106315613 Test Accuracy : 0.9745500087738037 \n",
      "****************************************************************************************************\n",
      "Iteration  215 : Loss =  0.3151045   Acc:  0.97862\n",
      "            : Test Loss :0.3747069239616394 Test Accuracy : 0.9747800230979919 \n",
      "****************************************************************************************************\n",
      "Iteration  216 : Loss =  0.31387877   Acc:  0.97875\n",
      "            : Test Loss :0.372116357088089 Test Accuracy : 0.9749799966812134 \n",
      "****************************************************************************************************\n",
      "Iteration  217 : Loss =  0.3115636   Acc:  0.978895\n",
      "            : Test Loss :0.37298184633255005 Test Accuracy : 0.9747400283813477 \n",
      "****************************************************************************************************\n",
      "Iteration  218 : Loss =  0.31238717   Acc:  0.97881836\n",
      "            : Test Loss :0.37456727027893066 Test Accuracy : 0.9747599959373474 \n",
      "****************************************************************************************************\n",
      "Iteration  219 : Loss =  0.31311584   Acc:  0.97874\n",
      "            : Test Loss :0.3715628981590271 Test Accuracy : 0.9749500155448914 \n",
      "****************************************************************************************************\n",
      "Iteration  220 : Loss =  0.31053177   Acc:  0.9789467\n",
      "            : Test Loss :0.37119510769844055 Test Accuracy : 0.9749600291252136 \n",
      "****************************************************************************************************\n",
      "Iteration  221 : Loss =  0.31002176   Acc:  0.9789733\n",
      "            : Test Loss :0.3728262186050415 Test Accuracy : 0.9749199748039246 \n",
      "****************************************************************************************************\n",
      "Iteration  222 : Loss =  0.31100032   Acc:  0.978865\n",
      "            : Test Loss :0.3710552453994751 Test Accuracy : 0.9748899936676025 \n",
      "****************************************************************************************************\n",
      "Iteration  223 : Loss =  0.30951095   Acc:  0.97899\n",
      "            : Test Loss :0.3699773848056793 Test Accuracy : 0.9750499725341797 \n",
      "****************************************************************************************************\n",
      "Iteration  224 : Loss =  0.30819428   Acc:  0.97910666\n",
      "            : Test Loss :0.37073028087615967 Test Accuracy : 0.9749199748039246 \n",
      "****************************************************************************************************\n",
      "Iteration  225 : Loss =  0.3085569   Acc:  0.9790483\n",
      "            : Test Loss :0.3702406883239746 Test Accuracy : 0.9749000072479248 \n",
      "****************************************************************************************************\n",
      "Iteration  226 : Loss =  0.3081785   Acc:  0.97907835\n",
      "            : Test Loss :0.36931467056274414 Test Accuracy : 0.9750900268554688 \n",
      "****************************************************************************************************\n",
      "Iteration  227 : Loss =  0.3069078   Acc:  0.9791567\n",
      "            : Test Loss :0.3690132200717926 Test Accuracy : 0.9750800132751465 \n",
      "****************************************************************************************************\n",
      "Iteration  228 : Loss =  0.30647504   Acc:  0.97916335\n",
      "            : Test Loss :0.3690674901008606 Test Accuracy : 0.9750999808311462 \n",
      "****************************************************************************************************\n",
      "Iteration  229 : Loss =  0.30652717   Acc:  0.97917\n",
      "            : Test Loss :0.3689228594303131 Test Accuracy : 0.9750699996948242 \n",
      "****************************************************************************************************\n",
      "Iteration  230 : Loss =  0.3058899   Acc:  0.97924\n",
      "            : Test Loss :0.36786317825317383 Test Accuracy : 0.9751600027084351 \n",
      "****************************************************************************************************\n",
      "Iteration  231 : Loss =  0.30491504   Acc:  0.97931665\n",
      "            : Test Loss :0.367779403924942 Test Accuracy : 0.9751499891281128 \n",
      "****************************************************************************************************\n",
      "Iteration  232 : Loss =  0.30474648   Acc:  0.97928166\n",
      "            : Test Loss :0.36818206310272217 Test Accuracy : 0.9750800132751465 \n",
      "****************************************************************************************************\n",
      "Iteration  233 : Loss =  0.30461523   Acc:  0.97931165\n",
      "            : Test Loss :0.36710992455482483 Test Accuracy : 0.9751600027084351 \n",
      "****************************************************************************************************\n",
      "Iteration  234 : Loss =  0.3036947   Acc:  0.97936165\n",
      "            : Test Loss :0.36670321226119995 Test Accuracy : 0.9752699732780457 \n",
      "****************************************************************************************************\n",
      "Iteration  235 : Loss =  0.30309674   Acc:  0.97943336\n",
      "            : Test Loss :0.36695215106010437 Test Accuracy : 0.9751799702644348 \n",
      "****************************************************************************************************\n",
      "Iteration  236 : Loss =  0.3029519   Acc:  0.97939664\n",
      "            : Test Loss :0.36638620495796204 Test Accuracy : 0.9751399755477905 \n",
      "****************************************************************************************************\n",
      "Iteration  237 : Loss =  0.30247623   Acc:  0.979445\n",
      "            : Test Loss :0.3659551739692688 Test Accuracy : 0.9752699732780457 \n",
      "****************************************************************************************************\n",
      "Iteration  238 : Loss =  0.30175242   Acc:  0.97948664\n",
      "            : Test Loss :0.3656788170337677 Test Accuracy : 0.9753599762916565 \n",
      "****************************************************************************************************\n",
      "Iteration  239 : Loss =  0.3012873   Acc:  0.97954166\n",
      "            : Test Loss :0.36548224091529846 Test Accuracy : 0.9752299785614014 \n",
      "****************************************************************************************************\n",
      "Iteration  240 : Loss =  0.30104005   Acc:  0.97952664\n",
      "            : Test Loss :0.36532828211784363 Test Accuracy : 0.9752500057220459 \n",
      "****************************************************************************************************\n",
      "Iteration  241 : Loss =  0.30055183   Acc:  0.97957\n",
      "            : Test Loss :0.3647158145904541 Test Accuracy : 0.9753100275993347 \n",
      "****************************************************************************************************\n",
      "Iteration  242 : Loss =  0.29990736   Acc:  0.97963834\n",
      "            : Test Loss :0.36448749899864197 Test Accuracy : 0.9753599762916565 \n",
      "****************************************************************************************************\n",
      "Iteration  243 : Loss =  0.29948652   Acc:  0.97966\n",
      "            : Test Loss :0.364471971988678 Test Accuracy : 0.9753299951553345 \n",
      "****************************************************************************************************\n",
      "Iteration  244 : Loss =  0.2991721   Acc:  0.9796633\n",
      "            : Test Loss :0.3639666736125946 Test Accuracy : 0.9753900170326233 \n",
      "****************************************************************************************************\n",
      "Iteration  245 : Loss =  0.2986799   Acc:  0.97970665\n",
      "            : Test Loss :0.3637041449546814 Test Accuracy : 0.975380003452301 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  246 : Loss =  0.29810828   Acc:  0.97976834\n",
      "            : Test Loss :0.3634474277496338 Test Accuracy : 0.9754499793052673 \n",
      "****************************************************************************************************\n",
      "Iteration  247 : Loss =  0.2976655   Acc:  0.9798\n",
      "            : Test Loss :0.36313778162002563 Test Accuracy : 0.9754199981689453 \n",
      "****************************************************************************************************\n",
      "Iteration  248 : Loss =  0.29730886   Acc:  0.979825\n",
      "            : Test Loss :0.36303406953811646 Test Accuracy : 0.9754400253295898 \n",
      "****************************************************************************************************\n",
      "Iteration  249 : Loss =  0.29686242   Acc:  0.97985166\n",
      "            : Test Loss :0.36256644129753113 Test Accuracy : 0.9754700064659119 \n",
      "****************************************************************************************************\n",
      "Iteration  250 : Loss =  0.2963267   Acc:  0.97985667\n",
      "            : Test Loss :0.3622795045375824 Test Accuracy : 0.9754899740219116 \n",
      "****************************************************************************************************\n",
      "Iteration  251 : Loss =  0.2958563   Acc:  0.9798783\n",
      "            : Test Loss :0.36215412616729736 Test Accuracy : 0.9754899740219116 \n",
      "****************************************************************************************************\n",
      "Iteration  252 : Loss =  0.29546383   Acc:  0.97992665\n",
      "            : Test Loss :0.36180707812309265 Test Accuracy : 0.9755100011825562 \n",
      "****************************************************************************************************\n",
      "Iteration  253 : Loss =  0.2950472   Acc:  0.9799467\n",
      "            : Test Loss :0.36159780621528625 Test Accuracy : 0.9755100011825562 \n",
      "****************************************************************************************************\n",
      "Iteration  254 : Loss =  0.29456386   Acc:  0.97998166\n",
      "            : Test Loss :0.3612205982208252 Test Accuracy : 0.9755399823188782 \n",
      "****************************************************************************************************\n",
      "Iteration  255 : Loss =  0.2940741   Acc:  0.98001665\n",
      "            : Test Loss :0.36099371314048767 Test Accuracy : 0.9755399823188782 \n",
      "****************************************************************************************************\n",
      "Iteration  256 : Loss =  0.2936418   Acc:  0.9800467\n",
      "            : Test Loss :0.36083516478538513 Test Accuracy : 0.9755499958992004 \n",
      "****************************************************************************************************\n",
      "Iteration  257 : Loss =  0.29323894   Acc:  0.98007834\n",
      "            : Test Loss :0.3604893088340759 Test Accuracy : 0.9756100177764893 \n",
      "****************************************************************************************************\n",
      "Iteration  258 : Loss =  0.29280466   Acc:  0.9800767\n",
      "            : Test Loss :0.36028242111206055 Test Accuracy : 0.9755499958992004 \n",
      "****************************************************************************************************\n",
      "Iteration  259 : Loss =  0.29233447   Acc:  0.98012\n",
      "            : Test Loss :0.3599802851676941 Test Accuracy : 0.9755899906158447 \n",
      "****************************************************************************************************\n",
      "Iteration  260 : Loss =  0.29187033   Acc:  0.98015165\n",
      "            : Test Loss :0.35969510674476624 Test Accuracy : 0.9756399989128113 \n",
      "****************************************************************************************************\n",
      "Iteration  261 : Loss =  0.2914451   Acc:  0.980185\n",
      "            : Test Loss :0.35958099365234375 Test Accuracy : 0.975600004196167 \n",
      "****************************************************************************************************\n",
      "Iteration  262 : Loss =  0.29103932   Acc:  0.9802\n",
      "            : Test Loss :0.35920923948287964 Test Accuracy : 0.9757199883460999 \n",
      "****************************************************************************************************\n",
      "Iteration  263 : Loss =  0.29062527   Acc:  0.98025167\n",
      "            : Test Loss :0.3590938448905945 Test Accuracy : 0.9756799936294556 \n",
      "****************************************************************************************************\n",
      "Iteration  264 : Loss =  0.29020122   Acc:  0.98024\n",
      "            : Test Loss :0.358761727809906 Test Accuracy : 0.9757800102233887 \n",
      "****************************************************************************************************\n",
      "Iteration  265 : Loss =  0.2898065   Acc:  0.98029333\n",
      "            : Test Loss :0.35875624418258667 Test Accuracy : 0.9757400155067444 \n",
      "****************************************************************************************************\n",
      "Iteration  266 : Loss =  0.28947988   Acc:  0.9803417\n",
      "            : Test Loss :0.35861045122146606 Test Accuracy : 0.9757599830627441 \n",
      "****************************************************************************************************\n",
      "Iteration  267 : Loss =  0.2892748   Acc:  0.9803117\n",
      "            : Test Loss :0.358928382396698 Test Accuracy : 0.9757000207901001 \n",
      "****************************************************************************************************\n",
      "Iteration  268 : Loss =  0.28920457   Acc:  0.98029333\n",
      "            : Test Loss :0.35904407501220703 Test Accuracy : 0.9757099747657776 \n",
      "****************************************************************************************************\n",
      "Iteration  269 : Loss =  0.2893549   Acc:  0.9803067\n",
      "            : Test Loss :0.35971683263778687 Test Accuracy : 0.9755100011825562 \n",
      "****************************************************************************************************\n",
      "Iteration  270 : Loss =  0.28948054   Acc:  0.980315\n",
      "            : Test Loss :0.3594677448272705 Test Accuracy : 0.9755600094795227 \n",
      "****************************************************************************************************\n",
      "Iteration  271 : Loss =  0.2895161   Acc:  0.980255\n",
      "            : Test Loss :0.3591510057449341 Test Accuracy : 0.9756500124931335 \n",
      "****************************************************************************************************\n",
      "Iteration  272 : Loss =  0.28850162   Acc:  0.980355\n",
      "            : Test Loss :0.35748663544654846 Test Accuracy : 0.9758899807929993 \n",
      "****************************************************************************************************\n",
      "Iteration  273 : Loss =  0.28718674   Acc:  0.9804583\n",
      "            : Test Loss :0.35691094398498535 Test Accuracy : 0.9758999943733215 \n",
      "****************************************************************************************************\n",
      "Iteration  274 : Loss =  0.2860806   Acc:  0.9805217\n",
      "            : Test Loss :0.356687992811203 Test Accuracy : 0.9759299755096436 \n",
      "****************************************************************************************************\n",
      "Iteration  275 : Loss =  0.28577814   Acc:  0.9805667\n",
      "            : Test Loss :0.3568649888038635 Test Accuracy : 0.9759899973869324 \n",
      "****************************************************************************************************\n",
      "Iteration  276 : Loss =  0.28585953   Acc:  0.98055166\n",
      "            : Test Loss :0.35708191990852356 Test Accuracy : 0.9758899807929993 \n",
      "****************************************************************************************************\n",
      "Iteration  277 : Loss =  0.28560665   Acc:  0.9806067\n",
      "            : Test Loss :0.3562975525856018 Test Accuracy : 0.9759899973869324 \n",
      "****************************************************************************************************\n",
      "Iteration  278 : Loss =  0.28500143   Acc:  0.98061\n",
      "            : Test Loss :0.3561016023159027 Test Accuracy : 0.9759899973869324 \n",
      "****************************************************************************************************\n",
      "Iteration  279 : Loss =  0.28424707   Acc:  0.98068833\n",
      "            : Test Loss :0.355495810508728 Test Accuracy : 0.9761800169944763 \n",
      "****************************************************************************************************\n",
      "Iteration  280 : Loss =  0.28375074   Acc:  0.98070335\n",
      "            : Test Loss :0.3555208444595337 Test Accuracy : 0.975820004940033 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  281 : Loss =  0.2834493   Acc:  0.9807183\n",
      "            : Test Loss :0.3552779257297516 Test Accuracy : 0.9761000275611877 \n",
      "****************************************************************************************************\n",
      "Iteration  282 : Loss =  0.2830184   Acc:  0.980755\n",
      "            : Test Loss :0.35475313663482666 Test Accuracy : 0.9762399792671204 \n",
      "****************************************************************************************************\n",
      "Iteration  283 : Loss =  0.2824767   Acc:  0.980805\n",
      "            : Test Loss :0.3547075092792511 Test Accuracy : 0.9762099981307983 \n",
      "****************************************************************************************************\n",
      "Iteration  284 : Loss =  0.28200248   Acc:  0.9808417\n",
      "            : Test Loss :0.35433411598205566 Test Accuracy : 0.9762600064277649 \n",
      "****************************************************************************************************\n",
      "Iteration  285 : Loss =  0.28170303   Acc:  0.98086\n",
      "            : Test Loss :0.3545067608356476 Test Accuracy : 0.9760800004005432 \n",
      "****************************************************************************************************\n",
      "Iteration  286 : Loss =  0.28145173   Acc:  0.98086\n",
      "            : Test Loss :0.3539970815181732 Test Accuracy : 0.9762799739837646 \n",
      "****************************************************************************************************\n",
      "Iteration  287 : Loss =  0.28095412   Acc:  0.98087835\n",
      "            : Test Loss :0.3536399304866791 Test Accuracy : 0.9762700200080872 \n",
      "****************************************************************************************************\n",
      "Iteration  288 : Loss =  0.28033486   Acc:  0.980975\n",
      "            : Test Loss :0.35322073101997375 Test Accuracy : 0.9764299988746643 \n",
      "****************************************************************************************************\n",
      "Iteration  289 : Loss =  0.27976692   Acc:  0.98097664\n",
      "            : Test Loss :0.35296422243118286 Test Accuracy : 0.9764099717140198 \n",
      "****************************************************************************************************\n",
      "Iteration  290 : Loss =  0.27940157   Acc:  0.98100835\n",
      "            : Test Loss :0.3530384302139282 Test Accuracy : 0.9762399792671204 \n",
      "****************************************************************************************************\n",
      "Iteration  291 : Loss =  0.2791731   Acc:  0.9810283\n",
      "            : Test Loss :0.3527536690235138 Test Accuracy : 0.97639000415802 \n",
      "****************************************************************************************************\n",
      "Iteration  292 : Loss =  0.2788866   Acc:  0.9810183\n",
      "            : Test Loss :0.3527213931083679 Test Accuracy : 0.9762600064277649 \n",
      "****************************************************************************************************\n",
      "Iteration  293 : Loss =  0.27848783   Acc:  0.9810867\n",
      "            : Test Loss :0.3521835207939148 Test Accuracy : 0.976419985294342 \n",
      "****************************************************************************************************\n",
      "Iteration  294 : Loss =  0.2779586   Acc:  0.9810867\n",
      "            : Test Loss :0.3520457148551941 Test Accuracy : 0.9764500260353088 \n",
      "****************************************************************************************************\n",
      "Iteration  295 : Loss =  0.27746385   Acc:  0.9811367\n",
      "            : Test Loss :0.3516632318496704 Test Accuracy : 0.9764599800109863 \n",
      "****************************************************************************************************\n",
      "Iteration  296 : Loss =  0.27704656   Acc:  0.9811817\n",
      "            : Test Loss :0.3515892028808594 Test Accuracy : 0.9765300154685974 \n",
      "****************************************************************************************************\n",
      "Iteration  297 : Loss =  0.27668408   Acc:  0.98120666\n",
      "            : Test Loss :0.35132256150245667 Test Accuracy : 0.9765499830245972 \n",
      "****************************************************************************************************\n",
      "Iteration  298 : Loss =  0.27631533   Acc:  0.98124164\n",
      "            : Test Loss :0.3511146306991577 Test Accuracy : 0.976580023765564 \n",
      "****************************************************************************************************\n",
      "Iteration  299 : Loss =  0.275899   Acc:  0.98127335\n",
      "            : Test Loss :0.350874125957489 Test Accuracy : 0.9765200018882751 \n",
      "****************************************************************************************************\n",
      "Iteration  300 : Loss =  0.27545887   Acc:  0.981295\n",
      "            : Test Loss :0.35056737065315247 Test Accuracy : 0.9765999913215637 \n",
      "****************************************************************************************************\n",
      "Iteration  301 : Loss =  0.27503803   Acc:  0.98134166\n",
      "            : Test Loss :0.35047921538352966 Test Accuracy : 0.9766600131988525 \n",
      "****************************************************************************************************\n",
      "Iteration  302 : Loss =  0.27466395   Acc:  0.9813617\n",
      "            : Test Loss :0.3501965403556824 Test Accuracy : 0.9765899777412415 \n",
      "****************************************************************************************************\n",
      "Iteration  303 : Loss =  0.27432996   Acc:  0.981375\n",
      "            : Test Loss :0.3502275049686432 Test Accuracy : 0.9765999913215637 \n",
      "****************************************************************************************************\n",
      "Iteration  304 : Loss =  0.27400613   Acc:  0.98143667\n",
      "            : Test Loss :0.3498718738555908 Test Accuracy : 0.976639986038208 \n",
      "****************************************************************************************************\n",
      "Iteration  305 : Loss =  0.27368176   Acc:  0.981375\n",
      "            : Test Loss :0.34996509552001953 Test Accuracy : 0.976639986038208 \n",
      "****************************************************************************************************\n",
      "Iteration  306 : Loss =  0.2733452   Acc:  0.981445\n",
      "            : Test Loss :0.3495413064956665 Test Accuracy : 0.9765999913215637 \n",
      "****************************************************************************************************\n",
      "Iteration  307 : Loss =  0.2730355   Acc:  0.98141\n",
      "            : Test Loss :0.3497949540615082 Test Accuracy : 0.976610004901886 \n",
      "****************************************************************************************************\n",
      "Iteration  308 : Loss =  0.27277362   Acc:  0.98145\n",
      "            : Test Loss :0.34937822818756104 Test Accuracy : 0.9765300154685974 \n",
      "****************************************************************************************************\n",
      "Iteration  309 : Loss =  0.27257884   Acc:  0.98146\n",
      "            : Test Loss :0.349948525428772 Test Accuracy : 0.9765400290489197 \n",
      "****************************************************************************************************\n",
      "Iteration  310 : Loss =  0.27252495   Acc:  0.98143166\n",
      "            : Test Loss :0.3495219349861145 Test Accuracy : 0.9765599966049194 \n",
      "****************************************************************************************************\n",
      "Iteration  311 : Loss =  0.2724372   Acc:  0.98147166\n",
      "            : Test Loss :0.35041895508766174 Test Accuracy : 0.9763799905776978 \n",
      "****************************************************************************************************\n",
      "Iteration  312 : Loss =  0.27255115   Acc:  0.9814067\n",
      "            : Test Loss :0.34963303804397583 Test Accuracy : 0.9765599966049194 \n",
      "****************************************************************************************************\n",
      "Iteration  313 : Loss =  0.2722293   Acc:  0.98143667\n",
      "            : Test Loss :0.3501948118209839 Test Accuracy : 0.9764000177383423 \n",
      "****************************************************************************************************\n",
      "Iteration  314 : Loss =  0.2719713   Acc:  0.98145\n",
      "            : Test Loss :0.3488384783267975 Test Accuracy : 0.976580023765564 \n",
      "****************************************************************************************************\n",
      "Iteration  315 : Loss =  0.27105534   Acc:  0.981525\n",
      "            : Test Loss :0.3485964834690094 Test Accuracy : 0.9766700267791748 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  316 : Loss =  0.2701443   Acc:  0.981595\n",
      "            : Test Loss :0.34751737117767334 Test Accuracy : 0.9765999913215637 \n",
      "****************************************************************************************************\n",
      "Iteration  317 : Loss =  0.26924577   Acc:  0.98167336\n",
      "            : Test Loss :0.34726402163505554 Test Accuracy : 0.9767299890518188 \n",
      "****************************************************************************************************\n",
      "Iteration  318 : Loss =  0.26863703   Acc:  0.98182833\n",
      "            : Test Loss :0.3471674919128418 Test Accuracy : 0.9766899943351746 \n",
      "****************************************************************************************************\n",
      "Iteration  319 : Loss =  0.268341   Acc:  0.981805\n",
      "            : Test Loss :0.3470802307128906 Test Accuracy : 0.9766700267791748 \n",
      "****************************************************************************************************\n",
      "Iteration  320 : Loss =  0.2682376   Acc:  0.98177\n",
      "            : Test Loss :0.34759286046028137 Test Accuracy : 0.9767299890518188 \n",
      "****************************************************************************************************\n",
      "Iteration  321 : Loss =  0.26823398   Acc:  0.9817617\n",
      "            : Test Loss :0.3471437394618988 Test Accuracy : 0.9766200184822083 \n",
      "****************************************************************************************************\n",
      "Iteration  322 : Loss =  0.26805758   Acc:  0.9817283\n",
      "            : Test Loss :0.34761717915534973 Test Accuracy : 0.9767299890518188 \n",
      "****************************************************************************************************\n",
      "Iteration  323 : Loss =  0.26784906   Acc:  0.9817517\n",
      "            : Test Loss :0.3467710018157959 Test Accuracy : 0.9766700267791748 \n",
      "****************************************************************************************************\n",
      "Iteration  324 : Loss =  0.26732343   Acc:  0.9817167\n",
      "            : Test Loss :0.34684911370277405 Test Accuracy : 0.9768199920654297 \n",
      "****************************************************************************************************\n",
      "Iteration  325 : Loss =  0.26674876   Acc:  0.981885\n",
      "            : Test Loss :0.34602564573287964 Test Accuracy : 0.9767600297927856 \n",
      "****************************************************************************************************\n",
      "Iteration  326 : Loss =  0.26611716   Acc:  0.98192\n",
      "            : Test Loss :0.3459177613258362 Test Accuracy : 0.9768499732017517 \n",
      "****************************************************************************************************\n",
      "Iteration  327 : Loss =  0.26555452   Acc:  0.98207164\n",
      "            : Test Loss :0.3455554246902466 Test Accuracy : 0.9767400026321411 \n",
      "****************************************************************************************************\n",
      "Iteration  328 : Loss =  0.2651353   Acc:  0.98203164\n",
      "            : Test Loss :0.34545570611953735 Test Accuracy : 0.9768400192260742 \n",
      "****************************************************************************************************\n",
      "Iteration  329 : Loss =  0.2648329   Acc:  0.98208\n",
      "            : Test Loss :0.34553611278533936 Test Accuracy : 0.9769300222396851 \n",
      "****************************************************************************************************\n",
      "Iteration  330 : Loss =  0.26461428   Acc:  0.98200333\n",
      "            : Test Loss :0.34527021646499634 Test Accuracy : 0.9767699837684631 \n",
      "****************************************************************************************************\n",
      "Iteration  331 : Loss =  0.26438582   Acc:  0.982075\n",
      "            : Test Loss :0.3455438017845154 Test Accuracy : 0.9769799709320068 \n",
      "****************************************************************************************************\n",
      "Iteration  332 : Loss =  0.26417917   Acc:  0.982035\n",
      "            : Test Loss :0.3450327217578888 Test Accuracy : 0.9767900109291077 \n",
      "****************************************************************************************************\n",
      "Iteration  333 : Loss =  0.26384276   Acc:  0.9820967\n",
      "            : Test Loss :0.3452553153038025 Test Accuracy : 0.9769799709320068 \n",
      "****************************************************************************************************\n",
      "Iteration  334 : Loss =  0.26351023   Acc:  0.982105\n",
      "            : Test Loss :0.34460097551345825 Test Accuracy : 0.9767400026321411 \n",
      "****************************************************************************************************\n",
      "Iteration  335 : Loss =  0.26305327   Acc:  0.98213667\n",
      "            : Test Loss :0.3447158634662628 Test Accuracy : 0.9770900011062622 \n",
      "****************************************************************************************************\n",
      "Iteration  336 : Loss =  0.26260796   Acc:  0.9822\n",
      "            : Test Loss :0.34412911534309387 Test Accuracy : 0.9767900109291077 \n",
      "****************************************************************************************************\n",
      "Iteration  337 : Loss =  0.26215905   Acc:  0.98218\n",
      "            : Test Loss :0.34418582916259766 Test Accuracy : 0.9770799875259399 \n",
      "****************************************************************************************************\n",
      "Iteration  338 : Loss =  0.26174092   Acc:  0.98226166\n",
      "            : Test Loss :0.34379369020462036 Test Accuracy : 0.9767900109291077 \n",
      "****************************************************************************************************\n",
      "Iteration  339 : Loss =  0.2613794   Acc:  0.9822367\n",
      "            : Test Loss :0.34383147954940796 Test Accuracy : 0.9770299792289734 \n",
      "****************************************************************************************************\n",
      "Iteration  340 : Loss =  0.26105854   Acc:  0.98231834\n",
      "            : Test Loss :0.34364068508148193 Test Accuracy : 0.976859986782074 \n",
      "****************************************************************************************************\n",
      "Iteration  341 : Loss =  0.26078418   Acc:  0.9822367\n",
      "            : Test Loss :0.3436305522918701 Test Accuracy : 0.9769399762153625 \n",
      "****************************************************************************************************\n",
      "Iteration  342 : Loss =  0.2605322   Acc:  0.982345\n",
      "            : Test Loss :0.34355536103248596 Test Accuracy : 0.9769999980926514 \n",
      "****************************************************************************************************\n",
      "Iteration  343 : Loss =  0.26030046   Acc:  0.98222166\n",
      "            : Test Loss :0.3435080051422119 Test Accuracy : 0.9769499897956848 \n",
      "****************************************************************************************************\n",
      "Iteration  344 : Loss =  0.26006487   Acc:  0.9823717\n",
      "            : Test Loss :0.34351733326911926 Test Accuracy : 0.9770799875259399 \n",
      "****************************************************************************************************\n",
      "Iteration  345 : Loss =  0.259838   Acc:  0.9822083\n",
      "            : Test Loss :0.3433270752429962 Test Accuracy : 0.9769300222396851 \n",
      "****************************************************************************************************\n",
      "Iteration  346 : Loss =  0.25955135   Acc:  0.98239\n",
      "            : Test Loss :0.3433263599872589 Test Accuracy : 0.9770799875259399 \n",
      "****************************************************************************************************\n",
      "Iteration  347 : Loss =  0.25925714   Acc:  0.9822583\n",
      "            : Test Loss :0.3429472744464874 Test Accuracy : 0.9769399762153625 \n",
      "****************************************************************************************************\n",
      "Iteration  348 : Loss =  0.25885296   Acc:  0.9824517\n",
      "            : Test Loss :0.3429371118545532 Test Accuracy : 0.9771199822425842 \n",
      "****************************************************************************************************\n",
      "Iteration  349 : Loss =  0.25847146   Acc:  0.98232\n",
      "            : Test Loss :0.342425674200058 Test Accuracy : 0.9769099950790405 \n",
      "****************************************************************************************************\n",
      "Iteration  350 : Loss =  0.25800872   Acc:  0.98253334\n",
      "            : Test Loss :0.34252476692199707 Test Accuracy : 0.9771400094032288 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  351 : Loss =  0.2576463   Acc:  0.98238\n",
      "            : Test Loss :0.3420349657535553 Test Accuracy : 0.9769300222396851 \n",
      "****************************************************************************************************\n",
      "Iteration  352 : Loss =  0.25730404   Acc:  0.982555\n",
      "            : Test Loss :0.34244418144226074 Test Accuracy : 0.9771999716758728 \n",
      "****************************************************************************************************\n",
      "Iteration  353 : Loss =  0.25712085   Acc:  0.98246\n",
      "            : Test Loss :0.3420664370059967 Test Accuracy : 0.9768400192260742 \n",
      "****************************************************************************************************\n",
      "Iteration  354 : Loss =  0.25705066   Acc:  0.9824517\n",
      "            : Test Loss :0.3428603410720825 Test Accuracy : 0.9771599769592285 \n",
      "****************************************************************************************************\n",
      "Iteration  355 : Loss =  0.2570836   Acc:  0.98247\n",
      "            : Test Loss :0.3426119387149811 Test Accuracy : 0.9767100214958191 \n",
      "****************************************************************************************************\n",
      "Iteration  356 : Loss =  0.2572944   Acc:  0.98237336\n",
      "            : Test Loss :0.3435154855251312 Test Accuracy : 0.9771900177001953 \n",
      "****************************************************************************************************\n",
      "Iteration  357 : Loss =  0.25728455   Acc:  0.98243666\n",
      "            : Test Loss :0.3429063856601715 Test Accuracy : 0.9765599966049194 \n",
      "****************************************************************************************************\n",
      "Iteration  358 : Loss =  0.25726503   Acc:  0.98235\n",
      "            : Test Loss :0.34309136867523193 Test Accuracy : 0.9772099852561951 \n",
      "****************************************************************************************************\n",
      "Iteration  359 : Loss =  0.25649837   Acc:  0.9824967\n",
      "            : Test Loss :0.3416602611541748 Test Accuracy : 0.9768900275230408 \n",
      "****************************************************************************************************\n",
      "Iteration  360 : Loss =  0.25555   Acc:  0.98251164\n",
      "            : Test Loss :0.3413749635219574 Test Accuracy : 0.9773300290107727 \n",
      "****************************************************************************************************\n",
      "Iteration  361 : Loss =  0.25455397   Acc:  0.98262\n",
      "            : Test Loss :0.34064438939094543 Test Accuracy : 0.9770299792289734 \n",
      "****************************************************************************************************\n",
      "Iteration  362 : Loss =  0.25392914   Acc:  0.98281\n",
      "            : Test Loss :0.3407893478870392 Test Accuracy : 0.9771299958229065 \n",
      "****************************************************************************************************\n",
      "Iteration  363 : Loss =  0.25378072   Acc:  0.9826583\n",
      "            : Test Loss :0.3410964906215668 Test Accuracy : 0.9770500063896179 \n",
      "****************************************************************************************************\n",
      "Iteration  364 : Loss =  0.25379854   Acc:  0.98275834\n",
      "            : Test Loss :0.34092557430267334 Test Accuracy : 0.976830005645752 \n",
      "****************************************************************************************************\n",
      "Iteration  365 : Loss =  0.25370255   Acc:  0.9826483\n",
      "            : Test Loss :0.3408857583999634 Test Accuracy : 0.9770600199699402 \n",
      "****************************************************************************************************\n",
      "Iteration  366 : Loss =  0.25319195   Acc:  0.98275834\n",
      "            : Test Loss :0.3400793969631195 Test Accuracy : 0.9769600033760071 \n",
      "****************************************************************************************************\n",
      "Iteration  367 : Loss =  0.25251865   Acc:  0.98276335\n",
      "            : Test Loss :0.3398200571537018 Test Accuracy : 0.9772999882698059 \n",
      "****************************************************************************************************\n",
      "Iteration  368 : Loss =  0.2518571   Acc:  0.98282\n",
      "            : Test Loss :0.3395162522792816 Test Accuracy : 0.9769999980926514 \n",
      "****************************************************************************************************\n",
      "Iteration  369 : Loss =  0.25149786   Acc:  0.98296833\n",
      "            : Test Loss :0.33968088030815125 Test Accuracy : 0.977180004119873 \n",
      "****************************************************************************************************\n",
      "Iteration  370 : Loss =  0.25142783   Acc:  0.982815\n",
      "            : Test Loss :0.33990079164505005 Test Accuracy : 0.9771699905395508 \n",
      "****************************************************************************************************\n",
      "Iteration  371 : Loss =  0.25142553   Acc:  0.98291665\n",
      "            : Test Loss :0.3399479389190674 Test Accuracy : 0.9770799875259399 \n",
      "****************************************************************************************************\n",
      "Iteration  372 : Loss =  0.25130594   Acc:  0.98280334\n",
      "            : Test Loss :0.3397487699985504 Test Accuracy : 0.9771199822425842 \n",
      "****************************************************************************************************\n",
      "Iteration  373 : Loss =  0.2509475   Acc:  0.98296165\n",
      "            : Test Loss :0.33958399295806885 Test Accuracy : 0.9772999882698059 \n",
      "****************************************************************************************************\n",
      "Iteration  374 : Loss =  0.25051335   Acc:  0.982885\n",
      "            : Test Loss :0.3391569256782532 Test Accuracy : 0.9770699739456177 \n",
      "****************************************************************************************************\n",
      "Iteration  375 : Loss =  0.25010717   Acc:  0.983025\n",
      "            : Test Loss :0.33955633640289307 Test Accuracy : 0.9773399829864502 \n",
      "****************************************************************************************************\n",
      "Iteration  376 : Loss =  0.24997902   Acc:  0.98290664\n",
      "            : Test Loss :0.33921799063682556 Test Accuracy : 0.9770500063896179 \n",
      "****************************************************************************************************\n",
      "Iteration  377 : Loss =  0.24995083   Acc:  0.9829933\n",
      "            : Test Loss :0.34011000394821167 Test Accuracy : 0.9774100184440613 \n",
      "****************************************************************************************************\n",
      "Iteration  378 : Loss =  0.25006285   Acc:  0.98291\n",
      "            : Test Loss :0.33951568603515625 Test Accuracy : 0.9769099950790405 \n",
      "****************************************************************************************************\n",
      "Iteration  379 : Loss =  0.24997538   Acc:  0.9828917\n",
      "            : Test Loss :0.3402525782585144 Test Accuracy : 0.9773899912834167 \n",
      "****************************************************************************************************\n",
      "Iteration  380 : Loss =  0.24980348   Acc:  0.9828783\n",
      "            : Test Loss :0.3392198383808136 Test Accuracy : 0.9770200252532959 \n",
      "****************************************************************************************************\n",
      "Iteration  381 : Loss =  0.24925894   Acc:  0.98299\n",
      "            : Test Loss :0.33961546421051025 Test Accuracy : 0.9772899746894836 \n",
      "****************************************************************************************************\n",
      "Iteration  382 : Loss =  0.2488622   Acc:  0.9829367\n",
      "            : Test Loss :0.33869194984436035 Test Accuracy : 0.9771199822425842 \n",
      "****************************************************************************************************\n",
      "Iteration  383 : Loss =  0.24821042   Acc:  0.9831283\n",
      "            : Test Loss :0.33880797028541565 Test Accuracy : 0.9772199988365173 \n",
      "****************************************************************************************************\n",
      "Iteration  384 : Loss =  0.24780062   Acc:  0.983035\n",
      "            : Test Loss :0.3382820188999176 Test Accuracy : 0.977180004119873 \n",
      "****************************************************************************************************\n",
      "Iteration  385 : Loss =  0.24726294   Acc:  0.9832367\n",
      "            : Test Loss :0.33798229694366455 Test Accuracy : 0.9772800207138062 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  386 : Loss =  0.24673815   Acc:  0.98313665\n",
      "            : Test Loss :0.3376529812812805 Test Accuracy : 0.9772999882698059 \n",
      "****************************************************************************************************\n",
      "Iteration  387 : Loss =  0.24619837   Acc:  0.98325336\n",
      "            : Test Loss :0.3372824192047119 Test Accuracy : 0.9772499799728394 \n",
      "****************************************************************************************************\n",
      "Iteration  388 : Loss =  0.24575058   Acc:  0.98326\n",
      "            : Test Loss :0.3372964560985565 Test Accuracy : 0.9773799777030945 \n",
      "****************************************************************************************************\n",
      "Iteration  389 : Loss =  0.24546155   Acc:  0.983275\n",
      "            : Test Loss :0.33720195293426514 Test Accuracy : 0.9771900177001953 \n",
      "****************************************************************************************************\n",
      "Iteration  390 : Loss =  0.24532513   Acc:  0.9833367\n",
      "            : Test Loss :0.337538480758667 Test Accuracy : 0.9772999882698059 \n",
      "****************************************************************************************************\n",
      "Iteration  391 : Loss =  0.24529333   Acc:  0.9832133\n",
      "            : Test Loss :0.3374640643596649 Test Accuracy : 0.9772599935531616 \n",
      "****************************************************************************************************\n",
      "Iteration  392 : Loss =  0.24521703   Acc:  0.983365\n",
      "            : Test Loss :0.3378283679485321 Test Accuracy : 0.9771699905395508 \n",
      "****************************************************************************************************\n",
      "Iteration  393 : Loss =  0.24515286   Acc:  0.98319\n",
      "            : Test Loss :0.3374256491661072 Test Accuracy : 0.9771299958229065 \n",
      "****************************************************************************************************\n",
      "Iteration  394 : Loss =  0.24487728   Acc:  0.98333\n",
      "            : Test Loss :0.3377626836299896 Test Accuracy : 0.9771699905395508 \n",
      "****************************************************************************************************\n",
      "Iteration  395 : Loss =  0.2446776   Acc:  0.983225\n",
      "            : Test Loss :0.33710020780563354 Test Accuracy : 0.9771299958229065 \n",
      "****************************************************************************************************\n",
      "Iteration  396 : Loss =  0.24427432   Acc:  0.9833883\n",
      "            : Test Loss :0.3375247120857239 Test Accuracy : 0.9773600101470947 \n",
      "****************************************************************************************************\n",
      "Iteration  397 : Loss =  0.24402913   Acc:  0.9832733\n",
      "            : Test Loss :0.33682069182395935 Test Accuracy : 0.9771599769592285 \n",
      "****************************************************************************************************\n",
      "Iteration  398 : Loss =  0.24364953   Acc:  0.983415\n",
      "            : Test Loss :0.3372032344341278 Test Accuracy : 0.9774399995803833 \n",
      "****************************************************************************************************\n",
      "Iteration  399 : Loss =  0.24331416   Acc:  0.983395\n",
      "            : Test Loss :0.33646559715270996 Test Accuracy : 0.9771199822425842 \n",
      "****************************************************************************************************\n",
      "Iteration  400 : Loss =  0.24289033   Acc:  0.98346835\n",
      "            : Test Loss :0.3366243541240692 Test Accuracy : 0.9775300025939941 \n",
      "****************************************************************************************************\n",
      "Iteration  401 : Loss =  0.24241592   Acc:  0.9834833\n",
      "            : Test Loss :0.33596840500831604 Test Accuracy : 0.9771999716758728 \n",
      "****************************************************************************************************\n",
      "Iteration  402 : Loss =  0.24193196   Acc:  0.98357\n",
      "            : Test Loss :0.33596619963645935 Test Accuracy : 0.9774600267410278 \n",
      "****************************************************************************************************\n",
      "Iteration  403 : Loss =  0.24147283   Acc:  0.983575\n",
      "            : Test Loss :0.3356225788593292 Test Accuracy : 0.9773200154304504 \n",
      "****************************************************************************************************\n",
      "Iteration  404 : Loss =  0.24108516   Acc:  0.9836183\n",
      "            : Test Loss :0.33554866909980774 Test Accuracy : 0.977400004863739 \n",
      "****************************************************************************************************\n",
      "Iteration  405 : Loss =  0.24077559   Acc:  0.98366666\n",
      "            : Test Loss :0.33554312586784363 Test Accuracy : 0.9774600267410278 \n",
      "****************************************************************************************************\n",
      "Iteration  406 : Loss =  0.24052644   Acc:  0.9836767\n",
      "            : Test Loss :0.3353826105594635 Test Accuracy : 0.9772599935531616 \n",
      "****************************************************************************************************\n",
      "Iteration  407 : Loss =  0.24031347   Acc:  0.98369664\n",
      "            : Test Loss :0.33557718992233276 Test Accuracy : 0.9775999784469604 \n",
      "****************************************************************************************************\n",
      "Iteration  408 : Loss =  0.24010867   Acc:  0.9837083\n",
      "            : Test Loss :0.33528706431388855 Test Accuracy : 0.9772499799728394 \n",
      "****************************************************************************************************\n",
      "Iteration  409 : Loss =  0.23991162   Acc:  0.9837217\n",
      "            : Test Loss :0.33560046553611755 Test Accuracy : 0.9775699973106384 \n",
      "****************************************************************************************************\n",
      "Iteration  410 : Loss =  0.239711   Acc:  0.9837083\n",
      "            : Test Loss :0.3352329134941101 Test Accuracy : 0.9772899746894836 \n",
      "****************************************************************************************************\n",
      "Iteration  411 : Loss =  0.23953548   Acc:  0.9837317\n",
      "            : Test Loss :0.33574801683425903 Test Accuracy : 0.9775800108909607 \n",
      "****************************************************************************************************\n",
      "Iteration  412 : Loss =  0.23942296   Acc:  0.98367834\n",
      "            : Test Loss :0.335391104221344 Test Accuracy : 0.9773100018501282 \n",
      "****************************************************************************************************\n",
      "Iteration  413 : Loss =  0.23934798   Acc:  0.98373336\n",
      "            : Test Loss :0.3363058567047119 Test Accuracy : 0.977400004863739 \n",
      "****************************************************************************************************\n",
      "Iteration  414 : Loss =  0.2395124   Acc:  0.983665\n",
      "            : Test Loss :0.33595651388168335 Test Accuracy : 0.9771900177001953 \n",
      "****************************************************************************************************\n",
      "Iteration  415 : Loss =  0.23958948   Acc:  0.98371\n",
      "            : Test Loss :0.3374159336090088 Test Accuracy : 0.9772599935531616 \n",
      "****************************************************************************************************\n",
      "Iteration  416 : Loss =  0.24016298   Acc:  0.98352\n",
      "            : Test Loss :0.33674418926239014 Test Accuracy : 0.9772099852561951 \n",
      "****************************************************************************************************\n",
      "Iteration  417 : Loss =  0.24001871   Acc:  0.9836417\n",
      "            : Test Loss :0.33799827098846436 Test Accuracy : 0.9771400094032288 \n",
      "****************************************************************************************************\n",
      "Iteration  418 : Loss =  0.24033807   Acc:  0.983425\n",
      "            : Test Loss :0.33633118867874146 Test Accuracy : 0.9771900177001953 \n",
      "****************************************************************************************************\n",
      "Iteration  419 : Loss =  0.23916532   Acc:  0.98368335\n",
      "            : Test Loss :0.3360386788845062 Test Accuracy : 0.9772899746894836 \n",
      "****************************************************************************************************\n",
      "Iteration  420 : Loss =  0.23818707   Acc:  0.9836917\n",
      "            : Test Loss :0.3345801532268524 Test Accuracy : 0.9774399995803833 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  421 : Loss =  0.2368571   Acc:  0.9839583\n",
      "            : Test Loss :0.33405712246894836 Test Accuracy : 0.9774100184440613 \n",
      "****************************************************************************************************\n",
      "Iteration  422 : Loss =  0.23608044   Acc:  0.9839683\n",
      "            : Test Loss :0.33426162600517273 Test Accuracy : 0.977649986743927 \n",
      "****************************************************************************************************\n",
      "Iteration  423 : Loss =  0.23590513   Acc:  0.98399836\n",
      "            : Test Loss :0.33436185121536255 Test Accuracy : 0.977429986000061 \n",
      "****************************************************************************************************\n",
      "Iteration  424 : Loss =  0.2361388   Acc:  0.9839633\n",
      "            : Test Loss :0.3355763554573059 Test Accuracy : 0.9774699807167053 \n",
      "****************************************************************************************************\n",
      "Iteration  425 : Loss =  0.23658232   Acc:  0.9838333\n",
      "            : Test Loss :0.33506280183792114 Test Accuracy : 0.9772700071334839 \n",
      "****************************************************************************************************\n",
      "Iteration  426 : Loss =  0.23653768   Acc:  0.98389\n",
      "            : Test Loss :0.3358518183231354 Test Accuracy : 0.9773200154304504 \n",
      "****************************************************************************************************\n",
      "Iteration  427 : Loss =  0.23645158   Acc:  0.98378\n",
      "            : Test Loss :0.3344530761241913 Test Accuracy : 0.9772999882698059 \n",
      "****************************************************************************************************\n",
      "Iteration  428 : Loss =  0.23551893   Acc:  0.983945\n",
      "            : Test Loss :0.33424291014671326 Test Accuracy : 0.9774900078773499 \n",
      "****************************************************************************************************\n",
      "Iteration  429 : Loss =  0.23465824   Acc:  0.98399836\n",
      "            : Test Loss :0.33336567878723145 Test Accuracy : 0.9776099920272827 \n",
      "****************************************************************************************************\n",
      "Iteration  430 : Loss =  0.23387724   Acc:  0.98415834\n",
      "            : Test Loss :0.33330824971199036 Test Accuracy : 0.9776600003242493 \n",
      "****************************************************************************************************\n",
      "Iteration  431 : Loss =  0.23349708   Acc:  0.984185\n",
      "            : Test Loss :0.3336057960987091 Test Accuracy : 0.9775500297546387 \n",
      "****************************************************************************************************\n",
      "Iteration  432 : Loss =  0.23345922   Acc:  0.98413336\n",
      "            : Test Loss :0.3336326777935028 Test Accuracy : 0.9774900078773499 \n",
      "****************************************************************************************************\n",
      "Iteration  433 : Loss =  0.2335379   Acc:  0.98418\n",
      "            : Test Loss :0.33429986238479614 Test Accuracy : 0.977429986000061 \n",
      "****************************************************************************************************\n",
      "Iteration  434 : Loss =  0.23363437   Acc:  0.9840317\n",
      "            : Test Loss :0.33374857902526855 Test Accuracy : 0.9774699807167053 \n",
      "****************************************************************************************************\n",
      "Iteration  435 : Loss =  0.23336433   Acc:  0.98413336\n",
      "            : Test Loss :0.33400511741638184 Test Accuracy : 0.9774699807167053 \n",
      "****************************************************************************************************\n",
      "Iteration  436 : Loss =  0.23302099   Acc:  0.9841033\n",
      "            : Test Loss :0.33310699462890625 Test Accuracy : 0.9775199890136719 \n",
      "****************************************************************************************************\n",
      "Iteration  437 : Loss =  0.23237139   Acc:  0.98423165\n",
      "            : Test Loss :0.33301398158073425 Test Accuracy : 0.977590024471283 \n",
      "****************************************************************************************************\n",
      "Iteration  438 : Loss =  0.23178922   Acc:  0.98423165\n",
      "            : Test Loss :0.3325803279876709 Test Accuracy : 0.9776399731636047 \n",
      "****************************************************************************************************\n",
      "Iteration  439 : Loss =  0.2313123   Acc:  0.98427665\n",
      "            : Test Loss :0.33254721760749817 Test Accuracy : 0.9776600003242493 \n",
      "****************************************************************************************************\n",
      "Iteration  440 : Loss =  0.23102519   Acc:  0.98428166\n",
      "            : Test Loss :0.3326791822910309 Test Accuracy : 0.9775500297546387 \n",
      "****************************************************************************************************\n",
      "Iteration  441 : Loss =  0.2308829   Acc:  0.9842967\n",
      "            : Test Loss :0.3326113820075989 Test Accuracy : 0.977620005607605 \n",
      "****************************************************************************************************\n",
      "Iteration  442 : Loss =  0.2307996   Acc:  0.984335\n",
      "            : Test Loss :0.33293843269348145 Test Accuracy : 0.9775300025939941 \n",
      "****************************************************************************************************\n",
      "Iteration  443 : Loss =  0.23073646   Acc:  0.98427165\n",
      "            : Test Loss :0.33266112208366394 Test Accuracy : 0.9775400161743164 \n",
      "****************************************************************************************************\n",
      "Iteration  444 : Loss =  0.23056246   Acc:  0.98433\n",
      "            : Test Loss :0.33290624618530273 Test Accuracy : 0.9775300025939941 \n",
      "****************************************************************************************************\n",
      "Iteration  445 : Loss =  0.23036921   Acc:  0.9842683\n",
      "            : Test Loss :0.3324877917766571 Test Accuracy : 0.977590024471283 \n",
      "****************************************************************************************************\n",
      "Iteration  446 : Loss =  0.23000944   Acc:  0.984355\n",
      "            : Test Loss :0.3324638605117798 Test Accuracy : 0.9774600267410278 \n",
      "****************************************************************************************************\n",
      "Iteration  447 : Loss =  0.22964332   Acc:  0.9843283\n",
      "            : Test Loss :0.3321751058101654 Test Accuracy : 0.9777299761772156 \n",
      "****************************************************************************************************\n",
      "Iteration  448 : Loss =  0.22924301   Acc:  0.98442\n",
      "            : Test Loss :0.3320075571537018 Test Accuracy : 0.9773600101470947 \n",
      "****************************************************************************************************\n",
      "Iteration  449 : Loss =  0.22892676   Acc:  0.98437166\n",
      "            : Test Loss :0.33219319581985474 Test Accuracy : 0.9777500033378601 \n",
      "****************************************************************************************************\n",
      "Iteration  450 : Loss =  0.22873093   Acc:  0.984455\n",
      "            : Test Loss :0.33206266164779663 Test Accuracy : 0.9773799777030945 \n",
      "****************************************************************************************************\n",
      "Iteration  451 : Loss =  0.22875488   Acc:  0.98440164\n",
      "            : Test Loss :0.3329007029533386 Test Accuracy : 0.9776899814605713 \n",
      "****************************************************************************************************\n",
      "Iteration  452 : Loss =  0.22890335   Acc:  0.9843683\n",
      "            : Test Loss :0.3329184651374817 Test Accuracy : 0.9772599935531616 \n",
      "****************************************************************************************************\n",
      "Iteration  453 : Loss =  0.22941485   Acc:  0.9842783\n",
      "            : Test Loss :0.33431485295295715 Test Accuracy : 0.977590024471283 \n",
      "****************************************************************************************************\n",
      "Iteration  454 : Loss =  0.22977525   Acc:  0.98426\n",
      "            : Test Loss :0.3341529667377472 Test Accuracy : 0.9770500063896179 \n",
      "****************************************************************************************************\n",
      "Iteration  455 : Loss =  0.23034678   Acc:  0.98406166\n",
      "            : Test Loss :0.33488836884498596 Test Accuracy : 0.9774199724197388 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  456 : Loss =  0.22986929   Acc:  0.984195\n",
      "            : Test Loss :0.3332611918449402 Test Accuracy : 0.9771999716758728 \n",
      "****************************************************************************************************\n",
      "Iteration  457 : Loss =  0.22902314   Acc:  0.984225\n",
      "            : Test Loss :0.3326221704483032 Test Accuracy : 0.9776999950408936 \n",
      "****************************************************************************************************\n",
      "Iteration  458 : Loss =  0.22741218   Acc:  0.9844083\n",
      "            : Test Loss :0.33107757568359375 Test Accuracy : 0.9776999950408936 \n",
      "****************************************************************************************************\n",
      "Iteration  459 : Loss =  0.22615568   Acc:  0.9846067\n",
      "            : Test Loss :0.3309832811355591 Test Accuracy : 0.9775500297546387 \n",
      "****************************************************************************************************\n",
      "Iteration  460 : Loss =  0.2256995   Acc:  0.9846417\n",
      "            : Test Loss :0.3315992057323456 Test Accuracy : 0.977869987487793 \n",
      "****************************************************************************************************\n",
      "Iteration  461 : Loss =  0.22593044   Acc:  0.9846017\n",
      "            : Test Loss :0.33181241154670715 Test Accuracy : 0.9773600101470947 \n",
      "****************************************************************************************************\n",
      "Iteration  462 : Loss =  0.22642826   Acc:  0.98454\n",
      "            : Test Loss :0.3325599133968353 Test Accuracy : 0.9777200222015381 \n",
      "****************************************************************************************************\n",
      "Iteration  463 : Loss =  0.2264267   Acc:  0.984525\n",
      "            : Test Loss :0.331723690032959 Test Accuracy : 0.9773600101470947 \n",
      "****************************************************************************************************\n",
      "Iteration  464 : Loss =  0.22609055   Acc:  0.98454666\n",
      "            : Test Loss :0.33158066868782043 Test Accuracy : 0.9777699708938599 \n",
      "****************************************************************************************************\n",
      "Iteration  465 : Loss =  0.22517952   Acc:  0.98460335\n",
      "            : Test Loss :0.33049288392066956 Test Accuracy : 0.9777100086212158 \n",
      "****************************************************************************************************\n",
      "Iteration  466 : Loss =  0.22437096   Acc:  0.98470336\n",
      "            : Test Loss :0.3305099606513977 Test Accuracy : 0.977590024471283 \n",
      "****************************************************************************************************\n",
      "Iteration  467 : Loss =  0.22395091   Acc:  0.98473835\n",
      "            : Test Loss :0.33069512248039246 Test Accuracy : 0.977940022945404 \n",
      "****************************************************************************************************\n",
      "Iteration  468 : Loss =  0.22392064   Acc:  0.98479\n",
      "            : Test Loss :0.3308938145637512 Test Accuracy : 0.9774900078773499 \n",
      "****************************************************************************************************\n",
      "Iteration  469 : Loss =  0.2240306   Acc:  0.98468834\n",
      "            : Test Loss :0.33119505643844604 Test Accuracy : 0.9778100252151489 \n",
      "****************************************************************************************************\n",
      "Iteration  470 : Loss =  0.2239127   Acc:  0.984775\n",
      "            : Test Loss :0.33068692684173584 Test Accuracy : 0.977590024471283 \n",
      "****************************************************************************************************\n",
      "Iteration  471 : Loss =  0.22353894   Acc:  0.9847367\n",
      "            : Test Loss :0.33043015003204346 Test Accuracy : 0.9779300093650818 \n",
      "****************************************************************************************************\n",
      "Iteration  472 : Loss =  0.22288299   Acc:  0.984845\n",
      "            : Test Loss :0.3298148810863495 Test Accuracy : 0.9777200222015381 \n",
      "****************************************************************************************************\n",
      "Iteration  473 : Loss =  0.22228387   Acc:  0.9848483\n",
      "            : Test Loss :0.3296760320663452 Test Accuracy : 0.9778100252151489 \n",
      "****************************************************************************************************\n",
      "Iteration  474 : Loss =  0.22191155   Acc:  0.9849067\n",
      "            : Test Loss :0.32982298731803894 Test Accuracy : 0.9779700040817261 \n",
      "****************************************************************************************************\n",
      "Iteration  475 : Loss =  0.22177584   Acc:  0.98494\n",
      "            : Test Loss :0.3297847509384155 Test Accuracy : 0.9776700139045715 \n",
      "****************************************************************************************************\n",
      "Iteration  476 : Loss =  0.22174378   Acc:  0.9849\n",
      "            : Test Loss :0.33008629083633423 Test Accuracy : 0.9779999852180481 \n",
      "****************************************************************************************************\n",
      "Iteration  477 : Loss =  0.22162646   Acc:  0.9849217\n",
      "            : Test Loss :0.3298332393169403 Test Accuracy : 0.977590024471283 \n",
      "****************************************************************************************************\n",
      "Iteration  478 : Loss =  0.22138552   Acc:  0.98492336\n",
      "            : Test Loss :0.3298386335372925 Test Accuracy : 0.9779899716377258 \n",
      "****************************************************************************************************\n",
      "Iteration  479 : Loss =  0.22098109   Acc:  0.9849783\n",
      "            : Test Loss :0.32944780588150024 Test Accuracy : 0.9775599837303162 \n",
      "****************************************************************************************************\n",
      "Iteration  480 : Loss =  0.22054978   Acc:  0.98499\n",
      "            : Test Loss :0.3292887210845947 Test Accuracy : 0.9779199957847595 \n",
      "****************************************************************************************************\n",
      "Iteration  481 : Loss =  0.22017874   Acc:  0.9850517\n",
      "            : Test Loss :0.32933586835861206 Test Accuracy : 0.9776999950408936 \n",
      "****************************************************************************************************\n",
      "Iteration  482 : Loss =  0.219931   Acc:  0.98504835\n",
      "            : Test Loss :0.3291777968406677 Test Accuracy : 0.9777699708938599 \n",
      "****************************************************************************************************\n",
      "Iteration  483 : Loss =  0.21979533   Acc:  0.98506\n",
      "            : Test Loss :0.3295818567276001 Test Accuracy : 0.9776999950408936 \n",
      "****************************************************************************************************\n",
      "Iteration  484 : Loss =  0.21972676   Acc:  0.98504\n",
      "            : Test Loss :0.32929420471191406 Test Accuracy : 0.9777200222015381 \n",
      "****************************************************************************************************\n",
      "Iteration  485 : Loss =  0.21967374   Acc:  0.985055\n",
      "            : Test Loss :0.3299258053302765 Test Accuracy : 0.9776700139045715 \n",
      "****************************************************************************************************\n",
      "Iteration  486 : Loss =  0.219635   Acc:  0.98498166\n",
      "            : Test Loss :0.3295770585536957 Test Accuracy : 0.9775099754333496 \n",
      "****************************************************************************************************\n",
      "Iteration  487 : Loss =  0.2195944   Acc:  0.9850417\n",
      "            : Test Loss :0.33056363463401794 Test Accuracy : 0.9777899980545044 \n",
      "****************************************************************************************************\n",
      "Iteration  488 : Loss =  0.21977153   Acc:  0.9849567\n",
      "            : Test Loss :0.330252468585968 Test Accuracy : 0.9775599837303162 \n",
      "****************************************************************************************************\n",
      "Iteration  489 : Loss =  0.21989909   Acc:  0.98499167\n",
      "            : Test Loss :0.33193522691726685 Test Accuracy : 0.977649986743927 \n",
      "****************************************************************************************************\n",
      "Iteration  490 : Loss =  0.22070874   Acc:  0.9847417\n",
      "            : Test Loss :0.33167630434036255 Test Accuracy : 0.9775199890136719 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  491 : Loss =  0.22087117   Acc:  0.98494667\n",
      "            : Test Loss :0.33343520760536194 Test Accuracy : 0.9773899912834167 \n",
      "****************************************************************************************************\n",
      "Iteration  492 : Loss =  0.22182557   Acc:  0.9845883\n",
      "            : Test Loss :0.33195266127586365 Test Accuracy : 0.9775099754333496 \n",
      "****************************************************************************************************\n",
      "Iteration  493 : Loss =  0.22079818   Acc:  0.98490334\n",
      "            : Test Loss :0.33153924345970154 Test Accuracy : 0.9774600267410278 \n",
      "****************************************************************************************************\n",
      "Iteration  494 : Loss =  0.21982066   Acc:  0.98484665\n",
      "            : Test Loss :0.3294526934623718 Test Accuracy : 0.977590024471283 \n",
      "****************************************************************************************************\n",
      "Iteration  495 : Loss =  0.21783328   Acc:  0.9851717\n",
      "            : Test Loss :0.3284100294113159 Test Accuracy : 0.9776800274848938 \n",
      "****************************************************************************************************\n",
      "Iteration  496 : Loss =  0.21653806   Acc:  0.9852567\n",
      "            : Test Loss :0.32851776480674744 Test Accuracy : 0.9778299927711487 \n",
      "****************************************************************************************************\n",
      "Iteration  497 : Loss =  0.21627444   Acc:  0.9852833\n",
      "            : Test Loss :0.32906532287597656 Test Accuracy : 0.9776700139045715 \n",
      "****************************************************************************************************\n",
      "Iteration  498 : Loss =  0.2168095   Acc:  0.98527664\n",
      "            : Test Loss :0.3305041193962097 Test Accuracy : 0.977649986743927 \n",
      "****************************************************************************************************\n",
      "Iteration  499 : Loss =  0.21766794   Acc:  0.98500335\n",
      "            : Test Loss :0.3302423059940338 Test Accuracy : 0.9775400161743164 \n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "    # load the training and test data    \n",
    "    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "    # reshape the feature data\n",
    "    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "    te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "    # noramlise feature data\n",
    "    tr_x = tr_x / 255.0\n",
    "    te_x = te_x / 255.0\n",
    "\n",
    "\n",
    "    # one hot encode the training labels and get the transpose\n",
    "    tr_y = np_utils.to_categorical(tr_y,10)\n",
    "    tr_y = tr_y.T\n",
    "\n",
    "\n",
    "    # one hot encode the test labels and get the transpose\n",
    "    te_y = np_utils.to_categorical(te_y,10)\n",
    "    te_y = te_y.T\n",
    "\n",
    "    return tr_x, tr_y, te_x, te_y\n",
    "     \n",
    "    \n",
    "\n",
    "\n",
    "def softmax(y_pred):\n",
    "    \n",
    "    \"\"\"Compute softmax values for each of the image in X .\n",
    "       And convert each predicted value into a probability betwen 0 and 1, summing to 1 \n",
    "    \"\"\"\n",
    "    return tf.exp(y_pred) / tf.reduce_sum(tf.exp(y_pred), axis=0) \n",
    "\n",
    "\n",
    "\"\"\"Forward pass for 1_2_1\"\"\"\n",
    "def forward_pass(x, w2,w1, b):\n",
    "    \n",
    "    x = tf.transpose(x)\n",
    "    \n",
    "    \n",
    "    \"\"\"Layer 1 - ReLu Layer with 300 neurons\"\"\"\n",
    "    y_pred_layer1 = tf.matmul(w1, x) + b\n",
    "    y_pred_relu = tf.maximum(y_pred_layer1, 0)\n",
    "    \n",
    "   \n",
    "    \"\"\"Layer 2 - Sigmoid Layer with 10 neurons\"\"\"\n",
    "    # We need to mutliply the output of the previous layer by the weights of this layer and add bias\n",
    "    y_pred_layer2 = tf.matmul(w2, y_pred_relu) + b  \n",
    "    \n",
    "\n",
    "    # Pipe the results through the signmoid activation function. \n",
    "    y_pred_softmax = softmax(y_pred_layer2)\n",
    "       \n",
    "\n",
    "    return y_pred_softmax\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy(y, y_pred):\n",
    "    '''Sometimes the softmax  values (probability) ouptutted by a neuron can se very very close to 0 or 0\n",
    "       In this case the log(0)==> not defined and will result in NaN values.\n",
    "       This can be handled using this function clip_by_value which replaces every value less than threshold\n",
    "       with the minimum value (1e-10)'''\n",
    "    \n",
    "    y_pred_ = tf.clip_by_value(y_pred, 1e-10, 1.0)\n",
    "    \n",
    "    \"\"\"Calculate the crosss entropy loss per image\"\"\"\n",
    "    num = - tf.reduce_sum( y * tf.math.log(y_pred), axis=0 )\n",
    "    \n",
    "    \"\"\"Calculate the average loss\"\"\"\n",
    "    return tf.reduce_mean(num, axis=0)\n",
    "       \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_accuracy(x, y, w2, w1, b):\n",
    "  \n",
    "    y_pred_sigmoid = forward_pass(x, w2, w1, b)\n",
    "    \n",
    "    # Round the predictions by the logistical unit to either 1 or 0\n",
    "    predictions = tf.round(y_pred_sigmoid)\n",
    "\n",
    "    # tf.equal will return a boolean array: True if prediction correct, False otherwise\n",
    "    # tf.cast converts the resulting boolean array to a numerical array \n",
    "    # 1 if True (correct prediction), 0 if False (incorrect prediction)\n",
    "    predictions_correct = tf.cast(tf.equal(predictions, y), tf.float32)\n",
    "\n",
    "    # Finally, we just determine the mean value of predictions_correct\n",
    "    accuracy = tf.reduce_mean(predictions_correct)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def main():\n",
    "    list_of_test_accuracies=[]\n",
    "    list_of_train_accuracies=[]\n",
    "    list_of_test_loss=[]\n",
    "    list_of_train_loss=[]\n",
    "    \n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    num_Iterations = 500\n",
    "    adam_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    tr_x, tr_y, te_x, te_y  = load_data()\n",
    "\n",
    "    tr_x = tf.cast(tr_x, tf.float32)\n",
    "    te_x = tf.cast(te_x, tf.float32)\n",
    "    tr_y = tf.cast(tr_y, tf.float32)\n",
    "    te_y = tf.cast(te_y, tf.float32)\n",
    "    \n",
    "\n",
    "\n",
    "    # We need a coefficient for each of the features and a single bias value\n",
    "    w2 = tf.Variable(tf.random.normal([ 10,300], mean=0.0, stddev=0.05))\n",
    "    w1 = tf.Variable(tf.random.normal([ 300,784], mean=0.0, stddev=0.05))\n",
    "    \n",
    "    print('w1 shape ==>',w1.shape)\n",
    "    print('w2 shape ==>',w2.shape)\n",
    "    \n",
    "    b = tf.Variable([0.])\n",
    "   \n",
    "    # Iterate our training loop\n",
    "    for i in range(num_Iterations):\n",
    "      \n",
    "        # Create an instance of GradientTape to monitor the forward pass\n",
    "        # and calcualte the gradients for each of the variables m and c\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            y_pred = forward_pass(tr_x, w2,w1, b)\n",
    "            currentLoss = cross_entropy(tr_y, y_pred)\n",
    "            list_of_train_loss.append(currentLoss)\n",
    "            \n",
    "        \"\"\"Calculate partial derivatives of Average Loss with respect to change in w and b\"\"\"\n",
    "        gradients = tape.gradient(currentLoss, [w1,w2, b])\n",
    "        accuracy = calculate_accuracy(tr_x, tr_y, w2,w1, b)\n",
    "        list_of_train_accuracies.append(accuracy)\n",
    "\n",
    "        print (\"Iteration \", i, \": Loss = \",currentLoss.numpy(), \"  Acc: \", accuracy.numpy())\n",
    "        \n",
    "        \"\"\"Give the above calculated derivatives to the optimizer to decent to the most optimal path\"\"\"\n",
    "        adam_optimizer.apply_gradients(zip(gradients, [w1,w2,b]))\n",
    "        \n",
    "        \"\"\"Lets see how the model performs with the newly upadted w and b on the test data\"\"\"\n",
    "        te_y_pred = forward_pass(te_x, w2, w1, b)\n",
    "        current_Test_Loss = cross_entropy(te_y, te_y_pred)\n",
    "        list_of_test_loss.append(current_Test_Loss.numpy())\n",
    "\n",
    "        test_accuracy = calculate_accuracy(te_x, te_y, w2, w1, b) \n",
    "        list_of_test_accuracies.append(test_accuracy.numpy())\n",
    "\n",
    "        print (\"            : Test Loss :{0} Test Accuracy : {1} \" .format(current_Test_Loss,test_accuracy) )\n",
    "\n",
    "        print(\"*\"*100)\n",
    "        \n",
    "    \"\"\"Plotting the performance\"\"\"\n",
    "    configuration = 'Network A \\n 300 ReLu  10 softmax neurons'\n",
    "\n",
    "    plt.title(\"Training Loss \\n\"+configuration+' Epochs:'+str (num_Iterations))\n",
    "    plt.plot(list_of_test_loss, label=\"Val Loss\")\n",
    "    plt.plot(list_of_train_loss, label=\"Train Loss\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
